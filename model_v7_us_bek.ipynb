{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben E Keith COVID Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: \n",
    "1. Process BEK data (source = data.attribytes.com)\n",
    "2. Analyze data using COVID segmentation\n",
    "3. Compare sell-out (Ben E Keith) to sell-in (McCain) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load libraries, initiate folder/file paths\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import teradatasql\n",
    "\n",
    "#path where dictionary file can be found\n",
    "#Neil\n",
    "DICTIONARY = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Distributor Sell-Out Dictionaries\\\\'\n",
    "#Joe\n",
    "#DICTIONARY = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Distributor Sell-Out Dictionaries\\\\'\n",
    "\n",
    "#main path\n",
    "#Neil\n",
    "PATH = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\\\'\n",
    "#Joe\n",
    "#PATH = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\\\'\n",
    "\n",
    "#backup path\n",
    "#Neil\n",
    "BACKUP = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\Backups\\\\'\n",
    "#Joe\n",
    "#BACKUP = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\Backups\\\\'\n",
    "\n",
    "#time dataframe\n",
    "TIME = pd.read_excel(DICTIONARY + 'Time Definitions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Dictionary\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def us_states():\n",
    "    us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY'\n",
    "    }\n",
    "\n",
    "    # thank you to @kinghelix and @trevormarburger for this idea\n",
    "    abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))\n",
    "    \n",
    "    return pd.DataFrame.from_dict(abbrev_us_state, orient = 'index', columns = ['State Name']).rename_axis('State').reset_index()\n",
    "\n",
    "\n",
    "def apply_dictionary(df, file_name):\n",
    "    \n",
    "    #create dictionary object from Excel file\n",
    "    #adding sheet_name = None makes it a dictionary type\n",
    "    _dict = pd.read_excel(DICTIONARY + file_name, sheet_name = None, engine='openpyxl')\n",
    "    \n",
    "    #create DataFrame from dictionary object called dict (short for dictionary)\n",
    "    dict_df = pd.DataFrame.from_dict(_dict['Segment Mapping'])\n",
    "    \n",
    "    #create DataFrame from dictionary object called cat (short for category)\n",
    "    sku_df = pd.DataFrame.from_dict(_dict['SKU Mapping'])\n",
    "    \n",
    "    #print shape of df (dimensions)\n",
    "    print(f'Shape before adding dictionary: {df.shape}', flush = True)\n",
    "\n",
    "    #Business Unit\tSIC Code\tSIC Sub\n",
    "    #Group\n",
    "    \n",
    "    #add lower case for merging\n",
    "    dict_df['Business Unit-lower'] = dict_df['Business Unit'].str.lower()\n",
    "    dict_df['SIC Code-lower'] = dict_df['SIC Code'].str.lower()\n",
    "    dict_df['SIC Sub-lower'] = dict_df['SIC Sub'].str.lower()\n",
    "    \n",
    "    #COVID Segmentation - L1\tCOVID Segmentation - L2\tCOVID Segmentation - (Restaurants)\tCOVID Segmentation - (Restaurants: Sub-Segment)\tRestaurant Service Type\tCuisine Type\n",
    "    dict_df = dict_df.groupby(['COVID Segmentation - L1','Business Unit-lower','SIC Code-lower','SIC Sub-lower',\n",
    "                               'COVID Segmentation - L2','COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)',\n",
    "                               'Restaurant Service Type','Cuisine Type'], dropna = False).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    #add lower case key columns for merging (removes case mismatch)\n",
    "    df['Business Unit-lower'] = df['Business Unit'].str.lower()\n",
    "    df['SIC Code-lower'] = df['SIC Code'].str.lower()\n",
    "    df['SIC Sub-lower'] = df['SIC Sub'].str.lower()\n",
    "    \n",
    "    df = df.rename(columns = {\n",
    "        'Customer City':'City', \n",
    "        'Customer State':'State',\n",
    "        'Manufacture Prod.Nbr.':'SKU ID'\n",
    "    })\n",
    "    \n",
    "    df = df.merge(dict_df, how = 'left', left_on = ['Business Unit-lower','SIC Code-lower','SIC Sub-lower'],\n",
    "                  right_on = ['Business Unit-lower','SIC Code-lower','SIC Sub-lower']).drop(columns = {'Business Unit-lower','SIC Code-lower','SIC Sub-lower'})\n",
    "    \n",
    "    #print shape of df (dimensions)\n",
    "    print(f'Shape after adding segmentation: {df.shape}', flush = True)\n",
    "    \n",
    "    #SKU Mapping\n",
    "    \n",
    "    df['SKU ID'] = df['SKU ID'].astype(str)\n",
    "    sku_df['Mfg ID'] = sku_df['Mfg ID'].astype(str)\n",
    "    \n",
    "    df['Mfg ID-lower'] = df['SKU ID'].str.lower()\n",
    "    sku_df['Mfg ID-lower'] = sku_df['Mfg ID'].str.lower()\n",
    "    \n",
    "    sku_df = sku_df.groupby(['Mfg ID-lower','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy'], dropna = False).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    df = df.merge(sku_df, how = 'left', left_on = ['Mfg ID-lower'],right_on = ['Mfg ID-lower']).drop(columns = {'Mfg ID-lower'})\n",
    "    \n",
    "    #print shape of df (dimensions)\n",
    "    print(f'Shape after adding product segmentation: {df.shape}', flush = True)\n",
    "    \n",
    "    df['Week Starting'] = pd.to_datetime(df['Week of'])\n",
    "    \n",
    "    print(f'Shape before adding time: {df.shape}', flush = True)\n",
    "    \n",
    "    df = df.merge(TIME[['Week Starting (Sun)', 'Calendar Week Year']], how = 'left', \n",
    "                  left_on = ['Week Starting'], right_on = ['Week Starting (Sun)']).drop(columns = {'Week Starting (Sun)'})\n",
    "    \n",
    "    print(f'Shape after adding time: {df.shape}', flush = True)\n",
    "    \n",
    "    #exclude certain records\n",
    "    df = df[~df['Calendar Week Year'].isna()]\n",
    "    df = df[df['Branch'] != 'Total']\n",
    "    df['LBS'] = pd.to_numeric(df['LBS'])\n",
    "    df['Calendar Week Year'] = df['Calendar Week Year'].astype('int64')\n",
    "    \n",
    "    #Merge states\n",
    "    df = df.merge(us_states(), how = 'left', on = 'State')\n",
    "       \n",
    "    df = clean_city(df)\n",
    "    \n",
    "    print(f'Shape after adding dictionary: {df.shape}', flush = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import File\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(file_name):\n",
    "    \n",
    "#import file\n",
    "    if '.csv' in file_name:\n",
    "        df = pd.read_csv(file_name, thousands = ',', encoding=\"utf-8\", low_memory = False, header = 0,na_values = \" \")\n",
    "        df = df[df['Branch'] != 'Total']\n",
    "        #df['Unnamed: 21'] = df['Unnamed: 21'].replace('[\\$,)]','', regex=True).replace('[(]','-', regex=True).astype(float)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        _import = pd.read_excel(file_name, sheet_name=None)\n",
    "        \n",
    "        for f in _import:\n",
    "            print(f)\n",
    "            if f == 'Sheet1':\n",
    "                add = pd.DataFrame.from_dict(_import[f])\n",
    "                \n",
    "                col = add.columns.to_list()\n",
    "            else:\n",
    "                add = _import[f].T.reset_index().T\n",
    "                add.columns = col\n",
    "        \n",
    "            df = df.append(add)\n",
    "            \n",
    "    return df.rename(columns={\n",
    "        'Unnamed: 21':'LBS',\n",
    "        'Unnamed: 22':'LBS'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculation Functions\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling(df, _list):\n",
    "    #groupby _list\n",
    "    df = df.groupby(_list, dropna = False)[['LBS','LBS_LY','LBS_Baseline']].sum().reset_index()\n",
    "    \n",
    "    #set index to all but last column in list\n",
    "    df = df.set_index(_list)\n",
    "    \n",
    "    #add new metric SMA_4 (simple moving average - 4 periods)\n",
    "    #level = all but last 2 items in list\n",
    "    df['LBS_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    df['LBS_Lag_2'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 2)\n",
    "    df['LBS_Lag_3'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 3)\n",
    "    df['LBS_Lag_4'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 4)\n",
    "    \n",
    "    df['SMA_4'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['LBS_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].shift(periods = 1)\n",
    "    df['LBS_LY_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    \n",
    "    df['SMA_4_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4'].shift(periods = 1)\n",
    "    df['SMA_4_LY_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_LY'].shift(periods = 1)\n",
    "    df['SMA_4_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_Baseline'].shift(periods = 1)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "def add_last_year(df, _list):\n",
    "    #list of groupby columns\n",
    "    #last item in list is Calendar Week Year which is used to pull previous history (Baseline Week = Calendar Week Year) of copied dataframe\n",
    "    _groupby = _list.copy()\n",
    "    \n",
    "    _merge_yoy = _list.copy()[0:-1]\n",
    "    _merge_yoy.extend(['YOY Week'])\n",
    "    \n",
    "    _merge_baseline = _list.copy()[0:-1]\n",
    "    _merge_baseline.extend(['Baseline Week'])\n",
    "    \n",
    "    df1 = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #groupby _list\n",
    "    df_new = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #add week dimensions to main dataframe\n",
    "    df_new = df_new.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_yoy, right_on=_groupby).drop(columns={'Calendar Week Year_y'}).rename(columns={'LBS_y':'LBS_LY'})\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_baseline, right_on=_groupby).drop(columns={'Calendar Week Year'}).rename(columns={\n",
    "        'LBS':'LBS_Baseline','Calendar Week Year_x':'Calendar Week Year','LBS_x':'LBS'})\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def add_precovid(df, _list, begin, end):\n",
    "    #datefield should be last in _list\n",
    "    datefield = _list[-1]\n",
    "          \n",
    "    #remove datefield from list\n",
    "    _list = _list[0:-1]\n",
    "    \n",
    "    #filter data not using last and rename columns\n",
    "    _df = df[(df[datefield] >= begin) & (df[datefield] <= end)].groupby(_list)['LBS'].sum() / 52\n",
    "    \n",
    "    return df.merge(\n",
    "        _df, how = 'left', left_on = _list, right_on = _list).rename(\n",
    "        columns = {'LBS_x':'LBS', 'LBS_y':'LBS_PRECOVID'}).fillna(\n",
    "        value = {'LBS_PRECOVID': 0})\n",
    "\n",
    "\n",
    "def add_time(df):\n",
    "    df = df.merge(TIME[['Calendar Week Year','Week Starting (Sun)','Week Ending (Sat)', 'COVID Week']],\n",
    "                   how = 'left', \n",
    "                   on = 'Calendar Week Year')\n",
    "    \n",
    "    df = df.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_1(df, _list, begin, end):\n",
    "    if 'Calendar Week Year' not in _list:\n",
    "        _list.extend(['Calendar Week Year'])\n",
    "    \n",
    "    df = full_dataframe(df, _list)\n",
    "    \n",
    "    #add last year lbs\n",
    "    df = add_last_year(df, _list)\n",
    "    \n",
    "    #add rolling calculation\n",
    "    df = add_rolling(df, _list)\n",
    "        \n",
    "    #add preCOVID baseline\n",
    "    df = add_precovid(df, _list, begin, end)\n",
    "    \n",
    "    df = df.round({\n",
    "        'LBS' : 2,    \n",
    "        'SMA_4' : 2,\n",
    "        'SMA_8' : 2,\n",
    "        'SMA_12' : 2,\n",
    "        'LBS_LY' : 2,    \n",
    "        'SMA_4_LY' : 2,\n",
    "        'SMA_8_LY' : 2,\n",
    "        'SMA_12_LY' : 2,\n",
    "        'LBS_Baseline' : 2,    \n",
    "        'SMA_4_Baseline' : 2,\n",
    "        'SMA_8_Baseline' : 2,\n",
    "        'SMA_12_Baseline' : 2,\n",
    "        'LBS_PRECOVID' : 2,\n",
    "        'LBS_Lag_1' : 2,\n",
    "        'LBS_Lag_2' : 2,\n",
    "        'LBS_Lag_3' : 2,\n",
    "        'LBS_Lag_4' : 2,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 2,\n",
    "        'SMA_4_LY_Lag_1' : 2,\n",
    "        'SMA_4_Baseline_Lag_1' : 2\n",
    "        \n",
    "    }).fillna(value = {\n",
    "        'LBS' : 0,    \n",
    "        'SMA_4' : 0,\n",
    "        'SMA_8' : 0,\n",
    "        'SMA_12' : 0,\n",
    "        'LBS_LY' : 0,    \n",
    "        'SMA_4_LY' : 0,\n",
    "        'SMA_8_LY' : 0,\n",
    "        'SMA_12_LY' : 0,\n",
    "        'LBS_Baseline' : 0,    \n",
    "        'SMA_4_Baseline' : 0,\n",
    "        'SMA_8_Baseline' : 0,\n",
    "        'SMA_12_Baseline' : 0,\n",
    "        'LBS_PRECOVID' : 0,\n",
    "        'LBS_Lag_1' : 0,\n",
    "        'LBS_Lag_2' : 0,\n",
    "        'LBS_Lag_3' : 0,\n",
    "        'LBS_Lag_4' : 0,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 0,\n",
    "        'SMA_4_LY_Lag_1' : 0,\n",
    "        'SMA_4_Baseline_Lag_1' : 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_backup(df, file_name):\n",
    "    \n",
    "    df.to_csv(BACKUP + file_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def td_to_pandas(query, cur, title=''):\n",
    "    _data = []\n",
    "    _start=dt.now()\n",
    "    print(dt.now().strftime('%m/%d/%Y'))\n",
    "    print(f'{title} Execution started...', end='', flush=True)\n",
    "    cur.execute (query)\n",
    "    print(f'finished. {dt.now() - _start}', flush=True) \n",
    "    _start_fetch=dt.now()\n",
    "    print(f'{title} Fetching data started...', end='', flush=True)\n",
    "    for row in cur.fetchall():\n",
    "        _data.append(row) \n",
    "    print(f'finished. {dt.now() - _start_fetch}', flush=True) \n",
    "    _start=dt.now()\n",
    "    print(f'{title} Creating DataFrame for started...', end='', flush=True)\n",
    "    _df = pd.DataFrame(_data)\n",
    "    _df.columns = [x[0].replace('SAP_', '').lower() for x in cur.description]\n",
    "    print(f'finished. {dt.now() - _start}', flush=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def td_dataframe(select_db, query):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            print('Database selected!', flush=True)            \n",
    "            dim_df = td_to_pandas(query, cur, 'Query:')\n",
    "            print('Dim:', dim_df.shape)\n",
    "    \n",
    "    return dim_df\n",
    "\n",
    "\n",
    "def restaurants(df):\n",
    "    #restaurants = df.loc[df['COVID Segmentation - (Restaurants)'] == 'Restaurants', :]\n",
    "    \n",
    "    if df.columns.isin(['COVID Segmentation - L2']).sum() > 0:\n",
    "        #Rename rows\n",
    "        df.loc[df['COVID Segmentation - L2'] == 'Independents (IOs) / Local Eateries / Takeaway', 'COVID Segmentation - L2'] = 'IO'\n",
    "        df.loc[\n",
    "            (df['COVID Segmentation - L2'] == 'All Other') | \n",
    "            (df['COVID Segmentation - L2'] == 'National Account') | \n",
    "            (df['COVID Segmentation - L2'] == 'Region Chains')| \n",
    "            (df['COVID Segmentation - L2'] == 'National Accounts'),\n",
    "            'COVID Segmentation - L2'] = 'Chain'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_list(df, work_list):\n",
    "    \n",
    "    _process = analyze_1(df, work_list, 201910, 202009)\n",
    "    \n",
    "    _process = restaurants(_process)\n",
    "    \n",
    "    _process['Distributor'] = 'Ben E Keith'\n",
    "    \n",
    "    _process = add_time(_process)\n",
    "    \n",
    "    #for standardizing output\n",
    "    work_list.extend(['Distributor','LBS','SMA_4','SMA_8','SMA_12',\n",
    "                      'YOY Week','LBS_LY','SMA_4_LY','SMA_8_LY','SMA_12_LY',\n",
    "                      'Baseline Week','LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "                      'LBS_Lag_1','LBS_Lag_2','LBS_Lag_3','LBS_Lag_4','LBS_Baseline_Lag_1','LBS_LY_Lag_1',\n",
    "                      'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "                      'LBS_PRECOVID','Week Starting (Sun)','Week Ending (Sat)','COVID Week'])\n",
    "    \n",
    "    if 'SKU ID' in work_list:\n",
    "        #last 26 weeks in dataframe\n",
    "        week_list = _process.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0}).sort_values(by = 'Calendar Week Year', ascending = True).squeeze().tolist()[-26:]\n",
    "        \n",
    "        #filter to only the last 26 weeks\n",
    "        _process = _process[_process['Calendar Week Year'].isin(week_list)]\n",
    "        \n",
    "    return _process[work_list]\n",
    "\n",
    "\n",
    "def is_missing(df):\n",
    "    #check for COVID Segmentation - L1\n",
    "    missing = df[df['COVID Segmentation - L1'].isna()].groupby(['Business Unit','SIC Code','SIC Sub'], as_index = False, dropna = False)['LBS'].sum()\n",
    "\n",
    "    if len(missing) > 0:\n",
    "        print('The following segments are missing:')\n",
    "        display(missing)\n",
    "        missing.to_excel(DICTIONARY + 'Segments Missing Dump\\\\' + dt.now().strftime('%Y%m%d') + '_bek_L1_missing.xlsx', index = False)\n",
    "    else:\n",
    "        print(f'Nothing missing for COVID Segmentation - L1', flush = True)\n",
    "\n",
    "    #check for product\n",
    "    missing = df[df['Consolidated Category'].isna()].groupby(['Group','Family','SKU ID','Prod Nbr','Product',\n",
    "                                                                        'Product Ext.Description','Pack / Size'], as_index = False, dropna = False)['LBS'].sum()\n",
    "\n",
    "    if len(missing) > 0:\n",
    "        print('The following products are missing:')\n",
    "        display(missing)\n",
    "        missing.to_excel(DICTIONARY + 'Segments Missing Dump\\\\' + dt.now().strftime('%Y%m%d') + r'_bek_missing.xlsx', index = False)\n",
    "    else:\n",
    "        print(f'Nothing missing for Product', flush = True)\n",
    "        \n",
    "def full_dataframe(df, _list):\n",
    "    weeks = df.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})\n",
    "    segments = df.groupby(_list[0:-1]).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    _df = segments.assign(key=1).merge(weeks.assign(key=1), how='outer', on='key').drop(columns = {'key'}) \n",
    "    \n",
    "    return _df.merge(df, how = 'left', on = _list) \n",
    "\n",
    "def clean_city(df):\n",
    "    df['City'] = df['City'].str.strip()\n",
    "    df['City'] = df['City'].str.upper()\n",
    "    df['City'].fillna('NA', inplace = True)\n",
    "    \n",
    "    #cities = 'TORONTO|MONTREAL|OTTAWA|CALGARY|VANCOUVER|WINNIPEG|MONTREAL|HAMILTON|HALIFAX'\n",
    "    cities = 'NOT USED CURRENTLY'\n",
    "    \n",
    "    #change each city name to the name of the city that matches, cleans up the city names\n",
    "    for c in cities.split('|'):\n",
    "        df.loc[df['City'].str.match(c), 'City'] = c\n",
    "    \n",
    "    #change all other cities to NA\n",
    "    df.loc[~df['City'].str.match(cities), 'City'] = 'NA'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sell-in vs. Sell-out\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teradata_sales(sellout):\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=BEK Performance;StartTime=20200901T101924;JobID=55096;Importance=666;'  FOR SESSION;\n",
    "    \n",
    "    #the current week is pulled from the time dictionary table\n",
    "    to_week = int(TIME[(TIME['Week Starting (Mon)'] <= dt.now()) & (TIME['Week Ending (Sun)'] >= dt.now())]['Calendar Week Year'].values)\n",
    "    \n",
    "    print(f'Starting Teradata connect...', flush = True)\n",
    "    \n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "        (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "        a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "        (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "        RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "        a16.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "        a17.DIVISION_NAME as DIVISION_NAME,\n",
    "        a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "        a12.SUB_CATEGORY_DESC as SUB_CATEGORY_DESC,\n",
    "        a13.PRODUCT_GROUP_FORMAT_DESC as L1_PRODUCT_HIERARCHY,\n",
    "        a13.PRODUCT_GROUP_SUB_FORMAT_DESC as L2_PRODUCT_HIERARCHY,\n",
    "        a15.MATERIAL_PRICING_GROUP_ID as MATERIAL_PRICING_GROUP_ID,\n",
    "        a18.MATERIAL_PRICING_GROUP_DESCRIPTION as MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "        TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "        a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "        sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "        on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "        on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "        on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_SALES_DATA as a15\n",
    "        on (a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL_ID and \n",
    "        a11.MATERIAL_ID = a15.MATERIAL_ID and \n",
    "        a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION_ID)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a16\n",
    "        on (a11.CUSTOMER_ID = a16.CUSTOMER and \n",
    "        a11.DISTRIBUTION_CHANNEL_ID = a16.DISTRIBUTION_CHANNEL and \n",
    "        a11.SALES_ORGANISATION_ID = a16.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a17\n",
    "        on (a13.DIVISION_ID = a17.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_PRICING_GROUP as a18\n",
    "        on (a15.MATERIAL_PRICING_GROUP_ID = a18.MATERIAL_PRICING_GROUP_ID)\n",
    "    where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "        and a11.SALES_ORGANISATION_ID in ('US01')\n",
    "        and a11.DISTRIBUTION_CHANNEL_ID in ('10')\n",
    "        and RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) in ('6500002764'))\n",
    "        and a14.CALENDAR_WEEK_NAME < ''' + str(to_week) + ''' \n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "        (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        a14.CALENDAR_WEEK_NAME,\n",
    "        (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "        a16.CUSTOMER_HIER_LVL_1_NAME,\n",
    "        a17.DIVISION_NAME,\n",
    "        a12.CATEGORY_DESC,\n",
    "        a12.SUB_CATEGORY_DESC,\n",
    "        a13.PRODUCT_GROUP_FORMAT_DESC,\n",
    "        a13.PRODUCT_GROUP_SUB_FORMAT_DESC,\n",
    "        a15.MATERIAL_PRICING_GROUP_ID,\n",
    "        a18.MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "        TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "        a13.MATERIAL_DESCRIPTION\n",
    "    ;'''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return teradata_transform(df, sellout)\n",
    "\n",
    "\n",
    "def teradata_transform(sellin, sellout):\n",
    "    #consolidates teradata sales with sellout data\n",
    "    \n",
    "    #convert from object datatype to float (exports as a number instead of string)\n",
    "    sellin['actual_volume_lbs'] = sellin['actual_volume_lbs'].astype('float64')\n",
    "    \n",
    "    #rename columns for consistancy\n",
    "    sellin = sellin.rename(columns = {'actual_volume_lbs':'LBS', \n",
    "                                      'calendar_week_number':'Calendar Week Year',\n",
    "                                      'l1_product_hierarchy':'L1 Product Hierarchy',\n",
    "                                      'l2_product_hierarchy':'L2 Product Hierarchy'})\n",
    "    \n",
    "    #transform calendar week year from teradata\n",
    "    sellin['Calendar Week Year'] = pd.to_numeric(sellin['Calendar Week Year'], errors = 'coerce')\n",
    "\n",
    "    #transform category so its consolidated\n",
    "    sellin['Consolidated Category'] = sellin['category_desc']\n",
    "    sellin.loc[sellin['Consolidated Category'] == 'Sweet Potato' , 'Consolidated Category'] = 'Potato'\n",
    "    sellin.loc[sellin['Consolidated Category'] != 'Potato' , 'Consolidated Category'] = 'Prepared Foods'\n",
    "    \n",
    "    #analyze sellin data\n",
    "    #sellin = analyze_1(sellin, ['Consolidated Category', 'L1 Product Hierarchy','L2 Product Hierarchy'], 201910, 202009)\n",
    "    #analyze sellin data\n",
    "    sellin = analyze_1(sellin, ['Consolidated Category'], 201910, 202009)\n",
    "    \n",
    "    '''\n",
    "    #   Column                 Non-Null Count  Dtype  \n",
    "    ---  ------                 --------------  -----  \n",
    "     0   Consolidated Category  296 non-null    object \n",
    "     1   Calendar Week Year     296 non-null    int64  \n",
    "     2   MCCAIN LBS             296 non-null    float64\n",
    "     3   MCCAIN SMA_4           296 non-null    float64\n",
    "     4   MCCAIN SMA_8           296 non-null    float64\n",
    "     5   MCCAIN SMA_12          296 non-null    float64\n",
    "     6   LBS_Lag_1              296 non-null    float64\n",
    "     7   LBS_Lag_2              296 non-null    float64\n",
    "     8   LBS_Lag_3              296 non-null    float64\n",
    "     9   LBS_Lag_4              296 non-null    float64\n",
    "     10  YOY Week               242 non-null    float64\n",
    "     11  Baseline Week          242 non-null    float64\n",
    "     12  LBS_LY                 296 non-null    float64\n",
    "     13  SMA_4_LY               296 non-null    float64\n",
    "     14  SMA_8_LY               296 non-null    float64\n",
    "     15  SMA_12_LY              296 non-null    float64\n",
    "     16  LBS_Baseline           296 non-null    float64\n",
    "     17  SMA_4_Baseline         296 non-null    float64\n",
    "     18  SMA_8_Baseline         296 non-null    float64\n",
    "     19  SMA_12_Baseline        296 non-null    float64\n",
    "     20  MCCAIN PRECOVID        296 non-null    float64\n",
    "    '''\n",
    "    \n",
    "    #rename columns accordingly\n",
    "    sellin = sellin.rename(columns = {'LBS':'MCCAIN LBS',\n",
    "                                      'SMA_4':'MCCAIN SMA_4',\n",
    "                                      'SMA_8':'MCCAIN SMA_8',\n",
    "                                      'SMA_12':'MCCAIN SMA_12',\n",
    "                                      'LBS_PRECOVID':'MCCAIN PRECOVID',\n",
    "                                      'LBS_Lag_1':'MCCAIN Lag_1',\n",
    "                                      'LBS_Lag_2':'MCCAIN Lag_2',\n",
    "                                      'LBS_Lag_3':'MCCAIN Lag_3',\n",
    "                                      'LBS_Lag_4':'MCCAIN Lag_4',\n",
    "                                      'LBS_Baseline' : 'MCCAIN LBS_Baseline',\n",
    "                                      'SMA_4_Baseline' : 'MCCAIN SMA_4_Baseline',\n",
    "                                      'SMA_8_Baseline' : 'MCCAIN SMA_8_Baseline',\n",
    "                                      'SMA_12_Baseline' : 'MCCAIN SMA_12_Baseline',\n",
    "                                      'SMA_4_Lag_1':'MCCAIN SMA_4_Lag_1',\n",
    "                                      'SMA_4_Baseline_Lag_1' : 'MCCAIN SMA_4_Baseline_Lag_1',\n",
    "                                      'LBS_Baseline_Lag_1': 'MCCAIN LBS_Baseline_Lag_1'\n",
    "                                     })\n",
    "    \n",
    "    '''\n",
    "    #   Column                 Non-Null Count  Dtype  \n",
    "    ---  ------                 --------------  -----  \n",
    "     0   Consolidated Category  242 non-null    object \n",
    "     1   Calendar Week Year     242 non-null    int64  \n",
    "     2   LBS                    242 non-null    float64\n",
    "     3   SMA_4                  242 non-null    float64\n",
    "     4   SMA_8                  242 non-null    float64\n",
    "     5   SMA_12                 242 non-null    float64\n",
    "     6   LBS_Lag_1              242 non-null    float64\n",
    "     7   LBS_Lag_2              242 non-null    float64\n",
    "     8   LBS_Lag_3              242 non-null    float64\n",
    "     9   LBS_Lag_4              242 non-null    float64\n",
    "     10  YOY Week               242 non-null    int64  \n",
    "     11  Baseline Week          242 non-null    int64  \n",
    "     12  LBS_LY                 242 non-null    float64\n",
    "     13  SMA_4_LY               242 non-null    float64\n",
    "     14  SMA_8_LY               242 non-null    float64\n",
    "     15  SMA_12_LY              242 non-null    float64\n",
    "     16  LBS_Baseline           242 non-null    float64\n",
    "     17  SMA_4_Baseline         242 non-null    float64\n",
    "     18  SMA_8_Baseline         242 non-null    float64\n",
    "     19  SMA_12_Baseline        242 non-null    float64\n",
    "     20  LBS_PRECOVID           242 non-null    float64\n",
    "    '''\n",
    "    \n",
    "    #analyze sellout data\n",
    "    df = analyze_1(sellout, ['Consolidated Category'], 201910, 202009)\n",
    "    \n",
    "    \n",
    "    df = df.merge(sellin[['Calendar Week Year','Consolidated Category','MCCAIN LBS','MCCAIN SMA_4','MCCAIN SMA_8','MCCAIN SMA_12','MCCAIN PRECOVID',\n",
    "                          'MCCAIN LBS_Baseline','MCCAIN SMA_4_Baseline','MCCAIN SMA_8_Baseline','MCCAIN SMA_12_Baseline',\n",
    "                          'MCCAIN Lag_1', 'MCCAIN Lag_2', 'MCCAIN Lag_3', 'MCCAIN Lag_4','MCCAIN LBS_Baseline_Lag_1',\n",
    "                          'MCCAIN SMA_4_Lag_1', 'MCCAIN SMA_4_Baseline_Lag_1']], how = 'left', \n",
    "                  left_on = ['Calendar Week Year','Consolidated Category'], right_on = ['Calendar Week Year','Consolidated Category'])\n",
    "    \n",
    "    df = df.fillna({'MCCAIN LBS': 0,\n",
    "                    'MCCAIN SMA_4': 0,\n",
    "                    'MCCAIN SMA_8': 0,\n",
    "                    'MCCAIN SMA_12': 0,\n",
    "                    'MCCAIN PRECOVID': 0,\n",
    "                    'MCCAIN Lag_1': 0,\n",
    "                    'MCCAIN Lag_2': 0,\n",
    "                    'MCCAIN Lag_3': 0,\n",
    "                    'MCCAIN Lag_4': 0,\n",
    "                    'MCCAIN LBS_Baseline': 0,\n",
    "                    'MCCAIN SMA_4_Baseline': 0,\n",
    "                    'MCCAIN SMA_8_Baseline': 0,\n",
    "                    'MCCAIN SMA_12_Baseline': 0,\n",
    "                    'MCCAIN LBS_Baseline_Lag_1':0,\n",
    "                    'MCCAIN SMA_4_Lag_1' : 0,\n",
    "                    'MCCAIN SMA_4_Baseline_Lag_1' : 0\n",
    "                   })\n",
    "    \n",
    "    df['Distributor'] = 'Ben E Keith'\n",
    "\n",
    "    df = add_time(df)\n",
    "    \n",
    "    df = df[['Consolidated Category','Distributor','Calendar Week Year',\n",
    "             'LBS','SMA_4','SMA_8','SMA_12','LBS_PRECOVID',\n",
    "             'LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "             'LBS_Lag_1', 'LBS_Lag_2', 'LBS_Lag_3', 'LBS_Lag_4', 'LBS_Baseline_Lag_1', 'SMA_4_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "             'MCCAIN LBS','MCCAIN SMA_4','MCCAIN SMA_8','MCCAIN SMA_12','MCCAIN PRECOVID',\n",
    "             'MCCAIN LBS_Baseline','MCCAIN SMA_4_Baseline','MCCAIN SMA_8_Baseline','MCCAIN SMA_12_Baseline',\n",
    "             'MCCAIN Lag_1', 'MCCAIN Lag_2', 'MCCAIN Lag_3', 'MCCAIN Lag_4','MCCAIN LBS_Baseline_Lag_1','MCCAIN SMA_4_Lag_1','MCCAIN SMA_4_Baseline_Lag_1',\n",
    "             'Week Starting (Sun)','Week Ending (Sat)','COVID Week']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Import Raw Data\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_24516\\4131771016.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(add)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_24516\\4131771016.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(add)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet3\n",
      "Imported 177038 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_24516\\4131771016.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(add)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 177665 entries, 0 to 177664\n",
    "Data columns (total 22 columns):\n",
    " #   Column                   Non-Null Count   Dtype  \n",
    "---  ------                   --------------   -----  \n",
    " 0   Branch                   177665 non-null  object \n",
    " 1   Business Unit            177664 non-null  object \n",
    " 2   SIC Code                 177664 non-null  object \n",
    " 3   SIC Sub                  177664 non-null  object \n",
    " 4   Customer Nbr.            177664 non-null  float64\n",
    " 5   Customer Name            177664 non-null  object \n",
    " 6   Customer Address1        177664 non-null  object \n",
    " 7   Customer Address2        8978 non-null    object \n",
    " 8   Customer City            177661 non-null  object \n",
    " 9   Customer State           177664 non-null  object \n",
    " 10  Customer Zip             177664 non-null  float64\n",
    " 11  Family                   177664 non-null  object \n",
    " 12  Brand                    177664 non-null  object \n",
    " 13  Group                    177664 non-null  object \n",
    " 14  Manufacture Prod.Nbr.    177664 non-null  object \n",
    " 15  Prod Nbr                 177664 non-null  float64\n",
    " 16  Product                  177664 non-null  object \n",
    " 17  Pack / Size              177664 non-null  object \n",
    " 18  Product Ext.Description  154392 non-null  object \n",
    " 19  Week of                  177664 non-null  object \n",
    " 20  Month                    177664 non-null  object \n",
    " 21  LBS                      177665 non-null  object \n",
    "dtypes: float64(3), object(19)\n",
    "memory usage: 29.8+ MB\n",
    "'''\n",
    "\n",
    "#new file name in historical file directory\n",
    "_new = import_file(PATH + 'BEK Update.xls')\n",
    "\n",
    "print(f'Imported {_new.shape[0]} records', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Apply Dictionary to Raw Data / Check For Missing Segmentation\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before adding dictionary: (177038, 22)\n",
      "Shape after adding segmentation: (177038, 28)\n",
      "Shape after adding product segmentation: (177038, 31)\n",
      "Shape before adding time: (177038, 32)\n",
      "Shape after adding time: (177038, 33)\n",
      "Shape after adding dictionary: (177034, 34)\n",
      "Nothing missing for COVID Segmentation - L1\n",
      "Nothing missing for Product\n"
     ]
    }
   ],
   "source": [
    "#add dictionary to new data\n",
    "_new_df = apply_dictionary(_new, 'BEK - US.xlsx')\n",
    "\n",
    "#look for missing segmentation\n",
    "is_missing(_new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Combine New and Base Datasets\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported shape...(2113771, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_24516\\943055581.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  _base = _import_df[~_import_df['Calendar Week Year'].isin(exclude_list)][['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape...(2126107, 15)\n"
     ]
    }
   ],
   "source": [
    "_import_df = pd.read_csv(BACKUP + 'BEK.csv', low_memory = False, thousands = ',', decimal = '.', dtype = {\n",
    "            'Calendar Week Year':np.int64,\n",
    "            'LBS':np.float64})\n",
    "\n",
    "print(f'Imported shape...{_import_df.shape}', flush = True)\n",
    "\n",
    "#create list to exclude from base data\n",
    "#exclude_list = _new_df.groupby('Calendar Week Year').size().reset_index().drop(columns={0}).squeeze().to_list()\n",
    "\n",
    "#create unique list values\n",
    "exclude_list = list(dict.fromkeys(_new_df['Calendar Week Year'].values.squeeze().tolist()))\n",
    "\n",
    "#turn list to string\n",
    "include = str(exclude_list)[1:-1]\n",
    "\n",
    "#import all records from base data minus the new data\n",
    "_base = _import_df[~_import_df['Calendar Week Year'].isin(exclude_list)][['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                 'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type',\n",
    "                 'Consolidated Category','SKU ID','Cuisine Type','L1 Product Hierarchy','L2 Product Hierarchy','Calendar Week Year','LBS']].append(\n",
    "        _new_df[['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                 'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type',\n",
    "                 'Consolidated Category','SKU ID','Cuisine Type','L1 Product Hierarchy','L2 Product Hierarchy','Calendar Week Year','LBS']])\n",
    "\n",
    "\n",
    "#clean up city column\n",
    "_base = clean_city(_base)\n",
    "\n",
    "#create a copy but just for restaurants#\n",
    "#_restaurants = restaurants(_base)\n",
    "\n",
    "print(f'Final shape...{_base.shape}', flush = True)\n",
    "#print(f'Restaurants shape...{_restaurants.shape}', flush = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Execute Analysis\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Segments\n",
      "Processing Sell in vs Sell out\n",
      "Starting Teradata connect...\n",
      "Database selected!\n",
      "07/07/2022\n",
      "Query: Execution started...finished. 0:01:16.042287\n",
      "Query: Fetching data started...finished. 0:00:11.625336\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.055248\n",
      "Dim: (34180, 16)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_list = []\n",
    "\n",
    "#Output 1: COVID L1 - List 0\n",
    "_list.append(['City','State Name','COVID Segmentation - L1','COVID Segmentation - L2','Restaurant Service Type','Consolidated Category'])\n",
    "\n",
    "#Output 2: COVID L1 - List 1\n",
    "_list.append(['State Name','COVID Segmentation - L1','COVID Segmentation - L2','Restaurant Service Type','SKU ID','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy'])\n",
    "\n",
    "#Create dataframes\n",
    "print(f'Processing Segments', flush = True)\n",
    "output1 = process_list(_base, _list[0])\n",
    "\n",
    "print(f'Processing Sell in vs Sell out', flush = True)\n",
    "output2 = teradata_sales(_base)\n",
    "\n",
    "#print(f'Processing SKU', flush = True)\n",
    "#output3 = process_list(_base, _list[1])\n",
    "\n",
    "#Output 2: COVID L1 - List 1\n",
    "#_list.append(['COVID Segmentation - L1','COVID Segmentation - L2','Restaurant Service Type','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy'])\n",
    "#output4 = process_list(_base, _list[2])\n",
    "\n",
    "print('All done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Upload Analysis to Teradata\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected! 07/07/2022 15:34:28 PM\n",
      "Deleting records for: Ben E Keith in table: SELLOUT_REGION\n",
      "Inserting records into SELLOUT_REGION\n",
      "Inserted 8470 records\n",
      "Database selected! 07/07/2022 15:34:38 PM\n",
      "Deleting records for: Ben E Keith in table: SELLOUT_AND_SELLIN\n",
      "Inserting records into SELLOUT_AND_SELLIN\n",
      "Inserted 28 records\n"
     ]
    }
   ],
   "source": [
    "def td_upload(select_db, df, table_name):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            d = dt.now().strftime('%m/%d/%Y %H:%M:%S %p')\n",
    "            print(f'Database selected! {d}', flush=True)            \n",
    "\n",
    "            delete_from_td(df, table_name, cur)\n",
    "            insert_into_td(df, table_name, cur)\n",
    "\n",
    "def delete_from_td(df, table_name, cur):\n",
    "    distributor = df.groupby('Distributor').size().reset_index().drop(columns=0).to_numpy()[0][0]\n",
    "    \n",
    "    print(f'Deleting records for: {distributor} in table: {table_name}', flush = True)          \n",
    "    \n",
    "    query = '''\n",
    "    DELETE FROM ''' + table_name  + ''' \n",
    "    WHERE \"Distributor\" = ''' + \"'\" + distributor + \"'\" + ''' AND \"Calendar Week Year\" IN (''' + include + \")\"\n",
    "    \n",
    "    #query = '''\n",
    "    #DELETE FROM ''' + table_name  + ''' \n",
    "    #WHERE \"Distributor\" = ''' + \"'\" + distributor + \"'\"\n",
    "    \n",
    "    cur.execute (query)\n",
    "    \n",
    "def insert_into_td(df, table_name, cur):\n",
    "    insert_list = df.values.tolist()\n",
    "    \n",
    "    #creates ?, ?,.... string used in query for teradata fastload\n",
    "    insert_columns = ('?, ' * len(df.columns)).rstrip(', ')\n",
    "    \n",
    "    print(f'Inserting records into {table_name}', flush = True)\n",
    "    \n",
    "    query = \"INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    #query = \"{fn teradata_try_fastload}INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    \n",
    "    cur.execute (query, insert_list)\n",
    "    \n",
    "    print(f'Inserted {df.shape[0]} records', flush = True)\n",
    "    \n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "#only send new data\n",
    "td_upload(select_db, output1[output1['Calendar Week Year'].isin(exclude_list)], 'SELLOUT_REGION')\n",
    "td_upload(select_db, output2[output2['Calendar Week Year'].isin(exclude_list)], 'SELLOUT_AND_SELLIN')\n",
    "\n",
    "#only send all data\n",
    "#td_upload(select_db, output1, 'SELLOUT_REGION')\n",
    "#td_upload(select_db, output2, 'SELLOUT_AND_SELLIN')\n",
    "\n",
    "#td_upload(select_db, output3[output3['Calendar Week Year'].isin(exclude_list)].astype({'SKU ID':'str'}), 'SELLOUT_REGION_SKU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Backup When Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_backup(_base, 'BEK.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
