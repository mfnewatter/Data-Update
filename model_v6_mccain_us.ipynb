{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCain Sell-In\n",
    "### 1. Libraries and constants\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teradatasql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "#path where dictionary file can be found\n",
    "#Neil\n",
    "DICTIONARY = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Distributor Sell-Out Dictionaries\\\\'\n",
    "#Joe\n",
    "#DICTIONARY = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Distributor Sell-Out Dictionaries\\\\'\n",
    "\n",
    "#main path\n",
    "#Neil\n",
    "PATH = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\\\'\n",
    "#Joe\n",
    "#PATH = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\\\'\n",
    "\n",
    "#current fiscal year and week (YYYYWW)\n",
    "TIME = pd.read_excel(DICTIONARY + 'Time Definitions.xlsx')\n",
    "\n",
    "#the current week is pulled from the time dictionary table\n",
    "WEEK = int(TIME[(TIME['Week Starting (Mon)'] <= dt.now()) & (TIME['Week Ending (Sun)'] >= dt.now())]['Calendar Week Year'].values)\n",
    "\n",
    "#WEEK = 202218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculation Functions\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling(df, _list):\n",
    "    #groupby _list\n",
    "    df = df.groupby(_list, dropna = False)[['LBS','LBS_LY','LBS_Baseline']].sum().reset_index()\n",
    "    \n",
    "    #set index to all but last column in list\n",
    "    df = df.set_index(_list)\n",
    "    \n",
    "    #add new metric SMA_4 (simple moving average - 4 periods)\n",
    "    #level = all but last 2 items in list\n",
    "    df['LBS_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    df['LBS_Lag_2'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 2)\n",
    "    df['LBS_Lag_3'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 3)\n",
    "    df['LBS_Lag_4'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 4)\n",
    "    \n",
    "    df['SMA_4'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['LBS_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].shift(periods = 1)\n",
    "    df['LBS_LY_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    \n",
    "    df['SMA_4_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4'].shift(periods = 1)\n",
    "    df['SMA_4_LY_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_LY'].shift(periods = 1)\n",
    "    df['SMA_4_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_Baseline'].shift(periods = 1)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "def add_last_year(df, _list):\n",
    "    #list of groupby columns\n",
    "    #last item in list is Calendar Week Year which is used to pull previous history (Baseline Week = Calendar Week Year) of copied dataframe\n",
    "    _groupby = _list.copy()\n",
    "    \n",
    "    _merge_yoy = _list.copy()[0:-1]\n",
    "    _merge_yoy.extend(['YOY Week'])\n",
    "    \n",
    "    _merge_baseline = _list.copy()[0:-1]\n",
    "    _merge_baseline.extend(['Baseline Week'])\n",
    "    \n",
    "    df1 = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #groupby _list\n",
    "    df_new = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #add week dimensions to main dataframe\n",
    "    df_new = df_new.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_yoy, right_on=_groupby).drop(columns={'Calendar Week Year_y'}).rename(columns={'LBS_y':'LBS_LY'})\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_baseline, right_on=_groupby).drop(columns={'Calendar Week Year'}).rename(columns={\n",
    "        'LBS':'LBS_Baseline','Calendar Week Year_x':'Calendar Week Year','LBS_x':'LBS'})\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def add_precovid(df, _list, begin, end):\n",
    "    #datefield should be last in _list\n",
    "    datefield = _list[-1]\n",
    "          \n",
    "    #remove datefield from list\n",
    "    _list = _list[0:-1]\n",
    "    \n",
    "    #filter data not using last and rename columns\n",
    "    _df = df[(df[datefield] >= begin) & (df[datefield] <= end)].groupby(_list)['LBS'].sum() / 52\n",
    "    \n",
    "    return df.merge(\n",
    "        _df, how = 'left', left_on = _list, right_on = _list).rename(\n",
    "        columns = {'LBS_x':'LBS', 'LBS_y':'LBS_PRECOVID'}).fillna(\n",
    "        value = {'LBS_PRECOVID': 0})\n",
    "\n",
    "\n",
    "def add_time(df):\n",
    "    df = df.merge(TIME[['Calendar Week Year','Week Starting (Sun)','Week Ending (Sat)', 'COVID Week']],\n",
    "                   how = 'left', \n",
    "                   on = 'Calendar Week Year')\n",
    "    \n",
    "    df = df.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_weight(df, _list):\n",
    "    test = df.groupby(_list)[['LBS','LBS_Baseline']].sum().reset_index()\n",
    "    test['Wt'] = test['LBS'] / test.groupby(['Calendar Week Year'])['LBS'].transform('sum')\n",
    "    \n",
    "    return test['Wt']\n",
    "\n",
    "def analyze_1(df, _list, begin, end):\n",
    "    if 'Calendar Week Year' not in _list:\n",
    "        _list.extend(['Calendar Week Year'])\n",
    "    \n",
    "    df = full_dataframe(df, _list)\n",
    "    \n",
    "    #add last year lbs\n",
    "    df = add_last_year(df, _list)\n",
    "    \n",
    "    #add rolling calculation\n",
    "    df = add_rolling(df, _list)\n",
    "    \n",
    "    #add preCOVID baseline\n",
    "    df = add_precovid(df, _list, begin, end)\n",
    "    \n",
    "    if _list[0] == 'Brand Desc':\n",
    "        df['Wt'] = add_weight(df, _list)\n",
    "    \n",
    "    df = df.round({\n",
    "        'LBS' : 2,    \n",
    "        'SMA_4' : 2,\n",
    "        'SMA_8' : 2,\n",
    "        'SMA_12' : 2,\n",
    "        'LBS_LY' : 2,    \n",
    "        'SMA_4_LY' : 2,\n",
    "        'SMA_8_LY' : 2,\n",
    "        'SMA_12_LY' : 2,\n",
    "        'LBS_Baseline' : 2,    \n",
    "        'SMA_4_Baseline' : 2,\n",
    "        'SMA_8_Baseline' : 2,\n",
    "        'SMA_12_Baseline' : 2,\n",
    "        'LBS_PRECOVID' : 2,\n",
    "        'LBS_Lag_1' : 2,\n",
    "        'LBS_Lag_2' : 2,\n",
    "        'LBS_Lag_3' : 2,\n",
    "        'LBS_Lag_4' : 2,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 2,\n",
    "        'SMA_4_LY_Lag_1' : 2,\n",
    "        'SMA_4_Baseline_Lag_1' : 2\n",
    "        \n",
    "    }).fillna(value = {\n",
    "        'LBS' : 0,    \n",
    "        'SMA_4' : 0,\n",
    "        'SMA_8' : 0,\n",
    "        'SMA_12' : 0,\n",
    "        'LBS_LY' : 0,    \n",
    "        'SMA_4_LY' : 0,\n",
    "        'SMA_8_LY' : 0,\n",
    "        'SMA_12_LY' : 0,\n",
    "        'LBS_Baseline' : 0,    \n",
    "        'SMA_4_Baseline' : 0,\n",
    "        'SMA_8_Baseline' : 0,\n",
    "        'SMA_12_Baseline' : 0,\n",
    "        'LBS_PRECOVID' : 0,\n",
    "        'LBS_Lag_1' : 0,\n",
    "        'LBS_Lag_2' : 0,\n",
    "        'LBS_Lag_3' : 0,\n",
    "        'LBS_Lag_4' : 0,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 0,\n",
    "        'SMA_4_LY_Lag_1' : 0,\n",
    "        'SMA_4_Baseline_Lag_1' : 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_backup(df, file_name):\n",
    "    \n",
    "    df.to_csv(BACKUP + file_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def td_to_pandas(query, cur, title=''):\n",
    "    _data = []\n",
    "    _start=dt.now()\n",
    "    print(dt.now().strftime('%m/%d/%Y %r'))\n",
    "    print(f'{title} Execution started...', end='', flush=True)\n",
    "    cur.execute (query)\n",
    "    print(f'finished. {dt.now() - _start}', flush=True) \n",
    "    _start_fetch=dt.now()\n",
    "    print(f'{title} Fetching data started...', end='', flush=True)\n",
    "    for row in cur.fetchall():\n",
    "        _data.append(row) \n",
    "    print(f'finished. {dt.now() - _start_fetch}', flush=True) \n",
    "    _start=dt.now()\n",
    "    print(f'{title} Creating DataFrame for started...', end='', flush=True)\n",
    "    _df = pd.DataFrame(_data)\n",
    "    _df.columns = [x[0].replace('SAP_', '').lower() for x in cur.description]\n",
    "    print(f'finished. {dt.now() - _start}', flush=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def td_dataframe(select_db, query):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            print('Database selected!', flush=True)            \n",
    "            dim_df = td_to_pandas(query, cur, 'Query:')\n",
    "            print('Dim:', dim_df.shape)\n",
    "    \n",
    "    return dim_df\n",
    "\n",
    "\n",
    "def process_list(df, work_list):\n",
    "    \n",
    "    _process = analyze_1(df, work_list, 201910, 202009)\n",
    "    \n",
    "    _process['Country'] = 'US'\n",
    "    \n",
    "    _process = add_time(_process)\n",
    "    \n",
    "    #for standardizing output\n",
    "    work_list.extend(['Country','LBS','SMA_4','SMA_8','SMA_12',\n",
    "                      'YOY Week','LBS_LY','SMA_4_LY','SMA_8_LY','SMA_12_LY',\n",
    "                      'Baseline Week','LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "                      'LBS_Lag_1','LBS_Lag_2','LBS_Lag_3','LBS_Lag_4','LBS_Baseline_Lag_1','LBS_LY_Lag_1',\n",
    "                      'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "                      'LBS_PRECOVID','Week Starting (Sun)','Week Ending (Sat)','COVID Week'])\n",
    "    \n",
    "    if work_list[0] == 'Brand Desc':\n",
    "        work_list.extend(['Wt'])\n",
    "    \n",
    "    return _process[work_list]\n",
    "\n",
    "\n",
    "def full_dataframe(df, _list):\n",
    "    weeks = df.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})\n",
    "    segments = df.groupby(_list[0:-1]).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    _df = segments.assign(key=1).merge(weeks.assign(key=1), how='outer', on='key').drop(columns = {'key'}) \n",
    "    \n",
    "    return _df.merge(df, how = 'left', on = _list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Teradata Queries\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teradata_sales(WEEK):\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Sysco COVID Performance;StartTime=20200901T131109;JobID=68215;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "        select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "            (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "            a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "            (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "            RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "            a16.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "            a13.DIVISION_ID as DIVISION,\n",
    "            a17.DIVISION_NAME as DIVISION_NAME,\n",
    "            a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "            a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "            a12.SUB_CATEGORY_SHORT_CODE as SUB_CATEGORY_SHORT_CODE,\n",
    "            a12.SUB_CATEGORY_DESC as SUB_CATEGORY_DESC,\n",
    "            a15.MATERIAL_PRICING_GROUP_ID as MATERIAL_PRICING_GROUP_ID,\n",
    "            a18.MATERIAL_PRICING_GROUP_DESCRIPTION as MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "            TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "            a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "            sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "        from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "        join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "        on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "        join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "        on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "        join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "        on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "        join DL_GBL_TAS_BI.D_MATERIAL_SALES_DATA as a15\n",
    "        on (a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL_ID and \n",
    "        a11.MATERIAL_ID = a15.MATERIAL_ID and \n",
    "        a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION_ID)\n",
    "        join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a16\n",
    "        on (a11.CUSTOMER_ID = a16.CUSTOMER and \n",
    "        a11.DISTRIBUTION_CHANNEL_ID = a16.DISTRIBUTION_CHANNEL and \n",
    "        a11.SALES_ORGANISATION_ID = a16.SALES_ORGANISATION)\n",
    "        join DL_GBL_TAS_BI.D_DIVISION as a17\n",
    "        on (a13.DIVISION_ID = a17.DIVISION_ID)\n",
    "        join DL_GBL_TAS_BI.D_MATERIAL_PRICING_GROUP as a18\n",
    "        on (a15.MATERIAL_PRICING_GROUP_ID = a18.MATERIAL_PRICING_GROUP_ID)\n",
    "        left join DL_GBL_TAS_BI.FACT_OM_ORDER_FULFILLMENT as a19\n",
    "        on (a11.SALES_ORDER_ID = a19.SALES_ORDER_ID) and (a11.MATERIAL_ID = a19.MATERIAL_ID)\n",
    "        where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "        and a11.SALES_ORGANISATION_ID in ('US01')\n",
    "        and a11.DISTRIBUTION_CHANNEL_ID in ('10'))\n",
    "        and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(WEEK - 1) + ''' \n",
    "        group by a14.FISCAL_WEEK_NUMBER,\n",
    "        (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "        a14.CALENDAR_WEEK_NAME,\n",
    "        (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        a16.CUSTOMER_HIER_LVL_1_NAME,\n",
    "        a13.DIVISION_ID,\n",
    "        a17.DIVISION_NAME,\n",
    "        a12.CATEGORY_SHORT_CODE,\n",
    "        a12.CATEGORY_DESC,\n",
    "        a12.SUB_CATEGORY_SHORT_CODE,\n",
    "        a12.SUB_CATEGORY_DESC,\n",
    "        a15.MATERIAL_PRICING_GROUP_ID,\n",
    "        a18.MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "        TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "        a13.MATERIAL_DESCRIPTION\n",
    "        '''\n",
    "    \n",
    "    #build dataframe from teradata query\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    #return transformed dataframe\n",
    "    return transform_teradata(df)\n",
    "\n",
    "\n",
    "def teradata_brand(WEEK):\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Brand COVID Performance;StartTime=20200901T113649;JobID=55922;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "    a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "    RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "    a16.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a13.DIVISION_ID as DIVISION,\n",
    "    a17.DIVISION_NAME as DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "    a15.MATERIAL_PRICING_GROUP_ID as MATERIAL_PRICING_GROUP_ID,\n",
    "    a18.MATERIAL_PRICING_GROUP_DESCRIPTION as MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "    a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "    a12.BRAND_SHORT_CODE as BRAND_SHORT_CODE,\n",
    "    a12.BRAND_DESC as BRAND_DESC,\n",
    "    sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "    on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "    on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "    on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_SALES_DATA as a15\n",
    "    on (a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL_ID and \n",
    "    a11.MATERIAL_ID = a15.MATERIAL_ID and \n",
    "    a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION_ID)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a16\n",
    "    on (a11.CUSTOMER_ID = a16.CUSTOMER and \n",
    "    a11.DISTRIBUTION_CHANNEL_ID = a16.DISTRIBUTION_CHANNEL and \n",
    "    a11.SALES_ORGANISATION_ID = a16.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a17\n",
    "    on (a13.DIVISION_ID = a17.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_PRICING_GROUP as a18\n",
    "    on (a15.MATERIAL_PRICING_GROUP_ID = a18.MATERIAL_PRICING_GROUP_ID)\n",
    "    where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "    and a11.SALES_ORGANISATION_ID in ('US01')\n",
    "    and a11.DISTRIBUTION_CHANNEL_ID in ('10')\n",
    "    and a12.BRAND_SHORT_CODE in ('002', '005', '042', '536', '544', '545', '638', '659', '688', '694', '093'))\n",
    "    and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(WEEK - 1) + ''' \n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    a14.CALENDAR_WEEK_NAME,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "    a16.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a13.DIVISION_ID,\n",
    "    a17.DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC,\n",
    "    a15.MATERIAL_PRICING_GROUP_ID,\n",
    "    a18.MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "    a13.MATERIAL_DESCRIPTION,\n",
    "    a12.BRAND_SHORT_CODE,\n",
    "    a12.BRAND_DESC\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return transform_teradata(df)\n",
    "\n",
    "\n",
    "def transform_teradata(df):\n",
    "\n",
    "    #update category_desc values based on row qualifiers\n",
    "    df['Consolidated Category'] = df['category_desc']\n",
    "    df.loc[df['Consolidated Category'] == 'Sweet Potato' , 'Consolidated Category'] = 'Potato'\n",
    "    df.loc[df['Consolidated Category'] != 'Potato' , 'Consolidated Category'] = 'Prepared Foods'\n",
    "    \n",
    "    #update calendar_week_name to numeric for future functions\n",
    "    df['calendar_week_number'] = pd.to_numeric(df['calendar_week_number'], errors = 'coerce')\n",
    "    \n",
    "    df = df.astype({'actual_volume_lbs':'float64'})\n",
    "\n",
    "    df = df.rename(columns={'actual_volume_lbs':'LBS',\n",
    "                          'calendar_week_number':'Calendar Week Year',\n",
    "                          'division_name':'Division Name',\n",
    "                          'material_pricing_group_description':'Material Pricing Group Description',\n",
    "                          'customer_hier_lvl_1_name':'Customer L1 Name',\n",
    "                          'brand_desc':'Brand Desc'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execute Analysis\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected!\n",
      "06/15/2022 12:49:48 PM\n",
      "Query: Execution started...finished. 0:06:58.113671\n",
      "Query: Fetching data started...finished. 0:05:16.633352\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.693399\n",
      "Dim: (523583, 17)\n",
      "Database selected!\n",
      "06/15/2022 01:02:15 PM\n",
      "Query: Execution started...finished. 0:03:08.262411\n",
      "Query: Fetching data started...finished. 0:00:04.137403\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.027788\n",
      "Dim: (19526, 17)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#pull sales data from teradata\n",
    "_sales = teradata_sales(WEEK)\n",
    "\n",
    "#blank list to add lists to\n",
    "_list = []\n",
    "\n",
    "#Output 1: Division Name - List 0\n",
    "_list.append(['Division Name','Consolidated Category'])\n",
    "\n",
    "#Output 2: MPG - List 1\n",
    "_list.append(['Material Pricing Group Description', 'Consolidated Category'])\n",
    "\n",
    "#Output 3: Customer L1 - List 2\n",
    "_list.append(['Division Name','Customer L1 Name'])\n",
    "\n",
    "#Create dataframes\n",
    "output1 = process_list(_sales, _list[0])\n",
    "output2 = process_list(_sales, _list[1])\n",
    "output3 = process_list(_sales, _list[2])\n",
    "\n",
    "#pull sales data by brand from teradata\n",
    "#seperate from the query above becaues of filters\n",
    "_brand = teradata_brand(WEEK)\n",
    "\n",
    "#Output 4: Brand - List 3\n",
    "_list.append(['Brand Desc'])\n",
    "\n",
    "output4 = process_list(_brand, _list[3])\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Teradata Update\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected! 06/15/2022 13:05:30 PM\n",
      "Deleting records for: US in table: SELLIN_DIVISION\n",
      "Inserting records into SELLIN_DIVISION\n",
      "Inserted 1980 records\n",
      "Database selected! 06/15/2022 13:05:35 PM\n",
      "Deleting records for: US in table: SELLIN_MPG\n",
      "Inserting records into SELLIN_MPG\n",
      "Inserted 7020 records\n",
      "Database selected! 06/15/2022 13:05:44 PM\n",
      "Deleting records for: US in table: SELLIN_CUSTOMER_L1\n",
      "Inserting records into SELLIN_CUSTOMER_L1\n",
      "Inserted 36000 records\n",
      "Database selected! 06/15/2022 13:06:31 PM\n",
      "Deleting records for: US in table: SELLIN_BRAND\n",
      "Inserting records into SELLIN_BRAND\n",
      "Inserted 1980 records\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "def td_upload(select_db, df, table_name):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            d = dt.now().strftime('%m/%d/%Y %H:%M:%S %p')\n",
    "            print(f'Database selected! {d}', flush=True)           \n",
    "\n",
    "            delete_from_td(df, table_name, cur)\n",
    "            insert_into_td(df, table_name, cur)\n",
    "\n",
    "def delete_from_td(df, table_name, cur):\n",
    "    distributor = df.groupby('Country').size().reset_index().drop(columns=0).to_numpy()[0][0]\n",
    "    \n",
    "    print(f'Deleting records for: {distributor} in table: {table_name}', flush = True)          \n",
    "        \n",
    "    query = '''\n",
    "    DELETE FROM ''' + table_name  + ''' \n",
    "    WHERE \"Country\" = ''' + \"'\" + distributor + \"'\"\n",
    "    \n",
    "    cur.execute (query)\n",
    "    \n",
    "def insert_into_td(df, table_name, cur):\n",
    "    insert_list = df.values.tolist()\n",
    "    \n",
    "    #creates ?, ?,.... string used in query for teradata fastload\n",
    "    insert_columns = ('?, ' * len(df.columns)).rstrip(', ')\n",
    "\n",
    "    print(f'Inserting records into {table_name}', flush = True)\n",
    "    \n",
    "    query = \"INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    #query = \"{fn teradata_try_fastload}INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    \n",
    "    cur.execute (query, insert_list)\n",
    "    \n",
    "    print(f'Inserted {df.shape[0]} records', flush = True)\n",
    "    \n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "td_upload(select_db, output1, 'SELLIN_DIVISION')\n",
    "td_upload(select_db, output2, 'SELLIN_MPG')\n",
    "td_upload(select_db, output3, 'SELLIN_CUSTOMER_L1')\n",
    "td_upload(select_db, output4, 'SELLIN_BRAND')\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Output to Excel\n",
    "Only used for Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: SELLIN_DIVISION\n",
      "Database selected!\n",
      "06/15/2022 04:53:33 PM\n",
      "Query: Execution started...finished. 0:00:01.252876\n",
      "Query: Fetching data started...finished. 0:00:00.400627\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.019052\n",
      "Dim: (3246, 31)\n",
      "Exporting table: SELLIN_MPG\n",
      "Database selected!\n",
      "06/15/2022 04:53:38 PM\n",
      "Query: Execution started...finished. 0:00:02.313674\n",
      "Query: Fetching data started...finished. 0:00:00.802662\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.042278\n",
      "Dim: (7020, 31)\n",
      "Exporting table: SELLIN_BRAND\n",
      "Database selected!\n",
      "06/15/2022 04:53:46 PM\n",
      "Query: Execution started...finished. 0:00:03.669617\n",
      "Query: Fetching data started...finished. 0:00:00.405549\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.008825\n",
      "Dim: (3760, 31)\n",
      "Exporting table: SELLIN_RETAIL\n",
      "Database selected!\n",
      "06/15/2022 04:53:56 PM\n",
      "Query: Execution started...finished. 0:00:02.839584\n",
      "Query: Fetching data started...finished. 0:00:00.201099\n",
      "Query: Creating DataFrame for started...finished. 0:00:00\n",
      "Dim: (2012, 30)\n",
      "Exporting table: SELLIN_CUSTOMER_L1\n",
      "Database selected!\n",
      "06/15/2022 04:54:03 PM\n",
      "Query: Execution started...finished. 0:00:05.201516\n",
      "Query: Fetching data started...finished. 0:00:23.571917\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.221935\n",
      "Dim: (36000, 31)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "table_list = ['SELLIN_DIVISION','SELLIN_MPG','SELLIN_BRAND','SELLIN_RETAIL','SELLIN_CUSTOMER_L1']\n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(PATH + 'Weekly Sellin Data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for table_name in table_list:\n",
    "    print(f'Exporting table: {table_name}', flush = True)\n",
    "    query = '''\n",
    "    SELECT * FROM ''' + table_name + '''\n",
    "    '''\n",
    "    df = td_dataframe(select_db, query)\n",
    "    df.to_excel(writer, sheet_name = table_name, index = False)\n",
    "    \n",
    "writer.save()\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected!\n",
      "06/15/2022 04:54:58 PM\n",
      "Query: Execution started...finished. 0:03:10.158272\n",
      "Query: Fetching data started...finished. 0:02:40.679493\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.400510\n",
      "Dim: (444908, 12)\n"
     ]
    }
   ],
   "source": [
    "def teradata_fillrate():\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Brand COVID Performance;StartTime=20200901T113649;JobID=55922;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    SELECT\n",
    "    a1.SALES_ORGANISATION_ID,\n",
    "    a3.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_L1,\n",
    "    a3.CUSTOMER_HIER_LVL_2_NAME as CUSTOMER_L2,\n",
    "    a2.DIVISION_DESCRIPTION,\n",
    "    a2.CATEGORY_DESC,\n",
    "    a2.PRODUCT_GROUP_FORMAT_DESC,\n",
    "    a2.PRODUCT_GROUP_SUB_FORMAT_DESC,\n",
    "    a2.MATERIAL_ID,\n",
    "    a2.MATERIAL_DESCRIPTION,\n",
    "    TD_WEEK_END(a1.ACTUAL_OR_PLANNED_PGI_DATE)+1 as WEEK_ENDING,\n",
    "    sum(cast(a1.SUM_ORDER_QUANTITY as float)) as ORDERED_CASES,\n",
    "    sum(cast(a1.DELIVERED_QUANTITY as float)) as DELIVERED_CASES\n",
    "    FROM DL_GBL_TAS_BI.FACT_OM_ORDER_FULFILLMENT as a1\n",
    "    JOIN DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a2\n",
    "        on a1.MATERIAL_ID = a2.MATERIAL_ID\n",
    "    JOIN DL_GBL_TAS_BI.H_CUSTOMER as a3\n",
    "        on a1.SOLD_TO_CUSTOMER_ID = a3.CUSTOMER_ID\n",
    "        and a1.SALES_ORGANISATION_ID = a3.SALES_ORGANISATION\n",
    "        and a1.DISTRIBUTION_CHANNEL_ID = a3.DISTRIBUTION_CHANNEL\n",
    "    WHERE a1.PGI_COMPLETE = 'PGIED ORDER'\n",
    "    and a1.DOCUMENT_TYPE = 'ZOR'\n",
    "    and a1.DISTRIBUTION_CHANNEL_ID = 10\n",
    "    and a1.SALES_ORGANISATION_ID in ('US01', 'CA01')\n",
    "    and a1.ACTUAL_OR_PLANNED_PGI_DATE > '2021-01-01'\n",
    "\n",
    "    GROUP BY\n",
    "    a1.SALES_ORGANISATION_ID,\n",
    "    a3.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a3.CUSTOMER_HIER_LVL_2_NAME,\n",
    "    a2.DIVISION_DESCRIPTION,\n",
    "    a2.CATEGORY_DESC,\n",
    "    a2.PRODUCT_GROUP_FORMAT_DESC,\n",
    "    a2.PRODUCT_GROUP_SUB_FORMAT_DESC,\n",
    "    a2.MATERIAL_ID,\n",
    "    a2.MATERIAL_DESCRIPTION,\n",
    "    TD_WEEK_END(a1.ACTUAL_OR_PLANNED_PGI_DATE)+1\n",
    "    ;'''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return df\n",
    "\n",
    "fill_rate = teradata_fillrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_organisation_id</th>\n",
       "      <th>customer_l1</th>\n",
       "      <th>customer_l2</th>\n",
       "      <th>division_description</th>\n",
       "      <th>category_desc</th>\n",
       "      <th>product_group_format_desc</th>\n",
       "      <th>product_group_sub_format_desc</th>\n",
       "      <th>material_id</th>\n",
       "      <th>material_description</th>\n",
       "      <th>week_ending</th>\n",
       "      <th>ordered_cases</th>\n",
       "      <th>delivered_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US01</td>\n",
       "      <td>BOR NATIONAL L1</td>\n",
       "      <td>BOR US - CENTRAL L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Potato</td>\n",
       "      <td>SPECIALTY</td>\n",
       "      <td>CUT</td>\n",
       "      <td>OIF252A</td>\n",
       "      <td>FS OREIDA DICED RNDM CT HB 6X5</td>\n",
       "      <td>2021-07-04</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US01</td>\n",
       "      <td>ENDICO L1</td>\n",
       "      <td>ENDICO L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Potato</td>\n",
       "      <td>CONVENTIONAL FRIES</td>\n",
       "      <td>FROZEN FRIES</td>\n",
       "      <td>000000001000000529</td>\n",
       "      <td>DD_END 6/5 3/8\" STRAIGHT CUT A GRADE FRY</td>\n",
       "      <td>2021-10-10</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US01</td>\n",
       "      <td>CASH WA DISTRIBUTING L1</td>\n",
       "      <td>CASH WA DISTRIBUTING L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>APPETIZERS</td>\n",
       "      <td>WHOLE VEGETABLE</td>\n",
       "      <td>000000000080008473</td>\n",
       "      <td>GCP BAT WHLE MSHRMS 6X2LB</td>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US01</td>\n",
       "      <td>KROGER L1</td>\n",
       "      <td>KROGER L2</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>LOCAL PORTFOLIO</td>\n",
       "      <td>LOCAL PORTFOLIO OTHER</td>\n",
       "      <td>000000001000007301</td>\n",
       "      <td>RT KRO BCN TWBE POT 8X10OZ</td>\n",
       "      <td>2021-07-25</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US01</td>\n",
       "      <td>CASH WA DISTRIBUTING L1</td>\n",
       "      <td>CASH WA DISTRIBUTING L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>APPETIZERS</td>\n",
       "      <td>ONION RINGS</td>\n",
       "      <td>000000000070010011</td>\n",
       "      <td>BCI 5/8 BRB THCK ON RINGS 6X2.5LB</td>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sales_organisation_id              customer_l1              customer_l2  \\\n",
       "0          US01                  BOR NATIONAL L1      BOR US - CENTRAL L2   \n",
       "1          US01                        ENDICO L1                ENDICO L2   \n",
       "2          US01          CASH WA DISTRIBUTING L1  CASH WA DISTRIBUTING L2   \n",
       "3          US01                        KROGER L1                KROGER L2   \n",
       "4          US01          CASH WA DISTRIBUTING L1  CASH WA DISTRIBUTING L2   \n",
       "\n",
       "  division_description   category_desc product_group_format_desc  \\\n",
       "0         Food Service          Potato                 SPECIALTY   \n",
       "1         Food Service          Potato        CONVENTIONAL FRIES   \n",
       "2         Food Service  Prepared Foods                APPETIZERS   \n",
       "3               Retail  Prepared Foods           LOCAL PORTFOLIO   \n",
       "4         Food Service  Prepared Foods                APPETIZERS   \n",
       "\n",
       "  product_group_sub_format_desc         material_id  \\\n",
       "0                           CUT             OIF252A   \n",
       "1                  FROZEN FRIES  000000001000000529   \n",
       "2               WHOLE VEGETABLE  000000000080008473   \n",
       "3         LOCAL PORTFOLIO OTHER  000000001000007301   \n",
       "4                   ONION RINGS  000000000070010011   \n",
       "\n",
       "                       material_description week_ending  ordered_cases  \\\n",
       "0            FS OREIDA DICED RNDM CT HB 6X5  2021-07-04           45.0   \n",
       "1  DD_END 6/5 3/8\" STRAIGHT CUT A GRADE FRY  2021-10-10         1150.0   \n",
       "2                 GCP BAT WHLE MSHRMS 6X2LB  2021-04-04           60.0   \n",
       "3                RT KRO BCN TWBE POT 8X10OZ  2021-07-25         1104.0   \n",
       "4         BCI 5/8 BRB THCK ON RINGS 6X2.5LB  2021-08-15          480.0   \n",
       "\n",
       "   delivered_cases  \n",
       "0             45.0  \n",
       "1           1150.0  \n",
       "2             60.0  \n",
       "3             74.0  \n",
       "4              0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444908 entries, 0 to 444907\n",
      "Data columns (total 12 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   sales_organisation_id          444908 non-null  object        \n",
      " 1   customer_l1                    444908 non-null  object        \n",
      " 2   customer_l2                    444908 non-null  object        \n",
      " 3   division_description           444908 non-null  object        \n",
      " 4   category_desc                  444908 non-null  object        \n",
      " 5   product_group_format_desc      444908 non-null  object        \n",
      " 6   product_group_sub_format_desc  444908 non-null  object        \n",
      " 7   material_id                    444908 non-null  object        \n",
      " 8   material_description           444908 non-null  object        \n",
      " 9   week_ending                    444908 non-null  datetime64[ns]\n",
      " 10  ordered_cases                  444908 non-null  float64       \n",
      " 11  delivered_cases                406129 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(9)\n",
      "memory usage: 40.7+ MB\n"
     ]
    }
   ],
   "source": [
    "fill_rate = fill_rate.astype({'week_ending':'datetime64'})\n",
    "\n",
    "fill_rate.loc[fill_rate['category_desc'].str.contains('Potato'), 'category_desc'] = 'Potato'\n",
    "fill_rate.loc[~fill_rate['category_desc'].str.contains('Potato'), 'category_desc'] = 'Prepared Foods'\n",
    "\n",
    "display(fill_rate.head())\n",
    "\n",
    "fill_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_rate.to_csv('FILL_RATE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
