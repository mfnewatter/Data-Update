{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCain Sell-In\n",
    "### 1. Libraries and constants\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teradatasql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "#path where dictionary file can be found\n",
    "#Neil\n",
    "DICTIONARY = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Distributor Sell-Out Dictionaries\\\\'\n",
    "#Joe\n",
    "#DICTIONARY = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Distributor Sell-Out Dictionaries\\\\'\n",
    "\n",
    "#main path\n",
    "#Neil\n",
    "PATH = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\\\'\n",
    "#Joe\n",
    "#PATH = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\\\'\n",
    "\n",
    "#current fiscal year and week (YYYYWW)\n",
    "TIME = pd.read_excel(DICTIONARY + 'Time Definitions.xlsx')\n",
    "\n",
    "#the current week is pulled from the time dictionary table\n",
    "WEEK = int(TIME[(TIME['Week Starting (Mon)'] <= dt.now()) & (TIME['Week Ending (Sun)'] >= dt.now())]['Calendar Week Year'].values)\n",
    "\n",
    "#WEEK = 202218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculation Functions\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling(df, _list):\n",
    "    #groupby _list\n",
    "    df = df.groupby(_list, dropna = False)[['LBS','LBS_LY','LBS_Baseline']].sum().reset_index()\n",
    "    \n",
    "    #set index to all but last column in list\n",
    "    df = df.set_index(_list)\n",
    "    \n",
    "    #add new metric SMA_4 (simple moving average - 4 periods)\n",
    "    #level = all but last 2 items in list\n",
    "    df['LBS_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    df['LBS_Lag_2'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 2)\n",
    "    df['LBS_Lag_3'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 3)\n",
    "    df['LBS_Lag_4'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 4)\n",
    "    \n",
    "    df['SMA_4'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['LBS_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].shift(periods = 1)\n",
    "    df['LBS_LY_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    \n",
    "    df['SMA_4_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4'].shift(periods = 1)\n",
    "    df['SMA_4_LY_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_LY'].shift(periods = 1)\n",
    "    df['SMA_4_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_Baseline'].shift(periods = 1)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "def add_last_year(df, _list):\n",
    "    #list of groupby columns\n",
    "    #last item in list is Calendar Week Year which is used to pull previous history (Baseline Week = Calendar Week Year) of copied dataframe\n",
    "    _groupby = _list.copy()\n",
    "    \n",
    "    _merge_yoy = _list.copy()[0:-1]\n",
    "    _merge_yoy.extend(['YOY Week'])\n",
    "    \n",
    "    _merge_baseline = _list.copy()[0:-1]\n",
    "    _merge_baseline.extend(['Baseline Week'])\n",
    "    \n",
    "    df1 = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #groupby _list\n",
    "    df_new = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #add week dimensions to main dataframe\n",
    "    df_new = df_new.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_yoy, right_on=_groupby).drop(columns={'Calendar Week Year_y'}).rename(columns={'LBS_y':'LBS_LY'})\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_baseline, right_on=_groupby).drop(columns={'Calendar Week Year'}).rename(columns={\n",
    "        'LBS':'LBS_Baseline','Calendar Week Year_x':'Calendar Week Year','LBS_x':'LBS'})\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def add_precovid(df, _list, begin, end):\n",
    "    #datefield should be last in _list\n",
    "    datefield = _list[-1]\n",
    "          \n",
    "    #remove datefield from list\n",
    "    _list = _list[0:-1]\n",
    "    \n",
    "    #filter data not using last and rename columns\n",
    "    _df = df[(df[datefield] >= begin) & (df[datefield] <= end)].groupby(_list)['LBS'].sum() / 52\n",
    "    \n",
    "    return df.merge(\n",
    "        _df, how = 'left', left_on = _list, right_on = _list).rename(\n",
    "        columns = {'LBS_x':'LBS', 'LBS_y':'LBS_PRECOVID'}).fillna(\n",
    "        value = {'LBS_PRECOVID': 0})\n",
    "\n",
    "\n",
    "def add_time(df):\n",
    "    df = df.merge(TIME[['Calendar Week Year','Week Starting (Mon)','Week Ending (Sun)', 'COVID Week']],\n",
    "                   how = 'left', \n",
    "                   on = 'Calendar Week Year')\n",
    "    \n",
    "    df = df.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_weight(df, _list):\n",
    "    test = df.groupby(_list)[['LBS','LBS_Baseline']].sum().reset_index()\n",
    "    test['Wt'] = test['LBS'] / test.groupby(['Calendar Week Year'])['LBS'].transform('sum')\n",
    "    \n",
    "    return test['Wt']\n",
    "\n",
    "def analyze_1(df, _list, begin, end):\n",
    "    if 'Calendar Week Year' not in _list:\n",
    "        _list.extend(['Calendar Week Year'])\n",
    "    \n",
    "    df = full_dataframe(df, _list)\n",
    "    \n",
    "    #add last year lbs\n",
    "    df = add_last_year(df, _list)\n",
    "    \n",
    "    #add rolling calculation\n",
    "    df = add_rolling(df, _list)\n",
    "    \n",
    "    #add preCOVID baseline\n",
    "    df = add_precovid(df, _list, begin, end)\n",
    "    \n",
    "    if _list[0] == 'Brand Desc':\n",
    "        df['Wt'] = add_weight(df, _list)\n",
    "    \n",
    "    df = df.round({\n",
    "        'LBS' : 2,    \n",
    "        'SMA_4' : 2,\n",
    "        'SMA_8' : 2,\n",
    "        'SMA_12' : 2,\n",
    "        'LBS_LY' : 2,    \n",
    "        'SMA_4_LY' : 2,\n",
    "        'SMA_8_LY' : 2,\n",
    "        'SMA_12_LY' : 2,\n",
    "        'LBS_Baseline' : 2,    \n",
    "        'SMA_4_Baseline' : 2,\n",
    "        'SMA_8_Baseline' : 2,\n",
    "        'SMA_12_Baseline' : 2,\n",
    "        'LBS_PRECOVID' : 2,\n",
    "        'LBS_Lag_1' : 2,\n",
    "        'LBS_Lag_2' : 2,\n",
    "        'LBS_Lag_3' : 2,\n",
    "        'LBS_Lag_4' : 2,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 2,\n",
    "        'SMA_4_LY_Lag_1' : 2,\n",
    "        'SMA_4_Baseline_Lag_1' : 2\n",
    "        \n",
    "    }).fillna(value = {\n",
    "        'LBS' : 0,    \n",
    "        'SMA_4' : 0,\n",
    "        'SMA_8' : 0,\n",
    "        'SMA_12' : 0,\n",
    "        'LBS_LY' : 0,    \n",
    "        'SMA_4_LY' : 0,\n",
    "        'SMA_8_LY' : 0,\n",
    "        'SMA_12_LY' : 0,\n",
    "        'LBS_Baseline' : 0,    \n",
    "        'SMA_4_Baseline' : 0,\n",
    "        'SMA_8_Baseline' : 0,\n",
    "        'SMA_12_Baseline' : 0,\n",
    "        'LBS_PRECOVID' : 0,\n",
    "        'LBS_Lag_1' : 0,\n",
    "        'LBS_Lag_2' : 0,\n",
    "        'LBS_Lag_3' : 0,\n",
    "        'LBS_Lag_4' : 0,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 0,\n",
    "        'SMA_4_LY_Lag_1' : 0,\n",
    "        'SMA_4_Baseline_Lag_1' : 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_backup(df, file_name):\n",
    "    \n",
    "    df.to_csv(BACKUP + file_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def td_to_pandas(query, cur, title=''):\n",
    "    _data = []\n",
    "    _start=dt.now()\n",
    "    print(dt.now().strftime('%m/%d/%Y %r'))\n",
    "    print(f'{title} Execution started...', end='', flush=True)\n",
    "    cur.execute (query)\n",
    "    print(f'finished. {dt.now() - _start}', flush=True) \n",
    "    _start_fetch=dt.now()\n",
    "    print(f'{title} Fetching data started...', end='', flush=True)\n",
    "    for row in cur.fetchall():\n",
    "        _data.append(row) \n",
    "    print(f'finished. {dt.now() - _start_fetch}', flush=True) \n",
    "    _start=dt.now()\n",
    "    print(f'{title} Creating DataFrame for started...', end='', flush=True)\n",
    "    _df = pd.DataFrame(_data)\n",
    "    _df.columns = [x[0].replace('SAP_', '').lower() for x in cur.description]\n",
    "    print(f'finished. {dt.now() - _start}', flush=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def td_dataframe(select_db, query):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            print('Database selected!', flush=True)            \n",
    "            dim_df = td_to_pandas(query, cur, 'Query:')\n",
    "            print('Dim:', dim_df.shape)\n",
    "    \n",
    "    return dim_df\n",
    "\n",
    "\n",
    "def process_list(df, work_list):\n",
    "    \n",
    "    _process = analyze_1(df, work_list, 201910, 202009)\n",
    "    \n",
    "    _process['Country'] = 'US'\n",
    "    \n",
    "    _process = add_time(_process)\n",
    "    \n",
    "    #for standardizing output\n",
    "    work_list.extend(['Country','LBS','SMA_4','SMA_8','SMA_12',\n",
    "                      'YOY Week','LBS_LY','SMA_4_LY','SMA_8_LY','SMA_12_LY',\n",
    "                      'Baseline Week','LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "                      'LBS_Lag_1','LBS_Lag_2','LBS_Lag_3','LBS_Lag_4','LBS_Baseline_Lag_1','LBS_LY_Lag_1',\n",
    "                      'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "                      'LBS_PRECOVID','Week Starting (Mon)','Week Ending (Sun)','COVID Week'])\n",
    "    \n",
    "    if work_list[0] == 'Brand Desc':\n",
    "        work_list.extend(['Wt'])\n",
    "    \n",
    "    return _process[work_list]\n",
    "\n",
    "\n",
    "def full_dataframe(df, _list):\n",
    "    weeks = df.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})\n",
    "    segments = df.groupby(_list[0:-1]).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    _df = segments.assign(key=1).merge(weeks.assign(key=1), how='outer', on='key').drop(columns = {'key'}) \n",
    "    \n",
    "    return _df.merge(df, how = 'left', on = _list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Run\n",
    "from azureml.data import TabularDataset\n",
    "from azureml.data.azure_sql_database_datastore import AzureSqlDatabaseDatastore\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_str = '''\n",
    "--DECLARE @MyDate DATE;\n",
    "--SET @MyDate = DATEADD(DAY, 7-DATEPART(WEEKDAY, GETDATE())-7,GETDATE());\n",
    "--SET DATEFIRST 1;\n",
    "\n",
    "SELECT \n",
    "      a1.[Invoice Date],\n",
    "      DATEADD(DAY, 7-DATEPART(WEEKDAY, a1.[Invoice Date]), a1.[Invoice Date]) as week_end_date\n",
    "      ,CONCAT(FORMAT(a3.[YearNumber], '0000'),FORMAT(a3.[WeekNumberOfYear], '0#')) as calendar_week_number\n",
    "      --,a1.[CustomerID]\n",
    "      ,a1.[Sales Org] as sales_org\n",
    "      ,a2.[Division] as division_name\n",
    "      ,a2.[Brand] as brand_desc\n",
    "      ,a2.[Family] as family_desc\n",
    "      ,a4.[Material Pricing Group] as material_pricing_group_description\n",
    "      ,CASE WHEN CHARINDEX(a2.[Category], 'Potato') > 0 THEN 'Potato' ELSE 'Prepared Foods' END as cat\n",
    "      ,SUM(a1.[Weight Lbs]) as actual_volume_lbs\n",
    "      ,SUM(a1.[Gross Sales]) as gross_sales\n",
    "      ,SUM(a1.[Net Sales]) as net_sales\n",
    "      ,SUM(a1.[Gross Profit]) as gross_profit\n",
    "      ,SUM(a1.[Cost Of Goods Sold]) as cost_of_goods_sold\n",
    "      ,SUM(a1.[Cost Of Sales]) as cost_of_sales\n",
    "  FROM [BI].[Factsellin] as a1\n",
    "  JOIN [BI].[DimProduct] as a2\n",
    "    ON a1.[ProductID] = a2.[ProductID]\n",
    "  JOIN [BI].[DimDate] as a3\n",
    "    ON a1.[Invoice Date] = a3.[FullDate]\n",
    "  JOIN [BI].[DimProductSalesData] as a4\n",
    "    ON a1.[ProductID] = a4.[ProductID]\n",
    "    AND a1.[Sales Org] = a4.[Sales Org]\n",
    "  WHERE a1.[Distribution Channel] = '10'\n",
    "  and a1.[Invoice Date] BETWEEN '2019-01-01' AND DATEADD(DAY, 7-DATEPART(WEEKDAY, GETDATE())-7,GETDATE())\n",
    "  and a1.[Sales Org] in ('US01')\n",
    "  GROUP BY\n",
    "    a1.[Invoice Date],\n",
    "    DATEADD(DAY, 7-DATEPART(WEEKDAY, a1.[Invoice Date]), a1.[Invoice Date])\n",
    "    ,CONCAT(FORMAT(a3.[YearNumber], '0000'),FORMAT(a3.[WeekNumberOfYear], '0#'))\n",
    "    --,a1.[CustomerID]\n",
    "    ,a1.[Sales Org]\n",
    "    ,a2.[Division]\n",
    "    ,a2.[Brand]\n",
    "    ,a2.[Family]\n",
    "    ,a4.[Material Pricing Group]\n",
    "    ,CASE WHEN CHARINDEX(a2.[Category], 'Potato') > 0 THEN 'Potato' ELSE 'Prepared Foods' END'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = '''\n",
    "SELECT \n",
    "      DATEADD(DAY, 7-((DatePart(WEEKDAY, a1.[Invoice Date]) + @@DATEFIRST + 6 - 1 ) % 7), a1.[Invoice Date]) as week_end_date\n",
    "      ,CONCAT(FORMAT(a3.[YearNumber], '0000'),FORMAT(a3.[WeekNumberOfYear], '0#')) as calendar_week_number\n",
    "      --,a1.[CustomerID]\n",
    "      ,a1.[Sales Org] as sales_org\n",
    "      ,a2.[Division] as division_name\n",
    "      ,a2.[Brand] as brand_desc\n",
    "      ,a2.[Family] as family_desc\n",
    "      ,a4.[Material Pricing Group] as material_pricing_group_description\n",
    "      ,CASE WHEN CHARINDEX(a2.[Category], 'Potato') > 0 THEN 'Potato' ELSE 'Prepared Foods' END as consolidated_category\n",
    "      ,SUM(a1.[Weight Lbs]) as actual_volume_lbs\n",
    "      ,SUM(a1.[Gross Sales]) as gross_sales\n",
    "      ,SUM(a1.[Net Sales]) as net_sales\n",
    "      ,SUM(a1.[Gross Profit]) as gross_profit\n",
    "      ,SUM(a1.[Cost Of Goods Sold]) as cost_of_goods_sold\n",
    "      ,SUM(a1.[Cost Of Sales]) as cost_of_sales\n",
    "  FROM [BI].[Factsellin] as a1\n",
    "  JOIN [BI].[DimProduct] as a2\n",
    "    ON a1.[ProductID] = a2.[ProductID]\n",
    "  JOIN [BI].[DimDate] as a3\n",
    "    ON a1.[Invoice Date] = a3.[FullDate]\n",
    "  JOIN [BI].[DimProductSalesData] as a4\n",
    "    ON a1.[ProductID] = a4.[ProductID]\n",
    "    AND a1.[Sales Org] = a4.[Sales Org]\n",
    "  WHERE a1.[Distribution Channel] = '10'\n",
    "  and a1.[Invoice Date] BETWEEN '2019-01-01' AND DATEADD(DAY, 7-DATEPART(WEEKDAY, GETDATE())-7,GETDATE())\n",
    "  and a1.[Sales Org] in ('US01')\n",
    "  GROUP BY\n",
    "    DATEADD(DAY, 7-((DatePart(WEEKDAY, a1.[Invoice Date]) + @@DATEFIRST + 6 - 1 ) % 7), a1.[Invoice Date])\n",
    "    ,CONCAT(FORMAT(a3.[YearNumber], '0000'),FORMAT(a3.[WeekNumberOfYear], '0#'))\n",
    "    --,a1.[CustomerID]\n",
    "    ,a1.[Sales Org]\n",
    "    ,a2.[Division]\n",
    "    ,a2.[Brand]\n",
    "    ,a2.[Family]\n",
    "    ,a4.[Material Pricing Group]\n",
    "    ,CASE WHEN CHARINDEX(a2.[Category], 'Potato') > 0 THEN 'Potato' ELSE 'Prepared Foods' END'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = '''\n",
    "SELECT \n",
    "      DATEADD(DAY, 7-((DatePart(WEEKDAY, a1.[Invoice Date]) + @@DATEFIRST + 6 - 1 ) % 7), a1.[Invoice Date]) as week_end_date\n",
    "      ,CONCAT(FORMAT(a3.[YearNumber], '0000'),FORMAT(a3.[WeekNumberOfYear], '0#')) as calendar_week_number\n",
    "      --,a1.[CustomerID]\n",
    "      ,a1.[Sales Org] as sales_org\n",
    "      ,a2.[Division] as division_name\n",
    "      ,a2.[Brand] as brand_desc\n",
    "      ,a2.[Family] as family_desc\n",
    "      ,a4.[Material Pricing Group] as material_pricing_group_description\n",
    "      ,CASE WHEN CHARINDEX(a2.[Category], 'Potato') > 0 THEN 'Potato' ELSE 'Prepared Foods' END as consolidated_category\n",
    "      ,SUM(a1.[Weight Lbs]) as actual_volume_lbs\n",
    "      ,SUM(a1.[Gross Sales]) as gross_sales\n",
    "      ,SUM(a1.[Net Sales]) as net_sales\n",
    "      ,SUM(a1.[Gross Profit]) as gross_profit\n",
    "      ,SUM(a1.[Cost Of Goods Sold]) as cost_of_goods_sold\n",
    "      ,SUM(a1.[Cost Of Sales]) as cost_of_sales\n",
    "  FROM [BI].[Factsellin] as a1\n",
    "  JOIN [BI].[DimProduct] as a2\n",
    "    ON a1.[ProductID] = a2.[ProductID]\n",
    "  JOIN [BI].[DimDate] as a3\n",
    "    ON a1.[Invoice Date] = a3.[FullDate]\n",
    "  JOIN [BI].[DimProductSalesData] as a4\n",
    "    ON a1.[ProductID] = a4.[ProductID]\n",
    "    AND a1.[Sales Org] = a4.[Sales Org]\n",
    "  WHERE a1.[Distribution Channel] = '10'\n",
    "  and a1.[Invoice Date] BETWEEN '2019-01-01' AND DATEADD(DAY, 7-DATEPART(WEEKDAY, GETDATE())-7,GETDATE())\n",
    "  and a1.[Sales Org] in ('US01')\n",
    "  GROUP BY\n",
    "    DATEADD(DAY, 7-((DatePart(WEEKDAY, a1.[Invoice Date]) + @@DATEFIRST + 6 - 1 ) % 7), a1.[Invoice Date])\n",
    "    ,CONCAT(FORMAT(a3.[YearNumber], '0000'),FORMAT(a3.[WeekNumberOfYear], '0#'))\n",
    "    --,a1.[CustomerID]\n",
    "    ,a1.[Sales Org]\n",
    "    ,a2.[Division]\n",
    "    ,a2.[Brand]\n",
    "    ,a2.[Family]\n",
    "    ,a4.[Material Pricing Group]\n",
    "    ,CASE WHEN CHARINDEX(a2.[Category], 'Potato') > 0 THEN 'Potato' ELSE 'Prepared Foods' END'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ws = Workspace.get(\n",
    "        name='Azure-ML-workspace-01-dev',\n",
    "        subscription_id='54d8e51a-03ad-4e44-b067-d6f54ef09a15',\n",
    "        resource_group='MF_GDA-ENT-ML-RG'\n",
    "    )\n",
    "\n",
    "#print(query_str)\n",
    "\n",
    "datastore: AzureSqlDatabaseDatastore = Datastore.get(ws, 'synapse_sql_datastore')\n",
    "query = (datastore, query_str)\n",
    "ds: TabularDataset = TabularDatasetFactory().from_sql_query(query, query_timeout=0)\n",
    "df = ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63921 entries, 0 to 63920\n",
      "Data columns (total 14 columns):\n",
      " #   Column                              Non-Null Count  Dtype         \n",
      "---  ------                              --------------  -----         \n",
      " 0   week_end_date                       63921 non-null  datetime64[ns]\n",
      " 1   calendar_week_number                63921 non-null  object        \n",
      " 2   sales_org                           63921 non-null  object        \n",
      " 3   division_name                       63920 non-null  object        \n",
      " 4   brand_desc                          63921 non-null  object        \n",
      " 5   family_desc                         63921 non-null  object        \n",
      " 6   material_pricing_group_description  63921 non-null  object        \n",
      " 7   consolidated_category               63921 non-null  object        \n",
      " 8   actual_volume_lbs                   63921 non-null  float64       \n",
      " 9   gross_sales                         63921 non-null  float64       \n",
      " 10  net_sales                           63921 non-null  float64       \n",
      " 11  gross_profit                        63921 non-null  float64       \n",
      " 12  cost_of_goods_sold                  63921 non-null  float64       \n",
      " 13  cost_of_sales                       63921 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6), object(7)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-05-30 00:00:00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['week_end_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_end_date</th>\n",
       "      <th>calendar_week_number</th>\n",
       "      <th>sales_org</th>\n",
       "      <th>division_name</th>\n",
       "      <th>brand_desc</th>\n",
       "      <th>family_desc</th>\n",
       "      <th>material_pricing_group_description</th>\n",
       "      <th>cat</th>\n",
       "      <th>actual_volume_lbs</th>\n",
       "      <th>gross_sales</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>cost_of_goods_sold</th>\n",
       "      <th>cost_of_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>202010</td>\n",
       "      <td>US01</td>\n",
       "      <td>QSR</td>\n",
       "      <td>Portillo's</td>\n",
       "      <td>App Onion-Regular Rings</td>\n",
       "      <td>NAT'L CHAIN SNACKS</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>29760.0</td>\n",
       "      <td>40176.0</td>\n",
       "      <td>38554.08</td>\n",
       "      <td>9696.86</td>\n",
       "      <td>27777.22</td>\n",
       "      <td>28857.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>202050</td>\n",
       "      <td>US01</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Wegmans</td>\n",
       "      <td>App Onion-Regular Rings</td>\n",
       "      <td>RT PRIV LBL SNACKS</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>12978.0</td>\n",
       "      <td>12487.44</td>\n",
       "      <td>42.06</td>\n",
       "      <td>10635.78</td>\n",
       "      <td>12445.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>202153</td>\n",
       "      <td>US01</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>McCain</td>\n",
       "      <td>SweetPotSpec-CutWhole Regular</td>\n",
       "      <td>PRIMARY BRANDED</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>7920.0</td>\n",
       "      <td>15444.0</td>\n",
       "      <td>9353.74</td>\n",
       "      <td>3604.58</td>\n",
       "      <td>4747.86</td>\n",
       "      <td>5749.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-26</td>\n",
       "      <td>202103</td>\n",
       "      <td>US01</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>Pot Fries-Conventional</td>\n",
       "      <td>PRIVATE LABEL POTATO</td>\n",
       "      <td>Potato</td>\n",
       "      <td>217404.0</td>\n",
       "      <td>108702.0</td>\n",
       "      <td>102777.74</td>\n",
       "      <td>774.00</td>\n",
       "      <td>83699.54</td>\n",
       "      <td>102003.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>201942</td>\n",
       "      <td>US01</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Hy-Vee</td>\n",
       "      <td>SweetPot Fries-Battered</td>\n",
       "      <td>RT PL POTATO GRADE A</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>2755.12</td>\n",
       "      <td>890.32</td>\n",
       "      <td>1659.88</td>\n",
       "      <td>1864.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_end_date calendar_week_number sales_org division_name  brand_desc  \\\n",
       "0    2020-03-10               202010      US01           QSR  Portillo's   \n",
       "1    2020-12-15               202050      US01        Retail     Wegmans   \n",
       "2    2021-01-05               202153      US01  Food Service      McCain   \n",
       "3    2021-01-26               202103      US01  Food Service      Alpine   \n",
       "4    2019-10-22               201942      US01        Retail      Hy-Vee   \n",
       "\n",
       "                     family_desc material_pricing_group_description  \\\n",
       "0        App Onion-Regular Rings                 NAT'L CHAIN SNACKS   \n",
       "1        App Onion-Regular Rings                 RT PRIV LBL SNACKS   \n",
       "2  SweetPotSpec-CutWhole Regular                    PRIMARY BRANDED   \n",
       "3         Pot Fries-Conventional               PRIVATE LABEL POTATO   \n",
       "4        SweetPot Fries-Battered               RT PL POTATO GRADE A   \n",
       "\n",
       "              cat  actual_volume_lbs  gross_sales  net_sales  gross_profit  \\\n",
       "0  Prepared Foods            29760.0      40176.0   38554.08       9696.86   \n",
       "1  Prepared Foods             8400.0      12978.0   12487.44         42.06   \n",
       "2  Prepared Foods             7920.0      15444.0    9353.74       3604.58   \n",
       "3          Potato           217404.0     108702.0  102777.74        774.00   \n",
       "4  Prepared Foods             2280.0       2848.0    2755.12        890.32   \n",
       "\n",
       "   cost_of_goods_sold  cost_of_sales  \n",
       "0            27777.22       28857.22  \n",
       "1            10635.78       12445.38  \n",
       "2             4747.86        5749.16  \n",
       "3            83699.54      102003.74  \n",
       "4             1659.88        1864.80  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_azure(df):\n",
    "    #df['YearNumber'] = df['YearNumber'].astype('str').replace('\\.0', '', regex=True)\n",
    "    #df['WeekNumberOfYear'] = df['WeekNumberOfYear'].astype('str').replace('\\.0', '', regex=True)\n",
    "    \n",
    "    #df['calendar_week_number'] = df['YearNumber'] + df['WeekNumberOfYear'].str.zfill(2)\n",
    "    #df['calendar_week_number'] = df.apply(lambda x: x['YearNumber'] + x['WeekNumberOfYear'])\n",
    "    \n",
    "    #update category_desc values based on row qualifiers\n",
    "    \n",
    "    #update calendar_week_name to numeric for future functions\n",
    "    #df['calendar_week_number'] = pd.to_numeric(df['calendar_week_number'], errors = 'coerce')\n",
    "    \n",
    "    #df = df.astype({'actual_volume_lbs':'float64'})\n",
    "    \n",
    "    df = df.astype({'calendar_week_number':'int64'}).rename(columns={'actual_volume_lbs':'LBS',\n",
    "                          'calendar_week_number':'Calendar Week Year',\n",
    "                          'division_name':'Division Name',\n",
    "                          'material_pricing_group_description':'Material Pricing Group Description',\n",
    "                          'customer_hier_lvl_1_name':'Customer L1 Name',\n",
    "                          'brand_desc':'Brand Desc',\n",
    "                           'consolidated_category':'Consolidated Category',\n",
    "                           'family_desc':'Family Desc'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_end_date</th>\n",
       "      <th>Calendar Week Year</th>\n",
       "      <th>sales_org</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Brand Desc</th>\n",
       "      <th>Family Desc</th>\n",
       "      <th>Material Pricing Group Description</th>\n",
       "      <th>Consolidated Category</th>\n",
       "      <th>LBS</th>\n",
       "      <th>gross_sales</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>cost_of_goods_sold</th>\n",
       "      <th>cost_of_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>202010</td>\n",
       "      <td>US01</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Wakefern</td>\n",
       "      <td>Pot Fries-Battered</td>\n",
       "      <td>RT PL POTATO GRADE A</td>\n",
       "      <td>Potato</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>2950.56</td>\n",
       "      <td>2866.48</td>\n",
       "      <td>583.54</td>\n",
       "      <td>1857.90</td>\n",
       "      <td>2282.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>201950</td>\n",
       "      <td>US01</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Anchor</td>\n",
       "      <td>App Cheese-Regular Sticks</td>\n",
       "      <td>ANCHOR SNACKS</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>421692.0</td>\n",
       "      <td>1665315.82</td>\n",
       "      <td>1137441.50</td>\n",
       "      <td>569036.78</td>\n",
       "      <td>517871.56</td>\n",
       "      <td>568404.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-09-10</td>\n",
       "      <td>201936</td>\n",
       "      <td>US01</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Quaker Steak and Lube</td>\n",
       "      <td>App Onion-Regular Rings</td>\n",
       "      <td>KEY ACCOUNT SNACKS</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>7104.0</td>\n",
       "      <td>10559.12</td>\n",
       "      <td>10152.58</td>\n",
       "      <td>851.68</td>\n",
       "      <td>7703.54</td>\n",
       "      <td>9300.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-26</td>\n",
       "      <td>202103</td>\n",
       "      <td>US01</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Idaho Valley</td>\n",
       "      <td>Pot Fries-Conventional</td>\n",
       "      <td>SECONDARY BRANDED</td>\n",
       "      <td>Potato</td>\n",
       "      <td>458940.0</td>\n",
       "      <td>282668.40</td>\n",
       "      <td>230091.76</td>\n",
       "      <td>69476.94</td>\n",
       "      <td>145793.04</td>\n",
       "      <td>160614.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>201942</td>\n",
       "      <td>US01</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Hy-Vee</td>\n",
       "      <td>SweetPot Fries-Battered</td>\n",
       "      <td>RT PL POTATO GRADE A</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2848.00</td>\n",
       "      <td>2755.12</td>\n",
       "      <td>890.32</td>\n",
       "      <td>1659.88</td>\n",
       "      <td>1864.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_end_date  Calendar Week Year sales_org Division Name  \\\n",
       "0    2020-03-10              202010      US01        Retail   \n",
       "1    2019-12-17              201950      US01  Food Service   \n",
       "2    2019-09-10              201936      US01  Food Service   \n",
       "3    2021-01-26              202103      US01  Food Service   \n",
       "4    2019-10-22              201942      US01        Retail   \n",
       "\n",
       "              Brand Desc                Family Desc  \\\n",
       "0               Wakefern         Pot Fries-Battered   \n",
       "1                 Anchor  App Cheese-Regular Sticks   \n",
       "2  Quaker Steak and Lube    App Onion-Regular Rings   \n",
       "3           Idaho Valley     Pot Fries-Conventional   \n",
       "4                 Hy-Vee    SweetPot Fries-Battered   \n",
       "\n",
       "  Material Pricing Group Description Consolidated Category       LBS  \\\n",
       "0               RT PL POTATO GRADE A                Potato    3456.0   \n",
       "1                      ANCHOR SNACKS        Prepared Foods  421692.0   \n",
       "2                 KEY ACCOUNT SNACKS        Prepared Foods    7104.0   \n",
       "3                  SECONDARY BRANDED                Potato  458940.0   \n",
       "4               RT PL POTATO GRADE A        Prepared Foods    2280.0   \n",
       "\n",
       "   gross_sales   net_sales  gross_profit  cost_of_goods_sold  cost_of_sales  \n",
       "0      2950.56     2866.48        583.54             1857.90        2282.94  \n",
       "1   1665315.82  1137441.50     569036.78           517871.56      568404.72  \n",
       "2     10559.12    10152.58        851.68             7703.54        9300.90  \n",
       "3    282668.40   230091.76      69476.94           145793.04      160614.82  \n",
       "4      2848.00     2755.12        890.32             1659.88        1864.80  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = transform_azure(df)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Teradata Queries\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def teradata_sales():\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Sysco COVID Performance;StartTime=20200901T131109;JobID=68215;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "        select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "            (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "            a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "            (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "            RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "            a16.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "            a13.DIVISION_ID as DIVISION,\n",
    "            a17.DIVISION_NAME as DIVISION_NAME,\n",
    "            a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "            a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "            a12.SUB_CATEGORY_SHORT_CODE as SUB_CATEGORY_SHORT_CODE,\n",
    "            a12.SUB_CATEGORY_DESC as SUB_CATEGORY_DESC,\n",
    "            a15.MATERIAL_PRICING_GROUP_ID as MATERIAL_PRICING_GROUP_ID,\n",
    "            a18.MATERIAL_PRICING_GROUP_DESCRIPTION as MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "            TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "            a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "            sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "        from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "        join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "        on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "        join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "        on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "        join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "        on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "        join DL_GBL_TAS_BI.D_MATERIAL_SALES_DATA as a15\n",
    "        on (a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL_ID and \n",
    "        a11.MATERIAL_ID = a15.MATERIAL_ID and \n",
    "        a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION_ID)\n",
    "        join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a16\n",
    "        on (a11.CUSTOMER_ID = a16.CUSTOMER and \n",
    "        a11.DISTRIBUTION_CHANNEL_ID = a16.DISTRIBUTION_CHANNEL and \n",
    "        a11.SALES_ORGANISATION_ID = a16.SALES_ORGANISATION)\n",
    "        join DL_GBL_TAS_BI.D_DIVISION as a17\n",
    "        on (a13.DIVISION_ID = a17.DIVISION_ID)\n",
    "        join DL_GBL_TAS_BI.D_MATERIAL_PRICING_GROUP as a18\n",
    "        on (a15.MATERIAL_PRICING_GROUP_ID = a18.MATERIAL_PRICING_GROUP_ID)\n",
    "        left join DL_GBL_TAS_BI.FACT_OM_ORDER_FULFILLMENT as a19\n",
    "        on (a11.SALES_ORDER_ID = a19.SALES_ORDER_ID) and (a11.MATERIAL_ID = a19.MATERIAL_ID)\n",
    "        where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "        and a11.SALES_ORGANISATION_ID in ('US01')\n",
    "        and a11.DISTRIBUTION_CHANNEL_ID in ('10'))\n",
    "        and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(WEEK - 1) + ''' \n",
    "        group by a14.FISCAL_WEEK_NUMBER,\n",
    "        (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "        a14.CALENDAR_WEEK_NAME,\n",
    "        (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        a16.CUSTOMER_HIER_LVL_1_NAME,\n",
    "        a13.DIVISION_ID,\n",
    "        a17.DIVISION_NAME,\n",
    "        a12.CATEGORY_SHORT_CODE,\n",
    "        a12.CATEGORY_DESC,\n",
    "        a12.SUB_CATEGORY_SHORT_CODE,\n",
    "        a12.SUB_CATEGORY_DESC,\n",
    "        a15.MATERIAL_PRICING_GROUP_ID,\n",
    "        a18.MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "        TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "        a13.MATERIAL_DESCRIPTION\n",
    "        '''\n",
    "    \n",
    "    #build dataframe from teradata query\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    #return transformed dataframe\n",
    "    return transform_teradata(df)\n",
    "\n",
    "\n",
    "def teradata_brand():\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Brand COVID Performance;StartTime=20200901T113649;JobID=55922;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "    a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "    RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "    a16.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a13.DIVISION_ID as DIVISION,\n",
    "    a17.DIVISION_NAME as DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "    a15.MATERIAL_PRICING_GROUP_ID as MATERIAL_PRICING_GROUP_ID,\n",
    "    a18.MATERIAL_PRICING_GROUP_DESCRIPTION as MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "    a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "    a12.BRAND_SHORT_CODE as BRAND_SHORT_CODE,\n",
    "    a12.BRAND_DESC as BRAND_DESC,\n",
    "    sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "    on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "    on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "    on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_SALES_DATA as a15\n",
    "    on (a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL_ID and \n",
    "    a11.MATERIAL_ID = a15.MATERIAL_ID and \n",
    "    a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION_ID)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a16\n",
    "    on (a11.CUSTOMER_ID = a16.CUSTOMER and \n",
    "    a11.DISTRIBUTION_CHANNEL_ID = a16.DISTRIBUTION_CHANNEL and \n",
    "    a11.SALES_ORGANISATION_ID = a16.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a17\n",
    "    on (a13.DIVISION_ID = a17.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_PRICING_GROUP as a18\n",
    "    on (a15.MATERIAL_PRICING_GROUP_ID = a18.MATERIAL_PRICING_GROUP_ID)\n",
    "    where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "    and a11.SALES_ORGANISATION_ID in ('US01')\n",
    "    and a11.DISTRIBUTION_CHANNEL_ID in ('10')\n",
    "    and a12.BRAND_SHORT_CODE in ('002', '005', '042', '536', '544', '545', '638', '659', '688', '694', '093'))\n",
    "    and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(WEEK - 1) + ''' \n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    a14.CALENDAR_WEEK_NAME,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "    a16.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a13.DIVISION_ID,\n",
    "    a17.DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC,\n",
    "    a15.MATERIAL_PRICING_GROUP_ID,\n",
    "    a18.MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "    a13.MATERIAL_DESCRIPTION,\n",
    "    a12.BRAND_SHORT_CODE,\n",
    "    a12.BRAND_DESC\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return transform_teradata(df)\n",
    "\n",
    "\n",
    "def transform_teradata(df):\n",
    "\n",
    "    #update category_desc values based on row qualifiers\n",
    "    df['Consolidated Category'] = df['category_desc']\n",
    "    df.loc[df['Consolidated Category'] == 'Sweet Potato' , 'Consolidated Category'] = 'Potato'\n",
    "    df.loc[df['Consolidated Category'] != 'Potato' , 'Consolidated Category'] = 'Prepared Foods'\n",
    "    \n",
    "    #update calendar_week_name to numeric for future functions\n",
    "    df['calendar_week_number'] = pd.to_numeric(df['calendar_week_number'], errors = 'coerce')\n",
    "    \n",
    "    df = df.astype({'actual_volume_lbs':'float64'})\n",
    "\n",
    "    df = df.rename(columns={'actual_volume_lbs':'LBS',\n",
    "                          'calendar_week_number':'Calendar Week Year',\n",
    "                          'division_name':'Division Name',\n",
    "                          'material_pricing_group_description':'Material Pricing Group Description',\n",
    "                          'customer_hier_lvl_1_name':'Customer L1 Name',\n",
    "                          'brand_desc':'Brand Desc'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execute Analysis\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected!\n",
      "07/08/2022 07:49:38 AM\n",
      "Query: Execution started...finished. 0:03:44.464201\n",
      "Query: Fetching data started...finished. 0:17:22.561265\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.733931\n",
      "Dim: (531537, 17)\n",
      "Database selected!\n",
      "07/08/2022 08:10:52 AM\n",
      "Query: Execution started...finished. 0:01:31.837933\n",
      "Query: Fetching data started...finished. 0:00:14.721962\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.023049\n",
      "Dim: (19809, 17)\n"
     ]
    }
   ],
   "source": [
    "#pull sales data from teradata\n",
    "_sales = teradata_sales()\n",
    "#_sales = df_test\n",
    "#pull sales data by brand from teradata\n",
    "#seperate from the query above becaues of filters\n",
    "_brand = teradata_brand()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done\n"
     ]
    }
   ],
   "source": [
    "#blank list to add lists to\n",
    "_list = []\n",
    "\n",
    "#Output 1: Division Name - List 0\n",
    "_list.append(['Division Name','Consolidated Category'])\n",
    "\n",
    "#Output 2: MPG - List 1\n",
    "_list.append(['Material Pricing Group Description', 'Consolidated Category'])\n",
    "\n",
    "#Output 3: Customer L1 - List 2\n",
    "_list.append(['Division Name','Customer L1 Name'])\n",
    "\n",
    "#Create dataframes\n",
    "output1 = process_list(_sales, _list[0])\n",
    "output2 = process_list(_sales, _list[1])\n",
    "output3 = process_list(_sales, _list[2])\n",
    "\n",
    "\n",
    "#Output 4: Brand - List 3\n",
    "_list.append(['Brand Desc'])\n",
    "\n",
    "output4 = process_list(_brand, _list[3])\n",
    "\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done\n"
     ]
    }
   ],
   "source": [
    "#table_list = ['SELLIN_DIVISION','SELLIN_MPG','SELLIN_BRAND','SELLIN_RETAIL','SELLIN_CUSTOMER_L1']\n",
    "table_list = ['SELLIN_DIVISION','SELLIN_MPG','SELLIN_BRAND']\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(PATH + 'Weekly Sellin Data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "output1.columns = output1.columns.str.lower()\n",
    "output2.columns = output2.columns.str.lower()\n",
    "output4.columns = output4.columns.str.lower()\n",
    "\n",
    "output1.to_excel(writer, sheet_name = 'SELLIN_DIVISION', index = False)\n",
    "output2.to_excel(writer, sheet_name = 'SELLIN_MPG', index = False)\n",
    "output4.to_excel(writer, sheet_name = 'SELLIN_BRAND', index = False)\n",
    "\n",
    "writer.save()\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Teradata Update\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected! 07/08/2022 08:12:50 AM\n",
      "Deleting records for: US in table: SELLIN_DIVISION\n",
      "Inserting records into SELLIN_DIVISION\n",
      "Inserted 2013 records\n",
      "Database selected! 07/08/2022 08:12:56 AM\n",
      "Deleting records for: US in table: SELLIN_MPG\n",
      "Inserting records into SELLIN_MPG\n",
      "Inserted 7137 records\n",
      "Database selected! 07/08/2022 08:13:07 AM\n",
      "Deleting records for: US in table: SELLIN_CUSTOMER_L1\n",
      "Inserting records into SELLIN_CUSTOMER_L1\n",
      "Inserted 36783 records\n",
      "Database selected! 07/08/2022 08:13:51 AM\n",
      "Deleting records for: US in table: SELLIN_BRAND\n",
      "Inserting records into SELLIN_BRAND\n",
      "Inserted 2013 records\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "def td_upload(select_db, df, table_name):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            d = dt.now().strftime('%m/%d/%Y %H:%M:%S %p')\n",
    "            print(f'Database selected! {d}', flush=True)           \n",
    "\n",
    "            delete_from_td(df, table_name, cur)\n",
    "            insert_into_td(df, table_name, cur)\n",
    "\n",
    "def delete_from_td(df, table_name, cur):\n",
    "    distributor = df.groupby('Country').size().reset_index().drop(columns=0).to_numpy()[0][0]\n",
    "    \n",
    "    print(f'Deleting records for: {distributor} in table: {table_name}', flush = True)          \n",
    "        \n",
    "    query = '''\n",
    "    DELETE FROM ''' + table_name  + ''' \n",
    "    WHERE \"Country\" = ''' + \"'\" + distributor + \"'\"\n",
    "    \n",
    "    cur.execute (query)\n",
    "    \n",
    "def insert_into_td(df, table_name, cur):\n",
    "    insert_list = df.values.tolist()\n",
    "    \n",
    "    #creates ?, ?,.... string used in query for teradata fastload\n",
    "    insert_columns = ('?, ' * len(df.columns)).rstrip(', ')\n",
    "\n",
    "    print(f'Inserting records into {table_name}', flush = True)\n",
    "    \n",
    "    query = \"INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    #query = \"{fn teradata_try_fastload}INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    \n",
    "    cur.execute (query, insert_list)\n",
    "    \n",
    "    print(f'Inserted {df.shape[0]} records', flush = True)\n",
    "    \n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "td_upload(select_db, output1, 'SELLIN_DIVISION')\n",
    "td_upload(select_db, output2, 'SELLIN_MPG')\n",
    "td_upload(select_db, output3, 'SELLIN_CUSTOMER_L1')\n",
    "td_upload(select_db, output4, 'SELLIN_BRAND')\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Output to Excel\n",
    "Only used for Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: SELLIN_DIVISION\n",
      "Database selected!\n",
      "07/08/2022 08:20:46 AM\n",
      "Query: Execution started...finished. 0:00:05.402885\n",
      "Query: Fetching data started...finished. 0:00:00.344795\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.027756\n",
      "Dim: (3300, 31)\n",
      "Exporting table: SELLIN_MPG\n",
      "Database selected!\n",
      "07/08/2022 08:20:55 AM\n",
      "Query: Execution started...finished. 0:00:07.401392\n",
      "Query: Fetching data started...finished. 0:00:00.742142\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.045977\n",
      "Dim: (7137, 31)\n",
      "Exporting table: SELLIN_BRAND\n",
      "Database selected!\n",
      "07/08/2022 08:21:07 AM\n",
      "Query: Execution started...finished. 0:00:05.051469\n",
      "Query: Fetching data started...finished. 0:00:00.378976\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.027109\n",
      "Dim: (3821, 31)\n",
      "Exporting table: SELLIN_RETAIL\n",
      "Database selected!\n",
      "07/08/2022 08:21:16 AM\n",
      "Query: Execution started...finished. 0:00:01.511803\n",
      "Query: Fetching data started...finished. 0:00:00.196695\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.017243\n",
      "Dim: (2044, 30)\n",
      "Exporting table: SELLIN_CUSTOMER_L1\n",
      "Database selected!\n",
      "07/08/2022 08:21:20 AM\n",
      "Query: Execution started...finished. 0:00:07.256380\n",
      "Query: Fetching data started...finished. 0:00:25.948337\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.241660\n",
      "Dim: (36783, 31)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "table_list = ['SELLIN_DIVISION','SELLIN_MPG','SELLIN_BRAND','SELLIN_RETAIL','SELLIN_CUSTOMER_L1']\n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(PATH + 'Weekly Sellin Data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for table_name in table_list:\n",
    "    print(f'Exporting table: {table_name}', flush = True)\n",
    "    query = '''\n",
    "    SELECT * FROM ''' + table_name + '''\n",
    "    '''\n",
    "    df = td_dataframe(select_db, query)\n",
    "    df.to_excel(writer, sheet_name = table_name, index = False)\n",
    "    \n",
    "writer.save()\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected!\n",
      "08/17/2022 10:42:33 AM\n",
      "Query: Execution started...finished. 0:02:54.793075\n",
      "Query: Fetching data started...finished. 0:02:20.240372\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.439261\n",
      "Dim: (489352, 12)\n"
     ]
    }
   ],
   "source": [
    "def teradata_fillrate():\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Brand COVID Performance;StartTime=20200901T113649;JobID=55922;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    SELECT\n",
    "    a1.SALES_ORGANISATION_ID,\n",
    "    a3.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_L1,\n",
    "    a3.CUSTOMER_HIER_LVL_2_NAME as CUSTOMER_L2,\n",
    "    a2.DIVISION_DESCRIPTION,\n",
    "    a2.CATEGORY_DESC,\n",
    "    a2.PRODUCT_GROUP_FORMAT_DESC,\n",
    "    a2.PRODUCT_GROUP_SUB_FORMAT_DESC,\n",
    "    a2.MATERIAL_ID,\n",
    "    a2.MATERIAL_DESCRIPTION,\n",
    "    TD_WEEK_END(a1.ACTUAL_OR_PLANNED_PGI_DATE)+1 as WEEK_ENDING,\n",
    "    sum(cast(a1.SUM_ORDER_QUANTITY as float)) as ORDERED_CASES,\n",
    "    sum(cast(a1.DELIVERED_QUANTITY as float)) as DELIVERED_CASES\n",
    "    FROM DL_GBL_TAS_BI.FACT_OM_ORDER_FULFILLMENT as a1\n",
    "    JOIN DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a2\n",
    "        on a1.MATERIAL_ID = a2.MATERIAL_ID\n",
    "    JOIN DL_GBL_TAS_BI.H_CUSTOMER as a3\n",
    "        on a1.SOLD_TO_CUSTOMER_ID = a3.CUSTOMER_ID\n",
    "        and a1.SALES_ORGANISATION_ID = a3.SALES_ORGANISATION\n",
    "        and a1.DISTRIBUTION_CHANNEL_ID = a3.DISTRIBUTION_CHANNEL\n",
    "    WHERE a1.PGI_COMPLETE = 'PGIED ORDER'\n",
    "    and a1.DOCUMENT_TYPE = 'ZOR'\n",
    "    and a1.DISTRIBUTION_CHANNEL_ID = 10\n",
    "    and a1.SALES_ORGANISATION_ID in ('US01', 'CA01')\n",
    "    and a1.ACTUAL_OR_PLANNED_PGI_DATE > '2021-01-01'\n",
    "\n",
    "    GROUP BY\n",
    "    a1.SALES_ORGANISATION_ID,\n",
    "    a3.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a3.CUSTOMER_HIER_LVL_2_NAME,\n",
    "    a2.DIVISION_DESCRIPTION,\n",
    "    a2.CATEGORY_DESC,\n",
    "    a2.PRODUCT_GROUP_FORMAT_DESC,\n",
    "    a2.PRODUCT_GROUP_SUB_FORMAT_DESC,\n",
    "    a2.MATERIAL_ID,\n",
    "    a2.MATERIAL_DESCRIPTION,\n",
    "    TD_WEEK_END(a1.ACTUAL_OR_PLANNED_PGI_DATE)+1\n",
    "    ;'''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return df\n",
    "\n",
    "fill_rate = teradata_fillrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_organisation_id</th>\n",
       "      <th>customer_l1</th>\n",
       "      <th>customer_l2</th>\n",
       "      <th>division_description</th>\n",
       "      <th>category_desc</th>\n",
       "      <th>product_group_format_desc</th>\n",
       "      <th>product_group_sub_format_desc</th>\n",
       "      <th>material_id</th>\n",
       "      <th>material_description</th>\n",
       "      <th>week_ending</th>\n",
       "      <th>ordered_cases</th>\n",
       "      <th>delivered_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US01</td>\n",
       "      <td>BOR NATIONAL L1</td>\n",
       "      <td>BOR US - CENTRAL L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Potato</td>\n",
       "      <td>SPECIALTY</td>\n",
       "      <td>CUT</td>\n",
       "      <td>OIF252A</td>\n",
       "      <td>FS OREIDA DICED RNDM CT HB 6X5</td>\n",
       "      <td>2021-07-04</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US01</td>\n",
       "      <td>ENDICO L1</td>\n",
       "      <td>ENDICO L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Potato</td>\n",
       "      <td>CONVENTIONAL FRIES</td>\n",
       "      <td>FROZEN FRIES</td>\n",
       "      <td>000000001000000529</td>\n",
       "      <td>DD_END 6/5 3/8\" STRAIGHT CUT A GRADE FRY</td>\n",
       "      <td>2021-10-10</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US01</td>\n",
       "      <td>CASH WA DISTRIBUTING L1</td>\n",
       "      <td>CASH WA DISTRIBUTING L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>APPETIZERS</td>\n",
       "      <td>WHOLE VEGETABLE</td>\n",
       "      <td>000000000080008473</td>\n",
       "      <td>GCP BAT WHLE MSHRMS 6X2LB</td>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US01</td>\n",
       "      <td>KROGER L1</td>\n",
       "      <td>KROGER L2</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>LOCAL PORTFOLIO</td>\n",
       "      <td>LOCAL PORTFOLIO OTHER</td>\n",
       "      <td>000000001000007301</td>\n",
       "      <td>RT KRO BCN TWBE POT 8X10OZ</td>\n",
       "      <td>2021-07-25</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US01</td>\n",
       "      <td>CASH WA DISTRIBUTING L1</td>\n",
       "      <td>CASH WA DISTRIBUTING L2</td>\n",
       "      <td>Food Service</td>\n",
       "      <td>Prepared Foods</td>\n",
       "      <td>APPETIZERS</td>\n",
       "      <td>ONION RINGS</td>\n",
       "      <td>000000000070010011</td>\n",
       "      <td>BCI 5/8 BRB THCK ON RINGS 6X2.5LB</td>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sales_organisation_id              customer_l1              customer_l2  \\\n",
       "0          US01                  BOR NATIONAL L1      BOR US - CENTRAL L2   \n",
       "1          US01                        ENDICO L1                ENDICO L2   \n",
       "2          US01          CASH WA DISTRIBUTING L1  CASH WA DISTRIBUTING L2   \n",
       "3          US01                        KROGER L1                KROGER L2   \n",
       "4          US01          CASH WA DISTRIBUTING L1  CASH WA DISTRIBUTING L2   \n",
       "\n",
       "  division_description   category_desc product_group_format_desc  \\\n",
       "0         Food Service          Potato                 SPECIALTY   \n",
       "1         Food Service          Potato        CONVENTIONAL FRIES   \n",
       "2         Food Service  Prepared Foods                APPETIZERS   \n",
       "3               Retail  Prepared Foods           LOCAL PORTFOLIO   \n",
       "4         Food Service  Prepared Foods                APPETIZERS   \n",
       "\n",
       "  product_group_sub_format_desc         material_id  \\\n",
       "0                           CUT             OIF252A   \n",
       "1                  FROZEN FRIES  000000001000000529   \n",
       "2               WHOLE VEGETABLE  000000000080008473   \n",
       "3         LOCAL PORTFOLIO OTHER  000000001000007301   \n",
       "4                   ONION RINGS  000000000070010011   \n",
       "\n",
       "                       material_description week_ending  ordered_cases  \\\n",
       "0            FS OREIDA DICED RNDM CT HB 6X5  2021-07-04           45.0   \n",
       "1  DD_END 6/5 3/8\" STRAIGHT CUT A GRADE FRY  2021-10-10         1150.0   \n",
       "2                 GCP BAT WHLE MSHRMS 6X2LB  2021-04-04           60.0   \n",
       "3                RT KRO BCN TWBE POT 8X10OZ  2021-07-25         1104.0   \n",
       "4         BCI 5/8 BRB THCK ON RINGS 6X2.5LB  2021-08-15          480.0   \n",
       "\n",
       "   delivered_cases  \n",
       "0             45.0  \n",
       "1           1150.0  \n",
       "2             60.0  \n",
       "3             74.0  \n",
       "4              0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 489352 entries, 0 to 489351\n",
      "Data columns (total 12 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   sales_organisation_id          489352 non-null  object        \n",
      " 1   customer_l1                    489352 non-null  object        \n",
      " 2   customer_l2                    489352 non-null  object        \n",
      " 3   division_description           489352 non-null  object        \n",
      " 4   category_desc                  489352 non-null  object        \n",
      " 5   product_group_format_desc      489352 non-null  object        \n",
      " 6   product_group_sub_format_desc  489351 non-null  object        \n",
      " 7   material_id                    489352 non-null  object        \n",
      " 8   material_description           489352 non-null  object        \n",
      " 9   week_ending                    489352 non-null  datetime64[ns]\n",
      " 10  ordered_cases                  489351 non-null  float64       \n",
      " 11  delivered_cases                446303 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(9)\n",
      "memory usage: 44.8+ MB\n"
     ]
    }
   ],
   "source": [
    "fill_rate = fill_rate.astype({'week_ending':'datetime64'})\n",
    "\n",
    "fill_rate.loc[fill_rate['category_desc'].str.contains('Potato'), 'category_desc'] = 'Potato'\n",
    "fill_rate.loc[~fill_rate['category_desc'].str.contains('Potato'), 'category_desc'] = 'Prepared Foods'\n",
    "\n",
    "display(fill_rate.head())\n",
    "\n",
    "fill_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_rate.to_csv('FILL_RATE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
