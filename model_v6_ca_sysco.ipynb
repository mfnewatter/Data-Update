{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sysco CA Sell-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: \n",
    "1. Process Sysco CA data\n",
    "2. Analyze data using COVID segmentation\n",
    "3. Compare sell-out (Sysco CA) to sell-in (McCain) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load libraries, initiate folder/file paths\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import teradatasql\n",
    "\n",
    "#path where dictionary file can be found\n",
    "#Neil\n",
    "DICTIONARY = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Distributor Sell-Out Dictionaries\\\\'\n",
    "#Joe\n",
    "#DICTIONARY = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Distributor Sell-Out Dictionaries\\\\'\n",
    "\n",
    "#main path\n",
    "#Neil\n",
    "PATH = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\\\'\n",
    "#Joe\n",
    "#PATH = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\\\'\n",
    "\n",
    "#backup path\n",
    "#Neil\n",
    "BACKUP = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\Backups\\\\'\n",
    "#Joe\n",
    "#BACKUP = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\Backups\\\\'\n",
    "\n",
    "#time dataframe\n",
    "TIME = pd.read_excel(DICTIONARY + 'Time Definitions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Dictionary\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_states():\n",
    "    us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY'\n",
    "    }\n",
    "\n",
    "    # thank you to @kinghelix and @trevormarburger for this idea\n",
    "    abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))\n",
    "    \n",
    "    return pd.DataFrame.from_dict(abbrev_us_state, orient = 'index', columns = ['State Name']).rename_axis('State').reset_index()\n",
    "\n",
    "\n",
    "def apply_dictionary(df, file_name):\n",
    "    \n",
    "    #create dictionary object from Excel file\n",
    "    #adding sheet_name = None makes it a dictionary type\n",
    "    _dict = pd.read_excel(DICTIONARY + file_name, sheet_name = None)\n",
    "    \n",
    "    #for testing, keys = sheet names\n",
    "    #print(_dict.keys())\n",
    "\n",
    "    #create DataFrame from dictionary object called dict (short for dictionary)\n",
    "    dict_df = pd.DataFrame.from_dict(_dict['Segment Mapping'])\n",
    "    \n",
    "    #create DataFrame from dictionary object called cat (short for category)\n",
    "    cat_df = pd.DataFrame.from_dict(_dict['Province Mapping'])\n",
    "    cat_df = cat_df[['Province','Cleaned Province Name','Geographic Region']]\n",
    "    \n",
    "    #print shape of df (dimensions)\n",
    "    print(f'Shape before adding dictionary: {df.shape}', flush = True)\n",
    "    \n",
    "    #testing total lbs to see if it matches after merge\n",
    "    total_lbs = df['Volume (Lbs)'].sum()\n",
    "    print(f'Total before dictionary: {total_lbs}', flush = True)\n",
    "    \n",
    "    #add lower case for merging\n",
    "    dict_df['Segment-lower'] = dict_df['Segment'].str.lower()\n",
    "    dict_df['Sector-lower'] = dict_df['Sector'].str.lower()\n",
    "    dict_df['Sub-segment-lower'] = dict_df['Sub-segment'].str.lower()\n",
    "    #fill all blanks with text 'blank' \n",
    "    #dict_df = dict_df.fillna('blank')\n",
    "    #dict_df = dict_df.groupby(['McCain COVID/MWOW Segmentation','Sector-lower','Segment-lower','Updated Sector']).size().reset_index().drop(columns={0}).replace('blank', np.NaN)\n",
    "    dict_df = dict_df.groupby(['McCain COVID/MWOW Segmentation','COVID Segmentation - L2','COVID Segmentation - (Restaurants)','Restaurant Service Type',\n",
    "                               'Sector-lower','Segment-lower','Sub-segment-lower'], dropna = False).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    #!!!-Update base data-!!!#\n",
    "    #new rule 12/9/2020: Sector MEU-IO -> MEU\n",
    "    #updated 2/25/2021: MEU0IO -> MEU\n",
    "    df.loc[(df['Sector'] == 'MEU-IO') | (df['Sector'] == 'MEU0IO'),'Sector'] = 'MEU'\n",
    "    df.loc[(df['Sector'] == 'MEU - IO'),'Sector'] = 'MEU'\n",
    "    df.loc[(df['Sector'] == 'MEU - Unit'),'Sector'] = 'MEU'\n",
    "    \n",
    "    #rename columns\n",
    "    df = df.rename(columns = {'?Sysco Fiscal Month':'Fiscal Month',\n",
    "                              'Sysco Fiscal Month':'Fiscal Month',\n",
    "                              'Sysco Fiscal Week':'Fiscal Week',\n",
    "                              'Sub-segment':'Sub Segment'})\n",
    "    \n",
    "    #add lower case key columns for merging (removes case mismatch)\n",
    "    df['City'] = df['City'].str.upper()\n",
    "    df['Sector-lower'] = df['Sector'].str.lower()\n",
    "    df['Segment-lower'] = df['Segment'].str.lower()\n",
    "    df['Sub-segment-lower'] = df['Sub Segment'].str.lower()\n",
    "    \n",
    "    #remove lower case key columns\n",
    "    df = df.merge(dict_df, how = 'left', left_on = ['Sector-lower', 'Segment-lower', 'Sub-segment-lower'], \n",
    "                  right_on = ['Sector-lower', 'Segment-lower', 'Sub-segment-lower']).drop(columns = {'Sector-lower','Segment-lower','Sub-segment-lower'})\n",
    "    \n",
    "    #add lower case for merging\n",
    "    #cat_df['Sub-segment-lower'] = cat_df['Sub-segment'].str.lower()\n",
    "    #cat_df = cat_df.fillna('blank')\n",
    "    #cat_df = cat_df.groupby(['McCain COVID/MWOW Segmentation','Sub-segment-lower','Restaurant Segment Flag','Service Type']).size().reset_index().drop(columns={0}).replace('blank', np.NaN)\n",
    "    \n",
    "    #add Clean Province Name\n",
    "    df = df.merge(cat_df, how = 'left', left_on = ['Province'], right_on = ['Province'])\n",
    "    #                    right_on = ['McCain COVID/MWOW Segmentation', 'Sub-segment-lower']).drop(columns = {'Sub-segment-lower'})\n",
    "    \n",
    "    #clean up 0's in Province Name, this creates errors with Teradata queries\n",
    "    df.loc[(df['Cleaned Province Name'] == 0) | (df['Cleaned Province Name'].isna()), 'Cleaned Province Name'] = 'Other'\n",
    "    \n",
    "    df = clean_city(df)\n",
    "    \n",
    "     #apply calendar week\n",
    "    df = df.merge(TIME[['Fiscal Week (Sysco)', 'Calendar Week Year']], how = 'left', left_on = ['Fiscal Week'], right_on = ['Fiscal Week (Sysco)']).drop(columns = {'Fiscal Week (Sysco)'})\n",
    "    \n",
    "    df = df.rename(columns = {'Volume (Lbs)':'LBS',\n",
    "                              'Category':'Consolidated Category'})\n",
    "    \n",
    "    #testing total lbs to see if it matches after merge\n",
    "    total_lbs = df['LBS'].sum()\n",
    "    print(f'Total after dictionary: {total_lbs}', flush = True)\n",
    "\n",
    "    #print final shape to see if anything changes (would indicate duplicates in dictionary)\n",
    "    print(f'Shape after adding dictionary: {df.shape}', flush = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Import File\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(file_name):\n",
    "    \n",
    "    if '.csv' in file_name:\n",
    "        _import = pd.read_csv(file_name, low_memory = False, thousands = ',', encoding='cp1252')\n",
    "    else:\n",
    "        _import = pd.read_excel(file_name)\n",
    "    \n",
    "    return _import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculation Functions\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling(df, _list):\n",
    "    #groupby _list\n",
    "    df = df.groupby(_list, dropna = False)[['LBS','LBS_LY','LBS_Baseline']].sum().reset_index()\n",
    "    \n",
    "    #set index to all but last column in list\n",
    "    df = df.set_index(_list)\n",
    "    \n",
    "    #add new metric SMA_4 (simple moving average - 4 periods)\n",
    "    #level = all but last 2 items in list\n",
    "    df['LBS_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    df['LBS_Lag_2'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 2)\n",
    "    df['LBS_Lag_3'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 3)\n",
    "    df['LBS_Lag_4'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 4)\n",
    "    \n",
    "    df['SMA_4'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['LBS_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].shift(periods = 1)\n",
    "    df['LBS_LY_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    \n",
    "    df['SMA_4_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4'].shift(periods = 1)\n",
    "    df['SMA_4_LY_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_LY'].shift(periods = 1)\n",
    "    df['SMA_4_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_Baseline'].shift(periods = 1)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "def add_last_year(df, _list):\n",
    "    #list of groupby columns\n",
    "    #last item in list is Calendar Week Year which is used to pull previous history (Baseline Week = Calendar Week Year) of copied dataframe\n",
    "    _groupby = _list.copy()\n",
    "    \n",
    "    _merge_yoy = _list.copy()[0:-1]\n",
    "    _merge_yoy.extend(['YOY Week'])\n",
    "    \n",
    "    _merge_baseline = _list.copy()[0:-1]\n",
    "    _merge_baseline.extend(['Baseline Week'])\n",
    "    \n",
    "    df1 = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #groupby _list\n",
    "    df_new = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #add week dimensions to main dataframe\n",
    "    df_new = df_new.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_yoy, right_on=_groupby).drop(columns={'Calendar Week Year_y'}).rename(columns={'LBS_y':'LBS_LY'})\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_baseline, right_on=_groupby).drop(columns={'Calendar Week Year'}).rename(columns={\n",
    "        'LBS':'LBS_Baseline','Calendar Week Year_x':'Calendar Week Year','LBS_x':'LBS'})\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def add_precovid(df, _list, begin, end):\n",
    "    #datefield should be last in _list\n",
    "    datefield = _list[-1]\n",
    "          \n",
    "    #remove datefield from list\n",
    "    _list = _list[0:-1]\n",
    "    \n",
    "    #filter data not using last and rename columns\n",
    "    _df = df[(df[datefield] >= begin) & (df[datefield] <= end)].groupby(_list)['LBS'].sum() / 52\n",
    "    \n",
    "    return df.merge(\n",
    "        _df, how = 'left', left_on = _list, right_on = _list).rename(\n",
    "        columns = {'LBS_x':'LBS', 'LBS_y':'LBS_PRECOVID'}).fillna(\n",
    "        value = {'LBS_PRECOVID': 0})\n",
    "\n",
    "\n",
    "def add_time(df):\n",
    "    df = df.merge(TIME[['Calendar Week Year','Week Starting (Sun)','Week Ending (Sat)', 'COVID Week']],\n",
    "                   how = 'left', \n",
    "                   on = 'Calendar Week Year')\n",
    "    \n",
    "    df = df.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_1(df, _list, begin, end):\n",
    "    if 'Calendar Week Year' not in _list:\n",
    "        _list.extend(['Calendar Week Year'])\n",
    "    \n",
    "    df = full_dataframe(df, _list)\n",
    "    \n",
    "     #add last year lbs\n",
    "    df = add_last_year(df, _list)\n",
    "    \n",
    "    #add rolling calculation\n",
    "    df = add_rolling(df, _list)\n",
    "        \n",
    "    #add preCOVID baseline\n",
    "    df = add_precovid(df, _list, begin, end)\n",
    "    \n",
    "    df = df.round({\n",
    "        'LBS' : 2,    \n",
    "        'SMA_4' : 2,\n",
    "        'SMA_8' : 2,\n",
    "        'SMA_12' : 2,\n",
    "        'LBS_LY' : 2,    \n",
    "        'SMA_4_LY' : 2,\n",
    "        'SMA_8_LY' : 2,\n",
    "        'SMA_12_LY' : 2,\n",
    "        'LBS_Baseline' : 2,    \n",
    "        'SMA_4_Baseline' : 2,\n",
    "        'SMA_8_Baseline' : 2,\n",
    "        'SMA_12_Baseline' : 2,\n",
    "        'LBS_PRECOVID' : 2,\n",
    "        'LBS_Lag_1' : 2,\n",
    "        'LBS_Lag_2' : 2,\n",
    "        'LBS_Lag_3' : 2,\n",
    "        'LBS_Lag_4' : 2,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 2,\n",
    "        'SMA_4_LY_Lag_1' : 2,\n",
    "        'SMA_4_Baseline_Lag_1' : 2\n",
    "        \n",
    "    }).fillna(value = {\n",
    "        'LBS' : 0,    \n",
    "        'SMA_4' : 0,\n",
    "        'SMA_8' : 0,\n",
    "        'SMA_12' : 0,\n",
    "        'LBS_LY' : 0,    \n",
    "        'SMA_4_LY' : 0,\n",
    "        'SMA_8_LY' : 0,\n",
    "        'SMA_12_LY' : 0,\n",
    "        'LBS_Baseline' : 0,    \n",
    "        'SMA_4_Baseline' : 0,\n",
    "        'SMA_8_Baseline' : 0,\n",
    "        'SMA_12_Baseline' : 0,\n",
    "        'LBS_PRECOVID' : 0,\n",
    "        'LBS_Lag_1' : 0,\n",
    "        'LBS_Lag_2' : 0,\n",
    "        'LBS_Lag_3' : 0,\n",
    "        'LBS_Lag_4' : 0,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 0,\n",
    "        'SMA_4_LY_Lag_1' : 0,\n",
    "        'SMA_4_Baseline_Lag_1' : 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_backup(df, file_name):\n",
    "    \n",
    "    df.to_csv(BACKUP + file_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def td_to_pandas(query, cur, title=''):\n",
    "    _data = []\n",
    "    _start=dt.now()\n",
    "    print(dt.now().strftime('%m/%d/%Y'))\n",
    "    print(f'{title} Execution started...', end='', flush=True)\n",
    "    cur.execute (query)\n",
    "    print(f'finished. {dt.now() - _start}', flush=True) \n",
    "    _start_fetch=dt.now()\n",
    "    print(f'{title} Fetching data started...', end='', flush=True)\n",
    "    for row in cur.fetchall():\n",
    "        _data.append(row) \n",
    "    print(f'finished. {dt.now() - _start_fetch}', flush=True) \n",
    "    _start=dt.now()\n",
    "    print(f'{title} Creating DataFrame for started...', end='', flush=True)\n",
    "    _df = pd.DataFrame(_data)\n",
    "    _df.columns = [x[0].replace('SAP_', '').lower() for x in cur.description]\n",
    "    print(f'finished. {dt.now() - _start}', flush=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def td_dataframe(select_db, query):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            print('Database selected!', flush=True)            \n",
    "            dim_df = td_to_pandas(query, cur, 'Query:')\n",
    "            print('Dim:', dim_df.shape)\n",
    "    \n",
    "    return dim_df\n",
    "\n",
    "\n",
    "def restaurants(df):\n",
    "    \n",
    "    #Rename rows\n",
    "    df.loc[df['COVID Segmentation - L2'] == 'Independents (IOs) / Local Eateries / Takeaway', 'COVID Segmentation - L2'] = 'IO'\n",
    "    df.loc[\n",
    "        (df['COVID Segmentation - L2'] == 'All Other') | \n",
    "        (df['COVID Segmentation - L2'] == 'National Account') | \n",
    "        (df['COVID Segmentation - L2'] == 'Region Chains')| \n",
    "        (df['COVID Segmentation - L2'] == 'National Accounts'),\n",
    "        'COVID Segmentation - L2'] = 'Chain'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_list(df, work_list):\n",
    "    \n",
    "    _process = analyze_1(df, work_list, 201910, 202009)\n",
    "    \n",
    "    _process = restaurants(_process)\n",
    "    \n",
    "    _process['Distributor'] = 'Sysco CA'\n",
    "    \n",
    "    _process = add_time(_process)\n",
    "    \n",
    "    #for standardizing output\n",
    "    work_list.extend(['Distributor','LBS','SMA_4','SMA_8','SMA_12',\n",
    "                      'YOY Week','LBS_LY','SMA_4_LY','SMA_8_LY','SMA_12_LY',\n",
    "                      'Baseline Week','LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "                      'LBS_Lag_1','LBS_Lag_2','LBS_Lag_3','LBS_Lag_4','LBS_Baseline_Lag_1','LBS_LY_Lag_1',\n",
    "                      'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "                      'LBS_PRECOVID','Week Starting (Sun)','Week Ending (Sat)','COVID Week'])\n",
    "    \n",
    "    return _process[work_list]\n",
    "\n",
    "def full_dataframe(df, _list):\n",
    "    weeks = df.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})\n",
    "    segments = df.groupby(_list[0:-1]).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    _df = segments.assign(key=1).merge(weeks.assign(key=1), how='outer', on='key').drop(columns = {'key'}) \n",
    "    \n",
    "    return _df.merge(df, how = 'left', on = _list) \n",
    "\n",
    "def clean_city(df):\n",
    "    #additional transformations\n",
    "    df['City'] = df['City'].str.strip()\n",
    "    df['City'] = df['City'].str.upper()\n",
    "    df['City'].fillna('NA', inplace = True)\n",
    "    \n",
    "    cities = 'TORONTO|MONTREAL|OTTAWA|CALGARY|VANCOUVER|WINNIPEG|MONTREAL|HAMILTON|HALIFAX'\n",
    "    \n",
    "    #change each city name to the name of the city that matches, cleans up the city names\n",
    "    for c in cities.split('|'):\n",
    "        df.loc[df['City'].str.match(c), 'City'] = c\n",
    "    \n",
    "    #change all other cities to NA\n",
    "    df.loc[~df['City'].str.match(cities), 'City'] = 'NA'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sell-in vs. Sell-out\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teradata_sales(sellout):\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=BEK Performance;StartTime=20200901T101924;JobID=55096;Importance=666;'  FOR SESSION;\n",
    "    \n",
    "    #the current week is pulled from the time dictionary table\n",
    "    to_week = int(TIME[(TIME['Week Starting (Mon)'] <= dt.now()) & (TIME['Week Ending (Sun)'] >= dt.now())]['Calendar Week Year'].values)\n",
    "    \n",
    "    print(f'Starting Teradata connect...', flush = True)\n",
    "    \n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "    a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "    a15.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a11.CUSTOMER_ID as CUSTOMER_ID,\n",
    "    a17.CUSTOMER_NAME as CUSTOMER_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "    a13.DIVISION_ID as DIVISION,\n",
    "    a16.DIVISION_NAME as DIVISION_NAME,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "    a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "    sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "     on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "     on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "     on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a15\n",
    "     on (a11.CUSTOMER_ID = a15.CUSTOMER and \n",
    "    a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL and \n",
    "    a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a16\n",
    "     on (a13.DIVISION_ID = a16.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_CUSTOMER as a17\n",
    "     on (a11.CUSTOMER_ID = a17.CUSTOMER_ID)\n",
    "    where (a11.SALES_ORGANISATION_ID in ('CA01')\n",
    "     and a11.DISTRIBUTION_CHANNEL_ID in ('10')\n",
    "     and RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) in ('6500002815', '6500002893'))\n",
    "     and a17.CUSTOMER_NAME like ('%SYSCO%')\n",
    "     and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(to_week - 1) + '''\n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    a14.CALENDAR_WEEK_NAME,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "    a15.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a11.CUSTOMER_ID,\n",
    "    a17.CUSTOMER_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC,\n",
    "    a13.DIVISION_ID,\n",
    "    a16.DIVISION_NAME,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "    a13.MATERIAL_DESCRIPTION\n",
    "    ;'''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return teradata_transform(df, sellout)\n",
    "\n",
    "\n",
    "def teradata_transform(sellin, sellout):\n",
    "    #consolidates teradata sales with sellout data\n",
    "    \n",
    "    #convert from object datatype to float (exports as a number instead of string)\n",
    "    sellin['actual_volume_lbs'] = sellin['actual_volume_lbs'].astype('float64')\n",
    "    \n",
    "    #rename columns for consistancy\n",
    "    sellin = sellin.rename(columns = {'actual_volume_lbs':'LBS', 'calendar_week_number':'Calendar Week Year'})\n",
    "    \n",
    "    #transform calendar week year from teradata\n",
    "    sellin['Calendar Week Year'] = pd.to_numeric(sellin['Calendar Week Year'], errors = 'coerce')\n",
    "\n",
    "    #transform category so its consolidated\n",
    "    sellin['Consolidated Category'] = sellin['category_desc']\n",
    "    sellin.loc[sellin['Consolidated Category'] == 'Sweet Potato' , 'Consolidated Category'] = 'Potato'\n",
    "    sellin.loc[sellin['Consolidated Category'] != 'Potato' , 'Consolidated Category'] = 'Prepared Foods'\n",
    "    \n",
    "    #analyze sellin data\n",
    "    sellin = analyze_1(sellin, ['Consolidated Category'], 201910, 202009)\n",
    "    \n",
    "    #rename columns accordingly\n",
    "    sellin = sellin.rename(columns = {'LBS':'MCCAIN LBS',\n",
    "                                      'SMA_4':'MCCAIN SMA_4',\n",
    "                                      'SMA_8':'MCCAIN SMA_8',\n",
    "                                      'SMA_12':'MCCAIN SMA_12',\n",
    "                                      'LBS_PRECOVID':'MCCAIN PRECOVID',\n",
    "                                      'LBS_Lag_1':'MCCAIN Lag_1',\n",
    "                                      'LBS_Lag_2':'MCCAIN Lag_2',\n",
    "                                      'LBS_Lag_3':'MCCAIN Lag_3',\n",
    "                                      'LBS_Lag_4':'MCCAIN Lag_4',\n",
    "                                      'LBS_Baseline' : 'MCCAIN LBS_Baseline',\n",
    "                                      'SMA_4_Baseline' : 'MCCAIN SMA_4_Baseline',\n",
    "                                      'SMA_8_Baseline' : 'MCCAIN SMA_8_Baseline',\n",
    "                                      'SMA_12_Baseline' : 'MCCAIN SMA_12_Baseline',\n",
    "                                      'SMA_4_Lag_1':'MCCAIN SMA_4_Lag_1',\n",
    "                                      'SMA_4_Baseline_Lag_1' : 'MCCAIN SMA_4_Baseline_Lag_1',\n",
    "                                      'LBS_Baseline_Lag_1': 'MCCAIN LBS_Baseline_Lag_1'\n",
    "                                     })\n",
    "    \n",
    "    #analyze sellout data\n",
    "    df = analyze_1(sellout, ['Consolidated Category'], 201910, 202009)\n",
    "                                    \n",
    "    df = df.merge(sellin[['Calendar Week Year','Consolidated Category','MCCAIN LBS','MCCAIN SMA_4','MCCAIN SMA_8','MCCAIN SMA_12','MCCAIN PRECOVID',\n",
    "                          'MCCAIN LBS_Baseline','MCCAIN SMA_4_Baseline','MCCAIN SMA_8_Baseline','MCCAIN SMA_12_Baseline',\n",
    "                          'MCCAIN Lag_1', 'MCCAIN Lag_2', 'MCCAIN Lag_3', 'MCCAIN Lag_4','MCCAIN LBS_Baseline_Lag_1',\n",
    "                          'MCCAIN SMA_4_Lag_1', 'MCCAIN SMA_4_Baseline_Lag_1']], how = 'left', \n",
    "                  left_on = ['Calendar Week Year','Consolidated Category'], right_on = ['Calendar Week Year','Consolidated Category'])\n",
    "    \n",
    "    df = df.fillna({'MCCAIN LBS': 0,\n",
    "                    'MCCAIN SMA_4': 0,\n",
    "                    'MCCAIN SMA_8': 0,\n",
    "                    'MCCAIN SMA_12': 0,\n",
    "                    'MCCAIN PRECOVID': 0,\n",
    "                    'MCCAIN Lag_1': 0,\n",
    "                    'MCCAIN Lag_2': 0,\n",
    "                    'MCCAIN Lag_3': 0,\n",
    "                    'MCCAIN Lag_4': 0,\n",
    "                    'MCCAIN LBS_Baseline': 0,\n",
    "                    'MCCAIN SMA_4_Baseline': 0,\n",
    "                    'MCCAIN SMA_8_Baseline': 0,\n",
    "                    'MCCAIN SMA_12_Baseline': 0,\n",
    "                    'MCCAIN LBS_Baseline_Lag_1':0,\n",
    "                    'MCCAIN SMA_4_Lag_1' : 0,\n",
    "                    'MCCAIN SMA_4_Baseline_Lag_1' : 0\n",
    "                   })\n",
    "    \n",
    "    df['Distributor'] = 'Sysco CA'\n",
    "\n",
    "    df = add_time(df)\n",
    "    \n",
    "    df = df[['Consolidated Category','Distributor','Calendar Week Year',\n",
    "             'LBS','SMA_4','SMA_8','SMA_12','LBS_PRECOVID',\n",
    "             'LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "             'LBS_Lag_1', 'LBS_Lag_2', 'LBS_Lag_3', 'LBS_Lag_4', 'LBS_Baseline_Lag_1', 'SMA_4_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "             'MCCAIN LBS','MCCAIN SMA_4','MCCAIN SMA_8','MCCAIN SMA_12','MCCAIN PRECOVID',\n",
    "             'MCCAIN LBS_Baseline','MCCAIN SMA_4_Baseline','MCCAIN SMA_8_Baseline','MCCAIN SMA_12_Baseline',\n",
    "             'MCCAIN Lag_1', 'MCCAIN Lag_2', 'MCCAIN Lag_3', 'MCCAIN Lag_4','MCCAIN LBS_Baseline_Lag_1','MCCAIN SMA_4_Lag_1','MCCAIN SMA_4_Baseline_Lag_1',\n",
    "             'Week Starting (Sun)','Week Ending (Sat)','COVID Week']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Import Raw Data\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1515212 records\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 962813 entries, 0 to 962812\n",
    "Data columns (total 8 columns):\n",
    " #   Column                    Non-Null Count   Dtype  \n",
    "---  ------                    --------------   -----  \n",
    " 0   Fiscal Week               962813 non-null  int64  \n",
    " 1   Dates                     962813 non-null  object \n",
    " 2   Account Type              962813 non-null  object \n",
    " 3   Customer (Address) STATE  962813 non-null  object \n",
    " 4   NPD Segmentation          930314 non-null  object \n",
    " 5   CS                        962813 non-null  float64\n",
    " 6   LBS                       962813 non-null  float64\n",
    " 7   Category                  962813 non-null  object \n",
    "dtypes: float64(2), int64(1), object(5)\n",
    "memory usage: 58.8+ MB\n",
    "'''\n",
    "\n",
    "#create main dataframe from base file\n",
    "_import = import_file(PATH + 'Sysco CA Weekly Update.csv')\n",
    "\n",
    "print(f'Imported {_import.shape[0]} records', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Apply Dictionary to Raw Data and check for missing classifications\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before adding dictionary: (1515212, 10)\n",
      "Total before dictionary: 432705891.8\n",
      "Total after dictionary: 432705891.8\n",
      "Shape after adding dictionary: (1515212, 17)\n",
      "Nothing missing for COVID Segmentation - L1\n"
     ]
    }
   ],
   "source": [
    "#add dictionary to base data\n",
    "_base = apply_dictionary(_import, 'Sysco - CA.xlsx').dropna(subset = {'Calendar Week Year'}).astype({'Calendar Week Year':'int64'})\n",
    "\n",
    "#check for COVID Segmentation - L1\n",
    "missing = _base[_base['McCain COVID/MWOW Segmentation'].isna()].groupby(['Segment','Sub Segment','Sector'], \n",
    "                                                                     as_index = False, dropna = False)['LBS'].sum()\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print('The following segments are missing:')\n",
    "    display(missing)\n",
    "    missing.to_excel(DICTIONARY + 'Segments Missing Dump\\\\' + dt.now().strftime('%Y%m%d') + '_sysco_ca_L1_missing.xlsx')\n",
    "else:\n",
    "    print(f'Nothing missing for COVID Segmentation - L1', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Execute Analysis\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Region\n",
      "Processing Sell in vs Sell out\n",
      "Starting Teradata connect...\n",
      "Database selected!\n",
      "07/21/2022\n",
      "Query: Execution started...finished. 0:00:46.954472\n",
      "Query: Fetching data started...finished. 0:01:00.686401\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.198690\n",
      "Dim: (155534, 15)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_list = []\n",
    "\n",
    "#Output 1: COVID L1 - List 0\n",
    "_list.append(['City','Cleaned Province Name','McCain COVID/MWOW Segmentation','COVID Segmentation - L2','Restaurant Service Type','Consolidated Category'])\n",
    "\n",
    "print(f'Processing Region', flush = True)\n",
    "output1 = process_list(_base, _list[0])\n",
    "\n",
    "print(f'Processing Sell in vs Sell out', flush = True)\n",
    "output2 = teradata_sales(_base)\n",
    "\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Latest Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-01-05    874\n",
       "2021-06-12    874\n",
       "2021-04-10    874\n",
       "2021-04-17    874\n",
       "2021-04-24    874\n",
       "             ... \n",
       "2020-03-28    874\n",
       "2020-04-04    874\n",
       "2020-04-11    874\n",
       "2020-04-18    874\n",
       "2023-06-17    874\n",
       "Name: Week Ending (Sat), Length: 185, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1['Week Ending (Sat)'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Upload Analysis to Teradata\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected! 07/21/2022 12:40:51 PM\n",
      "Deleting records for: Sysco CA in table: SELLOUT_REGION\n",
      "Inserting records into SELLOUT_REGION\n",
      "Inserted 161690 records\n",
      "Database selected! 07/21/2022 12:44:10 PM\n",
      "Deleting records for: Sysco CA in table: SELLOUT_AND_SELLIN\n",
      "Inserting records into SELLOUT_AND_SELLIN\n",
      "Inserted 370 records\n"
     ]
    }
   ],
   "source": [
    "def td_upload(select_db, df, table_name):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            d = dt.now().strftime('%m/%d/%Y %H:%M:%S %p')\n",
    "            print(f'Database selected! {d}', flush=True)            \n",
    "\n",
    "            delete_from_td(df, table_name, cur)\n",
    "            insert_into_td(df, table_name, cur)\n",
    "\n",
    "def delete_from_td(df, table_name, cur):\n",
    "    distributor = df.groupby('Distributor').size().reset_index().drop(columns=0).to_numpy()[0][0]\n",
    "    \n",
    "    print(f'Deleting records for: {distributor} in table: {table_name}', flush = True)          \n",
    "        \n",
    "    query = '''\n",
    "    DELETE FROM ''' + table_name  + ''' \n",
    "    WHERE \"Distributor\" = ''' + \"'\" + distributor + \"'\"\n",
    "    \n",
    "    cur.execute (query)\n",
    "    \n",
    "def insert_into_td(df, table_name, cur):\n",
    "    insert_list = df.values.tolist()\n",
    "    \n",
    "    #creates ?, ?,.... string used in query for teradata fastload\n",
    "    insert_columns = ('?, ' * len(df.columns)).rstrip(', ')\n",
    "    \n",
    "    print(f'Inserting records into {table_name}', flush = True)\n",
    "    \n",
    "    query = \"INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    #query = \"{fn teradata_try_fastload}INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    \n",
    "    cur.execute (query, insert_list)\n",
    "    \n",
    "    print(f'Inserted {df.shape[0]} records', flush = True)\n",
    "    \n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "td_upload(select_db, output1, 'SELLOUT_REGION')\n",
    "td_upload(select_db, output2, 'SELLOUT_AND_SELLIN')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
