{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCain Sell-In\n",
    "### 1. Libraries and constants\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teradatasql\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "#path where dictionary file can be found\n",
    "#Neil\n",
    "DICTIONARY = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Distributor Sell-Out Dictionaries\\\\'\n",
    "#Joe\n",
    "#DICTIONARY = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Distributor Sell-Out Dictionaries\\\\'\n",
    "\n",
    "#main path\n",
    "#Neil\n",
    "PATH = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\\\'\n",
    "#Joe\n",
    "#PATH = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\\\'\n",
    "\n",
    "#current fiscal year and week (YYYYWW)\n",
    "TIME = pd.read_excel(DICTIONARY + 'Time Definitions.xlsx')\n",
    "\n",
    "#the current week is pulled from the time dictionary table\n",
    "WEEK = int(TIME[(TIME['Week Starting (Mon)'] <= dt.now()) & (TIME['Week Ending (Sun)'] >= dt.now())]['Calendar Week Year'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculation Functions\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling(df, _list):\n",
    "    #groupby _list\n",
    "    df = df.groupby(_list, dropna = False)[['LBS','LBS_LY','LBS_Baseline']].sum().reset_index()\n",
    "    \n",
    "    #set index to all but last column in list\n",
    "    df = df.set_index(_list)\n",
    "    \n",
    "    #add new metric SMA_4 (simple moving average - 4 periods)\n",
    "    #level = all but last 2 items in list\n",
    "    df['LBS_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    df['LBS_Lag_2'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 2)\n",
    "    df['LBS_Lag_3'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 3)\n",
    "    df['LBS_Lag_4'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 4)\n",
    "    \n",
    "    df['SMA_4'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['LBS_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].shift(periods = 1)\n",
    "    df['LBS_LY_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    \n",
    "    df['SMA_4_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4'].shift(periods = 1)\n",
    "    df['SMA_4_LY_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_LY'].shift(periods = 1)\n",
    "    df['SMA_4_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_Baseline'].shift(periods = 1)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "def add_last_year(df, _list):\n",
    "    #list of groupby columns\n",
    "    #last item in list is Calendar Week Year which is used to pull previous history (Baseline Week = Calendar Week Year) of copied dataframe\n",
    "    _groupby = _list.copy()\n",
    "    \n",
    "    _merge_yoy = _list.copy()[0:-1]\n",
    "    _merge_yoy.extend(['YOY Week'])\n",
    "    \n",
    "    _merge_baseline = _list.copy()[0:-1]\n",
    "    _merge_baseline.extend(['Baseline Week'])\n",
    "    \n",
    "    df1 = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #groupby _list\n",
    "    df_new = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #add week dimensions to main dataframe\n",
    "    df_new = df_new.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_yoy, right_on=_groupby).drop(columns={'Calendar Week Year_y'}).rename(columns={'LBS_y':'LBS_LY'})\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_baseline, right_on=_groupby).drop(columns={'Calendar Week Year'}).rename(columns={\n",
    "        'LBS':'LBS_Baseline','Calendar Week Year_x':'Calendar Week Year','LBS_x':'LBS'})\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def add_precovid(df, _list, begin, end):\n",
    "    #datefield should be last in _list\n",
    "    datefield = _list[-1]\n",
    "          \n",
    "    #remove datefield from list\n",
    "    _list = _list[0:-1]\n",
    "    \n",
    "    #filter data not using last and rename columns\n",
    "    _df = df[(df[datefield] >= begin) & (df[datefield] <= end)].groupby(_list)['LBS'].sum() / 52\n",
    "    \n",
    "    return df.merge(\n",
    "        _df, how = 'left', left_on = _list, right_on = _list).rename(\n",
    "        columns = {'LBS_x':'LBS', 'LBS_y':'LBS_PRECOVID'}).fillna(\n",
    "        value = {'LBS_PRECOVID': 0})\n",
    "\n",
    "\n",
    "def add_time(df):\n",
    "    df = df.merge(TIME[['Calendar Week Year','Week Starting (Sun)','Week Ending (Sat)', 'COVID Week']],\n",
    "                   how = 'left', \n",
    "                   on = 'Calendar Week Year')\n",
    "    \n",
    "    df = df.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_weight(df, _list):\n",
    "    test = df.groupby(_list)[['LBS','LBS_Baseline']].sum().reset_index()\n",
    "    test['Wt'] = test['LBS'] / test.groupby(['Calendar Week Year'])['LBS'].transform('sum')\n",
    "    \n",
    "    return test['Wt']\n",
    "\n",
    "def analyze_1(df, _list, begin, end):\n",
    "    if 'Calendar Week Year' not in _list:\n",
    "        _list.extend(['Calendar Week Year'])\n",
    "    \n",
    "     #add last year lbs\n",
    "    df = add_last_year(df, _list)\n",
    "    \n",
    "    #add rolling calculation\n",
    "    df = add_rolling(df, _list)\n",
    "        \n",
    "    #add preCOVID baseline\n",
    "    df = add_precovid(df, _list, begin, end)\n",
    "    \n",
    "    if _list[0] == 'Brand Desc':\n",
    "        df['Wt'] = add_weight(df, _list)\n",
    "    \n",
    "    df = df.round({\n",
    "        'LBS' : 2,    \n",
    "        'SMA_4' : 2,\n",
    "        'SMA_8' : 2,\n",
    "        'SMA_12' : 2,\n",
    "        'LBS_LY' : 2,    \n",
    "        'SMA_4_LY' : 2,\n",
    "        'SMA_8_LY' : 2,\n",
    "        'SMA_12_LY' : 2,\n",
    "        'LBS_Baseline' : 2,    \n",
    "        'SMA_4_Baseline' : 2,\n",
    "        'SMA_8_Baseline' : 2,\n",
    "        'SMA_12_Baseline' : 2,\n",
    "        'LBS_PRECOVID' : 2,\n",
    "        'LBS_Lag_1' : 2,\n",
    "        'LBS_Lag_2' : 2,\n",
    "        'LBS_Lag_3' : 2,\n",
    "        'LBS_Lag_4' : 2,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 2,\n",
    "        'SMA_4_LY_Lag_1' : 2,\n",
    "        'SMA_4_Baseline_Lag_1' : 2\n",
    "        \n",
    "    }).fillna(value = {\n",
    "        'LBS' : 0,    \n",
    "        'SMA_4' : 0,\n",
    "        'SMA_8' : 0,\n",
    "        'SMA_12' : 0,\n",
    "        'LBS_LY' : 0,    \n",
    "        'SMA_4_LY' : 0,\n",
    "        'SMA_8_LY' : 0,\n",
    "        'SMA_12_LY' : 0,\n",
    "        'LBS_Baseline' : 0,    \n",
    "        'SMA_4_Baseline' : 0,\n",
    "        'SMA_8_Baseline' : 0,\n",
    "        'SMA_12_Baseline' : 0,\n",
    "        'LBS_PRECOVID' : 0,\n",
    "        'LBS_Lag_1' : 0,\n",
    "        'LBS_Lag_2' : 0,\n",
    "        'LBS_Lag_3' : 0,\n",
    "        'LBS_Lag_4' : 0,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 0,\n",
    "        'SMA_4_LY_Lag_1' : 0,\n",
    "        'SMA_4_Baseline_Lag_1' : 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_backup(df, file_name):\n",
    "    \n",
    "    df.to_csv(BACKUP + file_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def td_to_pandas(query, cur, title=''):\n",
    "    _data = []\n",
    "    _start=dt.now()\n",
    "    print(dt.now().strftime('%m/%d/%Y %r'))\n",
    "    print(f'{title} Execution started...', end='', flush=True)\n",
    "    cur.execute (query)\n",
    "    print(f'finished. {dt.now() - _start}', flush=True) \n",
    "    _start_fetch=dt.now()\n",
    "    print(f'{title} Fetching data started...', end='', flush=True)\n",
    "    for row in cur.fetchall():\n",
    "        _data.append(row) \n",
    "    print(f'finished. {dt.now() - _start_fetch}', flush=True) \n",
    "    _start=dt.now()\n",
    "    print(f'{title} Creating DataFrame for started...', end='', flush=True)\n",
    "    _df = pd.DataFrame(_data)\n",
    "    _df.columns = [x[0].replace('SAP_', '').lower() for x in cur.description]\n",
    "    print(f'finished. {dt.now() - _start}', flush=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def td_dataframe(select_db, query):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            print('Database selected!', flush=True)            \n",
    "            dim_df = td_to_pandas(query, cur, 'Query:')\n",
    "            print('Dim:', dim_df.shape)\n",
    "    \n",
    "    return dim_df\n",
    "\n",
    "\n",
    "def process_list(df, work_list):\n",
    "    \n",
    "    _process = analyze_1(df, work_list, 201910, 202009)\n",
    "    \n",
    "    _process['Country'] = 'Canada'\n",
    "    \n",
    "    _process = add_time(_process)\n",
    "    \n",
    "    #for standardizing output\n",
    "    work_list.extend(['Country','LBS','SMA_4','SMA_8','SMA_12',\n",
    "                      'YOY Week','LBS_LY','SMA_4_LY','SMA_8_LY','SMA_12_LY',\n",
    "                      'Baseline Week','LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "                      'LBS_Lag_1','LBS_Lag_2','LBS_Lag_3','LBS_Lag_4','LBS_Baseline_Lag_1','LBS_LY_Lag_1',\n",
    "                      'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "                      'LBS_PRECOVID','Week Starting (Sun)','Week Ending (Sat)','COVID Week'])\n",
    "    \n",
    "    if work_list[0] == 'Brand Desc':\n",
    "        work_list.extend(['Wt'])\n",
    "        \n",
    "    return _process[work_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Teradata Queries\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#exclude Wong Wing - 4/7/21\n",
    "#('0048','0052','0053','0054','0055','0079','0546','0547')\n",
    "\n",
    "def teradata_sales():\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Sysco COVID Performance;StartTime=20200901T131109;JobID=68215;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query ='''select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "    a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "    a15.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_2,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_2,\n",
    "    a15.CUSTOMER_HIER_LVL_2_NAME as CUSTOMER_HIER_LVL_2_NAME,\n",
    "    a11.CUSTOMER_ID as CUSTOMER_ID,\n",
    "    a17.CUSTOMER_NAME as CUSTOMER_NAME,\n",
    "    a13.DIVISION_ID as DIVISION,\n",
    "    a16.DIVISION_NAME as DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "    a12.SUB_CATEGORY_SHORT_CODE as SUB_CATEGORY_SHORT_CODE,\n",
    "    a12.SUB_CATEGORY_DESC as SUB_CATEGORY_DESC,\n",
    "    a12.FAMILY_SHORT_CODE as FAMILY_SHORT_CODE,\n",
    "    a12.FAMILY_DESC as FAMILY_DESC,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "    a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "    sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "     on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "     on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "     on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a15\n",
    "     on (a11.CUSTOMER_ID = a15.CUSTOMER and \n",
    "    a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL and \n",
    "    a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a16\n",
    "     on (a13.DIVISION_ID = a16.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_CUSTOMER as a17\n",
    "     on (a11.CUSTOMER_ID = a17.CUSTOMER_ID)\n",
    "    where(a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "     and a11.SALES_ORGANISATION_ID in ('CA01')\n",
    "     and a11.DISTRIBUTION_CHANNEL_ID in ('10'))\n",
    "     and a12.FAMILY_SHORT_CODE not in ('0048','0052','0053','0054','0055','0079','0546','0547')\n",
    "     and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(WEEK - 1) + ''' \n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    a14.CALENDAR_WEEK_NAME,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "    a15.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_2,CAST(10 AS INTEGER)),\n",
    "    a15.CUSTOMER_HIER_LVL_2_NAME,\n",
    "    a11.CUSTOMER_ID,\n",
    "    a17.CUSTOMER_NAME,\n",
    "    a13.DIVISION_ID,\n",
    "    a16.DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC,\n",
    "    a12.SUB_CATEGORY_SHORT_CODE,\n",
    "    a12.SUB_CATEGORY_DESC,\n",
    "    a12.FAMILY_SHORT_CODE,\n",
    "    a12.FAMILY_DESC,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "    a13.MATERIAL_DESCRIPTION;\n",
    "    '''\n",
    "    \n",
    "    #build dataframe from teradata query\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    #return transformed dataframe\n",
    "    return transform_teradata(df)\n",
    "\n",
    "\n",
    "def teradata_brand():\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=Brand COVID Performance;StartTime=20200901T113649;JobID=55922;Importance=666;'  FOR SESSION;\n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query ='''select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "    a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "    a15.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a11.CUSTOMER_ID as CUSTOMER_ID,\n",
    "    a17.CUSTOMER_NAME as CUSTOMER_NAME,\n",
    "    a13.DIVISION_ID as DIVISION,\n",
    "    a16.DIVISION_NAME as DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "    a12.BRAND_SHORT_CODE as BRAND_SHORT_CODE,\n",
    "    a12.BRAND_DESC as BRAND_DESC,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "    a13.MATERIAL_DESCRIPTION  MATERIAL_NAME,\n",
    "    sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "     on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "     on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "     on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a15\n",
    "     on (a11.CUSTOMER_ID = a15.CUSTOMER and \n",
    "    a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL and \n",
    "    a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a16\n",
    "     on (a13.DIVISION_ID = a16.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_CUSTOMER as a17\n",
    "     on (a11.CUSTOMER_ID = a17.CUSTOMER_ID)\n",
    "    where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "     and a11.SALES_ORGANISATION_ID in ('CA01')\n",
    "     and a11.DISTRIBUTION_CHANNEL_ID in ('10')\n",
    "     and a12.BRAND_SHORT_CODE in ('042', '068', '104', '112', '156', '225', '240', '500', '005', '002'))\n",
    "     and a14.CALENDAR_WEEK_NAME between 201901 and ''' + str(WEEK - 1) + '''\n",
    "     and a11.CUSTOMER_ID not in ('1000076341','1000076333','1000076214','1000076325','1000076175','1000086306','1000086305','1000086301')\n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "    (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    a14.CALENDAR_WEEK_NAME,\n",
    "    (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "    RIGHT(a15.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "    a15.CUSTOMER_HIER_LVL_1_NAME,\n",
    "    a11.CUSTOMER_ID,\n",
    "    a17.CUSTOMER_NAME,\n",
    "    a13.DIVISION_ID,\n",
    "    a16.DIVISION_NAME,\n",
    "    a12.CATEGORY_SHORT_CODE,\n",
    "    a12.CATEGORY_DESC,\n",
    "    a12.BRAND_SHORT_CODE,\n",
    "    a12.BRAND_DESC,\n",
    "    TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "    a13.MATERIAL_DESCRIPTION;\n",
    "    '''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return transform_teradata(df)\n",
    "\n",
    "\n",
    "def transform_teradata(df):\n",
    "\n",
    "    #update category_desc values based on row qualifiers\n",
    "    df['Consolidated Category'] = df['category_desc']\n",
    "    df.loc[df['Consolidated Category'] == 'Sweet Potato' , 'Consolidated Category'] = 'Potato'\n",
    "    df.loc[df['Consolidated Category'] != 'Potato' , 'Consolidated Category'] = 'Prepared Foods'\n",
    "    \n",
    "    #update calendar_week_name to numeric for future functions\n",
    "    df['calendar_week_number'] = pd.to_numeric(df['calendar_week_number'], errors = 'coerce')\n",
    "    \n",
    "    df = df.astype({'actual_volume_lbs':'float64'})\n",
    "\n",
    "    df = df.rename(columns={'actual_volume_lbs':'LBS',\n",
    "                          'calendar_week_number':'Calendar Week Year',\n",
    "                          'division_name':'Division Name',\n",
    "                          'material_pricing_group_description':'Material Pricing Group Description',\n",
    "                          'customer_hier_lvl_1_name':'Customer L1 Name',\n",
    "                          'brand_desc':'Brand Desc',\n",
    "                          'family_desc': 'Family Desc'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execute Analysis\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected!\n",
      "05/19/2022 02:57:18 PM\n",
      "Query: Execution started...finished. 0:02:15.746866\n",
      "Query: Fetching data started...finished. 0:04:10.319712\n",
      "Query: Creating DataFrame for started...finished. 0:00:01.311164\n",
      "Dim: (561999, 21)\n",
      "Database selected!\n",
      "05/19/2022 03:03:53 PM\n",
      "Query: Execution started...finished. 0:03:23.073494\n",
      "Query: Fetching data started...finished. 0:00:01.642879\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.019830\n",
      "Dim: (12124, 17)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#pull sales data from teradata\n",
    "_sales = teradata_sales()\n",
    "\n",
    "#blank list to add lists to\n",
    "_list = []\n",
    "\n",
    "#Output 1: Division Name - List 0\n",
    "_list.append(['Division Name','Consolidated Category'])\n",
    "\n",
    "#Output 2: MPG - List 1\n",
    "_list.append(['Family Desc'])\n",
    "\n",
    "#Create dataframes\n",
    "output1 = process_list(_sales, _list[0])\n",
    "output2 = process_list(_sales[_sales['Division Name'] == 'Retail'], _list[1])\n",
    "\n",
    "#pull sales data by brand from teradata\n",
    "#seperate from the query above becaues of filters\n",
    "_brand = teradata_brand()\n",
    "\n",
    "#Output 3: Brand - List 2\n",
    "_list.append(['Brand Desc'])\n",
    "\n",
    "output3 = process_list(_brand, _list[2])\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Teradata Update\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected! 05/19/2022 03:07:24 PM\n",
      "Deleting records for: Canada in table: SELLIN_DIVISION\n",
      "Inserting records into SELLIN_DIVISION\n",
      "Inserted 1238 records\n",
      "Database selected! 05/19/2022 03:11:45 PM\n",
      "Deleting records for: Canada in table: SELLIN_RETAIL\n",
      "Inserting records into SELLIN_RETAIL\n",
      "Inserted 1968 records\n",
      "Database selected! 05/19/2022 03:13:39 PM\n",
      "Deleting records for: Canada in table: SELLIN_BRAND\n",
      "Inserting records into SELLIN_BRAND\n",
      "Inserted 1740 records\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "def td_upload(select_db, df, table_name):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            d = dt.now().strftime('%m/%d/%Y %r')\n",
    "            print(f'Database selected! {d}', flush=True)            \n",
    "\n",
    "            delete_from_td(df, table_name, cur)\n",
    "            insert_into_td(df, table_name, cur)\n",
    "\n",
    "def delete_from_td(df, table_name, cur):\n",
    "    distributor = df.groupby('Country').size().reset_index().drop(columns=0).to_numpy()[0][0]\n",
    "    \n",
    "    print(f'Deleting records for: {distributor} in table: {table_name}', flush = True)          \n",
    "        \n",
    "    query = '''\n",
    "    DELETE FROM ''' + table_name  + ''' \n",
    "    WHERE \"Country\" = ''' + \"'\" + distributor + \"'\"\n",
    "    \n",
    "    cur.execute (query)\n",
    "    \n",
    "def insert_into_td(df, table_name, cur):\n",
    "    insert_list = df.values.tolist()\n",
    "    \n",
    "    #creates ?, ?,.... string used in query for teradata fastload\n",
    "    insert_columns = ('?, ' * len(df.columns)).rstrip(', ')\n",
    "\n",
    "    print(f'Inserting records into {table_name}', flush = True)\n",
    "    \n",
    "    query = \"INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    #query = \"{fn teradata_try_fastload}INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    \n",
    "    cur.execute (query, insert_list)\n",
    "    \n",
    "    print(f'Inserted {df.shape[0]} records', flush = True)\n",
    "    \n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "td_upload(select_db, output1, 'SELLIN_DIVISION')\n",
    "td_upload(select_db, output2, 'SELLIN_RETAIL')\n",
    "td_upload(select_db, output3, 'SELLIN_BRAND')\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Output to Excel\n",
    "Only used for Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: SELLIN_DIVISION\n",
      "Database selected!\n",
      "04/06/2022 09:49:48 PM\n",
      "Query: Execution started...finished. 0:00:04.607016\n",
      "Query: Fetching data started...finished. 0:00:00.318056\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.013547\n",
      "Dim: (3066, 31)\n",
      "Exporting table: SELLIN_MPG\n",
      "Database selected!\n",
      "04/06/2022 09:49:56 PM\n",
      "Query: Execution started...finished. 0:00:04.552704\n",
      "Query: Fetching data started...finished. 0:00:00.667789\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.031000\n",
      "Dim: (6460, 31)\n",
      "Exporting table: SELLIN_BRAND\n",
      "Database selected!\n",
      "04/06/2022 09:50:04 PM\n",
      "Query: Execution started...finished. 0:00:04.403383\n",
      "Query: Fetching data started...finished. 0:00:00.359017\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.014447\n",
      "Dim: (3554, 31)\n",
      "Exporting table: SELLIN_RETAIL\n",
      "Database selected!\n",
      "04/06/2022 09:50:11 PM\n",
      "Query: Execution started...finished. 0:00:02.472207\n",
      "Query: Fetching data started...finished. 0:00:00.186778\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.005410\n",
      "Dim: (1903, 30)\n",
      "Exporting table: SELLIN_CUSTOMER_L1\n",
      "Database selected!\n",
      "04/06/2022 09:50:16 PM\n",
      "Query: Execution started...finished. 0:00:06.946710\n",
      "Query: Fetching data started...finished. 0:00:16.236338\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.149300\n",
      "Dim: (34000, 31)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "table_list = ['SELLIN_DIVISION','SELLIN_MPG','SELLIN_BRAND','SELLIN_RETAIL','SELLIN_CUSTOMER_L1']\n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "#writer = pd.ExcelWriter(PATH + str(dt.now().strftime('%m-%d-%Y')) + 'Weekly Sellin Data.csv', engine='xlsxwriter')\n",
    "\n",
    "for table_name in table_list:\n",
    "    print(f'Exporting table: {table_name}', flush = True)\n",
    "    query = '''\n",
    "    SELECT * FROM ''' + table_name + '''\n",
    "    '''\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    df.to_csv(PATH + str(dt.now().strftime('%Y%m%d')) + ' ' + table_name + '.csv')\n",
    "    #df.to_excel(writer, sheet_name = table_name, index = False)\n",
    "    \n",
    "writer.save()\n",
    "\n",
    "print('All done', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: SELLOUT_REGION\n",
      "Database selected!\n",
      "09/24/2021 03:55:59 PM\n",
      "Query: Execution started...finished. 0:00:02.689834\n",
      "Query: Fetching data started...finished. 0:08:25.960192\n",
      "Query: Creating DataFrame for started...finished. 0:00:04.762162\n",
      "Dim: (1036856, 30)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "table_list = ['SELLOUT_REGION']\n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "for table_name in table_list:\n",
    "    print(f'Exporting table: {table_name}', flush = True)\n",
    "    query = '''\n",
    "    SELECT * FROM ''' + table_name + '''\n",
    "    '''\n",
    "    df = td_dataframe(select_db, query)\n",
    "    df.to_csv(table_name, index = False)\n",
    "\n",
    "print('All done', flush = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
