{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFG Sell-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: \n",
    "1. Process PFG data (source = mealticket.com)\n",
    "2. Analyze data using COVID segmentation\n",
    "3. Compare sell-out (PFG) to sell-in (McCain) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load libraries, initiate folder/file paths\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import teradatasql\n",
    "\n",
    "#path where dictionary file can be found\n",
    "#Neil\n",
    "DICTIONARY = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Distributor Sell-Out Dictionaries\\\\'\n",
    "#Joe\n",
    "#DICTIONARY = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Distributor Sell-Out Dictionaries\\\\'\n",
    "\n",
    "#main path\n",
    "#Neil\n",
    "PATH = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\\\'\n",
    "#Joe\n",
    "#PATH = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\\\'\n",
    "\n",
    "#backup path\n",
    "#Neil\n",
    "BACKUP = r'C:\\Users\\NEWATTER\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\Backups\\\\'\n",
    "#Joe\n",
    "#BACKUP = r'C:\\Users\\jcronk\\McCain Foods Limited\\GNA Data Strategy & Analytics - COVID Recovery\\Historical Sell-Out Sales\\Backups\\\\'\n",
    "\n",
    "#time dataframe\n",
    "TIME = pd.read_excel(DICTIONARY + 'Time Definitions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Dictionary\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_states():\n",
    "    us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY'\n",
    "    }\n",
    "\n",
    "    # thank you to @kinghelix and @trevormarburger for this idea\n",
    "    abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))\n",
    "    \n",
    "    return pd.DataFrame.from_dict(abbrev_us_state, orient = 'index', columns = ['State Name']).rename_axis('State').reset_index()\n",
    "\n",
    "\n",
    "def apply_dictionary(df, file_name):\n",
    "    \n",
    "    print(f'Starting dataframe shape: {df.shape}', flush = True)\n",
    "    \n",
    "    #create dictionary object from Excel file\n",
    "    #adding sheet_name = None makes it a dictionary type\n",
    "    _dict = pd.read_excel(DICTIONARY + file_name, sheet_name = None, engine='openpyxl')\n",
    "    \n",
    "    #for testing, keys = sheet names\n",
    "    #print(_dict.keys())\n",
    "    \n",
    "    #create DataFrame from dictionary object called dict (short for dictionary)\n",
    "    dict_df = pd.DataFrame.from_dict(_dict['Segment Mapping'])\n",
    "    sku_df = pd.DataFrame.from_dict(_dict['SKU Mapping'])\n",
    "    \n",
    "    man_df = pd.DataFrame.from_dict(_dict['Manufacturer Mapping'])\n",
    "    \n",
    "    manufacturer = man_df[man_df['Mfg. Inclusion Flag'] == 'Include']['Manufacturer'].tolist()\n",
    "    \n",
    "    print(f'These manufacturers were included: {manufacturer}', flush = True)\n",
    "    \n",
    "    excluded = df['Manufacturer'].value_counts().reset_index().drop(columns={'Manufacturer'}).rename(columns = {'index':'Manufacturer'})\n",
    "    excluded = excluded[~excluded['Manufacturer'].isin(manufacturer)]\n",
    "    \n",
    "    #display(df.groupby['Manufacturer'].size().reset_index().drop(columns={0}))\n",
    "    \n",
    "    print(f'These manufacturers were not included: {excluded}', flush = True)\n",
    "    \n",
    "    df = df[df['Manufacturer'].isin(manufacturer)]\n",
    "    \n",
    "    #strip blanks from segment\n",
    "    df.loc[:, 'Segment'] = df['Segment'].str.strip()\n",
    "    \n",
    "    #convert Invoice Week to date\n",
    "    df.loc[:, 'Invoice Week'] = pd.to_datetime(df['Invoice Week'])\n",
    "    \n",
    "    \n",
    "    #print shape of df (dimensions)\n",
    "    print(f'Shape before adding dictionary: {df.shape}', flush = True)\n",
    "    #add lower case for merging\n",
    "    \n",
    "    dict_df.loc[:, 'customer_class_lower'] = dict_df['Customer Class'].str.lower()\n",
    "    dict_df.loc[:, 'segment_lower'] = dict_df['Segment'].str.strip().str.lower()\n",
    "    dict_df.loc[:, 'account_type_lower'] = dict_df['Account Type'].str.strip().str.lower()\n",
    "    \n",
    "    #Type Name\tCategory Name\tCOVID Segmentation - L1\tCOVID Segmentation - L2\tCOVID Segmentation - (Restaurants)\tCOVID Segmentation - (Restaurants: Sub-Segment)\tRestaurant Service Type\n",
    "    \n",
    "    dict_df = dict_df.groupby(['customer_class_lower','segment_lower','account_type_lower','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                               'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type','Cuisine Type']\n",
    "                              , dropna = False).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    #add lower case key columns for merging (removes case mismatch)\n",
    "    \n",
    "    df.loc[:, 'customer_class_lower'] = df['Customer Class'].str.lower()\n",
    "    df.loc[:, 'segment_lower'] = df['Segment'].str.strip().str.lower()\n",
    "    df.loc[:, 'account_type_lower'] = df['Account Type'].str.strip().str.lower()\n",
    "    \n",
    "    \n",
    "    df = df.merge(dict_df, how = 'left', left_on = [\n",
    "        'customer_class_lower','segment_lower','account_type_lower'],\n",
    "        right_on = ['customer_class_lower','segment_lower','account_type_lower']).drop(columns = {\n",
    "            'customer_class_lower','segment_lower','account_type_lower'})\n",
    "    \n",
    "    #print(f'Shape after 1st merge: {df.shape}', flush = True)\n",
    "    \n",
    "    sku_df = sku_df.groupby(['Mfr SKU','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy','Case Weight Lbs'], dropna = False).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    #both SKU fields need to be strings in order to match\n",
    "    sku_df.loc[:, 'Mfr SKU'] = sku_df['Mfr SKU'].astype(str)\n",
    "    df.loc[:, 'MFR SKU'] = df['MFR SKU'].astype(str)\n",
    "    \n",
    "    df = df.merge(sku_df[['Mfr SKU','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy','Case Weight Lbs']], how = 'left', left_on = ['MFR SKU'], right_on = ['Mfr SKU']).drop(columns = {'Mfr SKU'})\n",
    "    \n",
    "    #print(f'Shape after 2nd merge: {df.shape}', flush = True)\n",
    "    \n",
    "    df = df.astype({'Qty':'float64','Weight':'float64','Case Weight Lbs':'float64'})\n",
    "    \n",
    "    #calculate case weight if weight = 0 and qty > 0\n",
    "    df.loc[(df['Weight'] == 0) & (df['Qty'] > 0), 'Weight'] = df['Case Weight Lbs'] * df['Qty']\n",
    "    \n",
    "    #add time\n",
    "    \n",
    "    df = df.merge(TIME[['Week Starting (Mon)', 'Calendar Week Year']], how = 'left', left_on = ['Invoice Week'], right_on = ['Week Starting (Mon)']).drop(columns={'Week Starting (Mon)'})\n",
    "    \n",
    "    #rename metric Weight for consistancy\n",
    "    df = df.rename(columns={\n",
    "        'Weight':'LBS',\n",
    "        'MFR SKU':'SKU ID'})\n",
    "    \n",
    "    df = df[~df['Calendar Week Year'].isna()]\n",
    "    \n",
    "    #Clean US States\n",
    "    df.loc[df['State'] == 'tn', 'State'] = 'TN'\n",
    "    df = df.merge(us_states(), how = 'left', on = 'State')\n",
    "    df.loc[df['State Name'].isna(), ['State', 'State Name']] = 'None'\n",
    "    \n",
    "    print(f'Shape after adding dictionary: {df.shape}', flush = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import File\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(file_name):\n",
    "    \n",
    "#import file\n",
    "    if '.csv' in file_name:\n",
    "        _import = pd.read_csv(file_name, low_memory = False)\n",
    "    else:\n",
    "        _import = pd.read_excel(file_name, engine='openpyxl')\n",
    "    \n",
    "    return _import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculation Functions\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling(df, _list):\n",
    "    #groupby _list\n",
    "    df = df.groupby(_list, dropna = False)[['LBS','LBS_LY','LBS_Baseline']].sum().reset_index()\n",
    "    \n",
    "    #set index to all but last column in list\n",
    "    df = df.set_index(_list)\n",
    "    \n",
    "    #add new metric SMA_4 (simple moving average - 4 periods)\n",
    "    #level = all but last 2 items in list\n",
    "    df['LBS_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    df['LBS_Lag_2'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 2)\n",
    "    df['LBS_Lag_3'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 3)\n",
    "    df['LBS_Lag_4'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 4)\n",
    "    \n",
    "    df['SMA_4'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12'] = df.groupby(level=_list[0:-1])['LBS'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_LY'] = df.groupby(level=_list[0:-1])['LBS_LY'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['SMA_4_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "    df['SMA_8_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(8, min_periods=1).mean())\n",
    "    df['SMA_12_Baseline'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].apply(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    \n",
    "    df['LBS_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['LBS_Baseline'].shift(periods = 1)\n",
    "    df['LBS_LY_Lag_1'] = df.groupby(level=_list[0:-1])['LBS'].shift(periods = 1)\n",
    "    \n",
    "    df['SMA_4_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4'].shift(periods = 1)\n",
    "    df['SMA_4_LY_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_LY'].shift(periods = 1)\n",
    "    df['SMA_4_Baseline_Lag_1'] = df.groupby(level=_list[0:-1])['SMA_4_Baseline'].shift(periods = 1)\n",
    "    \n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "def add_last_year(df, _list):\n",
    "    #list of groupby columns\n",
    "    #last item in list is Calendar Week Year which is used to pull previous history (Baseline Week = Calendar Week Year) of copied dataframe\n",
    "    _groupby = _list.copy()\n",
    "    \n",
    "    _merge_yoy = _list.copy()[0:-1]\n",
    "    _merge_yoy.extend(['YOY Week'])\n",
    "    \n",
    "    _merge_baseline = _list.copy()[0:-1]\n",
    "    _merge_baseline.extend(['Baseline Week'])\n",
    "    \n",
    "    df1 = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #groupby _list\n",
    "    df_new = df.groupby(_list, dropna = False)['LBS'].sum().reset_index()\n",
    "    \n",
    "    #add week dimensions to main dataframe\n",
    "    df_new = df_new.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_yoy, right_on=_groupby).drop(columns={'Calendar Week Year_y'}).rename(columns={'LBS_y':'LBS_LY'})\n",
    "    \n",
    "    df_new = df_new.merge(df1, how='left', left_on=_merge_baseline, right_on=_groupby).drop(columns={'Calendar Week Year'}).rename(columns={\n",
    "        'LBS':'LBS_Baseline','Calendar Week Year_x':'Calendar Week Year','LBS_x':'LBS'})\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def add_precovid(df, _list, begin, end):\n",
    "    #datefield should be last in _list\n",
    "    datefield = _list[-1]\n",
    "          \n",
    "    #remove datefield from list\n",
    "    _list = _list[0:-1]\n",
    "    \n",
    "    #filter data not using last and rename columns\n",
    "    _df = df[(df[datefield] >= begin) & (df[datefield] <= end)].groupby(_list)['LBS'].sum() / 52\n",
    "    \n",
    "    return df.merge(\n",
    "        _df, how = 'left', left_on = _list, right_on = _list).rename(\n",
    "        columns = {'LBS_x':'LBS', 'LBS_y':'LBS_PRECOVID'}).fillna(\n",
    "        value = {'LBS_PRECOVID': 0})\n",
    "\n",
    "\n",
    "def add_time(df):\n",
    "    df = df.merge(TIME[['Calendar Week Year','Week Starting (Sun)','Week Ending (Sat)', 'COVID Week']],\n",
    "                   how = 'left', \n",
    "                   on = 'Calendar Week Year')\n",
    "    \n",
    "    df = df.merge(TIME[['Calendar Week Year','YOY Week','Baseline Week']], how = 'left', left_on = 'Calendar Week Year', right_on = 'Calendar Week Year')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_1(df, _list, begin, end):\n",
    "    if 'Calendar Week Year' not in _list:\n",
    "        _list.extend(['Calendar Week Year'])\n",
    "    \n",
    "    df = full_dataframe(df, _list)\n",
    "    \n",
    "    #add last year lbs\n",
    "    df = add_last_year(df, _list)\n",
    "    \n",
    "    #add rolling calculation\n",
    "    df = add_rolling(df, _list)\n",
    "        \n",
    "    #add preCOVID baseline\n",
    "    df = add_precovid(df, _list, begin, end)\n",
    "    \n",
    "    df = df.round({\n",
    "        'LBS' : 2,    \n",
    "        'SMA_4' : 2,\n",
    "        'SMA_8' : 2,\n",
    "        'SMA_12' : 2,\n",
    "        'LBS_LY' : 2,    \n",
    "        'SMA_4_LY' : 2,\n",
    "        'SMA_8_LY' : 2,\n",
    "        'SMA_12_LY' : 2,\n",
    "        'LBS_Baseline' : 2,    \n",
    "        'SMA_4_Baseline' : 2,\n",
    "        'SMA_8_Baseline' : 2,\n",
    "        'SMA_12_Baseline' : 2,\n",
    "        'LBS_PRECOVID' : 2,\n",
    "        'LBS_Lag_1' : 2,\n",
    "        'LBS_Lag_2' : 2,\n",
    "        'LBS_Lag_3' : 2,\n",
    "        'LBS_Lag_4' : 2,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 2,\n",
    "        'SMA_4_LY_Lag_1' : 2,\n",
    "        'SMA_4_Baseline_Lag_1' : 2\n",
    "        \n",
    "    }).fillna(value = {\n",
    "        'LBS' : 0,    \n",
    "        'SMA_4' : 0,\n",
    "        'SMA_8' : 0,\n",
    "        'SMA_12' : 0,\n",
    "        'LBS_LY' : 0,    \n",
    "        'SMA_4_LY' : 0,\n",
    "        'SMA_8_LY' : 0,\n",
    "        'SMA_12_LY' : 0,\n",
    "        'LBS_Baseline' : 0,    \n",
    "        'SMA_4_Baseline' : 0,\n",
    "        'SMA_8_Baseline' : 0,\n",
    "        'SMA_12_Baseline' : 0,\n",
    "        'LBS_PRECOVID' : 0,\n",
    "        'LBS_Lag_1' : 0,\n",
    "        'LBS_Lag_2' : 0,\n",
    "        'LBS_Lag_3' : 0,\n",
    "        'LBS_Lag_4' : 0,\n",
    "        'LBS_Baseline_Lag_1': 2,\n",
    "        'LBS_LY_Lag_1': 2,\n",
    "        'SMA_4_Lag_1' : 0,\n",
    "        'SMA_4_LY_Lag_1' : 0,\n",
    "        'SMA_4_Baseline_Lag_1' : 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_backup(df, file_name):\n",
    "    \n",
    "    df.to_csv(BACKUP + file_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def td_to_pandas(query, cur, title=''):\n",
    "    _data = []\n",
    "    _start=dt.now()\n",
    "    print(dt.now().strftime('%m/%d/%Y %r'))\n",
    "    print(f'{title} Execution started...', end='', flush=True)\n",
    "    cur.execute (query)\n",
    "    print(f'finished. {dt.now() - _start}', flush=True) \n",
    "    _start_fetch=dt.now()\n",
    "    print(f'{title} Fetching data started...', end='', flush=True)\n",
    "    for row in cur.fetchall():\n",
    "        _data.append(row) \n",
    "    print(f'finished. {dt.now() - _start_fetch}', flush=True) \n",
    "    _start=dt.now()\n",
    "    print(f'{title} Creating DataFrame for started...', end='', flush=True)\n",
    "    _df = pd.DataFrame(_data)\n",
    "    _df.columns = [x[0].replace('SAP_', '').lower() for x in cur.description]\n",
    "    print(f'finished. {dt.now() - _start}', flush=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def td_dataframe(select_db, query):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            print('Database selected!', flush=True)            \n",
    "            dim_df = td_to_pandas(query, cur, 'Query:')\n",
    "            print('Dim:', dim_df.shape)\n",
    "    \n",
    "    return dim_df\n",
    "\n",
    "\n",
    "def restaurants(df):\n",
    "    #restaurants = df.loc[df['COVID Segmentation - (Restaurants)'] == 'Restaurants', :]\n",
    "    \n",
    "    #Rename rows\n",
    "    df.loc[df['COVID Segmentation - L2'] == 'Independents (IOs) / Local Eateries / Takeaway', 'COVID Segmentation - L2'] = 'IO'\n",
    "    df.loc[\n",
    "        (df['COVID Segmentation - L2'] == 'All Other') | \n",
    "        (df['COVID Segmentation - L2'] == 'National Account') | \n",
    "        (df['COVID Segmentation - L2'] == 'Region Chains')| \n",
    "        (df['COVID Segmentation - L2'] == 'National Accounts'),\n",
    "        'COVID Segmentation - L2'] = 'Chain'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_list(df, work_list):\n",
    "    \n",
    "    _process = analyze_1(df, work_list, 201910, 202009)\n",
    "    \n",
    "    _process = restaurants(_process)\n",
    "    \n",
    "    _process['Distributor'] = 'PFG'\n",
    "    \n",
    "    _process = add_time(_process)\n",
    "    \n",
    "    #for standardizing output\n",
    "    work_list.extend(['Distributor','LBS','SMA_4','SMA_8','SMA_12',\n",
    "                      'YOY Week','LBS_LY','SMA_4_LY','SMA_8_LY','SMA_12_LY',\n",
    "                      'Baseline Week','LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "                      'LBS_Lag_1','LBS_Lag_2','LBS_Lag_3','LBS_Lag_4','LBS_Baseline_Lag_1','LBS_LY_Lag_1',\n",
    "                      'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "                      'LBS_PRECOVID','Week Starting (Sun)','Week Ending (Sat)','COVID Week'])\n",
    "    \n",
    "    if 'SKU ID' in work_list:\n",
    "        #last 26 weeks in dataframe\n",
    "        week_list = _process.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0}).sort_values(by = 'Calendar Week Year', ascending = True).squeeze().tolist()[-26:]\n",
    "        \n",
    "        #filter to only the last 26 weeks\n",
    "        _process = _process[_process['Calendar Week Year'].isin(week_list)]\n",
    "    \n",
    "    return _process[work_list]\n",
    "\n",
    "def is_missing(df):\n",
    "    #check for COVID Segmentation - L1\n",
    "    missing = _new_df[_new_df['COVID Segmentation - L1'].isna()].groupby(['Customer Class','Segment', 'Account Type'], as_index = False, dropna = False)['LBS'].sum()\n",
    "\n",
    "    if len(missing) > 0:\n",
    "        print('The following segments are missing:')\n",
    "        display(missing)\n",
    "        missing.to_excel(DICTIONARY + 'Segments Missing Dump\\\\' + dt.now().strftime('%Y%m%d') + '_PFG_L1_missing.xlsx', index = False)\n",
    "    else:\n",
    "        print(f'Nothing missing for COVID Segmentation - L1', flush = True)\n",
    "\n",
    "    #check for product\n",
    "    missing = _new_df[_new_df['Consolidated Category'].isna()].groupby(['Brand','Sub-Category','SKU ID','Item Name','Pack','Size','Unit Type','GTIN','Dist SKU'], as_index = False, dropna = False)['LBS'].sum()\n",
    "\n",
    "    if len(missing) > 0:\n",
    "        print('The following products are missing:')\n",
    "        display(missing)\n",
    "        missing.to_excel(DICTIONARY + 'Segments Missing Dump\\\\' + dt.now().strftime('%Y%m%d') + r'_PFG_product_missing.xlsx', index = False)\n",
    "    else:\n",
    "        print(f'Nothing missing for Product', flush = True)\n",
    "\n",
    "        \n",
    "def full_dataframe(df, _list):\n",
    "    weeks = df.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})\n",
    "    segments = df.groupby(_list[0:-1]).size().reset_index().drop(columns={0})\n",
    "    \n",
    "    _df = segments.assign(key=1).merge(weeks.assign(key=1), how='outer', on='key').drop(columns = {'key'}) \n",
    "    \n",
    "    return _df.merge(df, how = 'left', on = _list) \n",
    "\n",
    "def clean_city(df):\n",
    "    df['City'] = df['City'].str.strip()\n",
    "    df['City'] = df['City'].str.upper()\n",
    "    df['City'].fillna('NA', inplace = True)\n",
    "    \n",
    "    #cities = 'TORONTO|MONTREAL|OTTAWA|CALGARY|VANCOUVER|WINNIPEG|MONTREAL|HAMILTON|HALIFAX'\n",
    "    cities = 'NOT USED CURRENTLY'\n",
    "    \n",
    "    #change each city name to the name of the city that matches, cleans up the city names\n",
    "    for c in cities.split('|'):\n",
    "        df.loc[df['City'].str.match(c), 'City'] = c\n",
    "    \n",
    "    #change all other cities to NA\n",
    "    df.loc[~df['City'].str.match(cities), 'City'] = 'NA'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sell-in vs. Sell-out\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teradata_sales(sellout):\n",
    "    #SET QUERY_BAND = 'ApplicationName=MicroStrategy;Version=9.0;ClientUser=NEWATTER;Source=Vantage; Action=BEK Performance;StartTime=20200901T101924;JobID=55096;Importance=666;'  FOR SESSION;\n",
    "    \n",
    "    #the current week is pulled from the time dictionary table\n",
    "    to_week = int(TIME[(TIME['Week Starting (Mon)'] <= dt.now()) & (TIME['Week Ending (Sun)'] >= dt.now())]['Calendar Week Year'].values)\n",
    "    \n",
    "    print(f'Starting Teradata connect...', flush = True)\n",
    "    \n",
    "    select_db = \"DATABASE DL_GBL_TAS_BI\"\n",
    "\n",
    "    query = '''\n",
    "    select a14.FISCAL_WEEK_NUMBER as FISCAL_WEEK_NUMBER,\n",
    "        (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW) as FISCAL_WEEK,\n",
    "        a14.CALENDAR_WEEK_NAME as CALENDAR_WEEK_NUMBER,\n",
    "        (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW) as CALENDAR_WEEK,\n",
    "        RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) as CUSTOMER_HIER_LVL_1,\n",
    "        a16.CUSTOMER_HIER_LVL_1_NAME as CUSTOMER_HIER_LVL_1_NAME,\n",
    "        a13.DIVISION_ID as DIVISION,\n",
    "        a17.DIVISION_NAME as DIVISION_NAME,\n",
    "        a12.CATEGORY_SHORT_CODE as CATEGORY_SHORT_CODE,\n",
    "        a12.CATEGORY_DESC as CATEGORY_DESC,\n",
    "        a12.SUB_CATEGORY_SHORT_CODE as SUB_CATEGORY_SHORT_CODE,\n",
    "        a12.SUB_CATEGORY_DESC as SUB_CATEGORY_DESC,\n",
    "        a15.MATERIAL_PRICING_GROUP_ID as MATERIAL_PRICING_GROUP_ID,\n",
    "        a18.MATERIAL_PRICING_GROUP_DESCRIPTION as MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "        TRIM (LEADING '0' FROM a13.MATERIAL_ID) as MATERIAL_ID,\n",
    "        a13.MATERIAL_DESCRIPTION as MATERIAL_NAME,\n",
    "        sum(a11.SALES_VOLUME_WEIGHT_LBS) as ACTUAL_VOLUME_LBS\n",
    "    from DL_GBL_TAS_BI.FACT_SALES_ACTUAL as a11\n",
    "    join DL_GBL_TAS_BI.VW_H_PRODUCT_ALL_SALES as a12\n",
    "        on (a11.MATERIAL_ID = a12.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_DN_ALL as a13\n",
    "        on (a11.MATERIAL_ID = a13.MATERIAL_ID)\n",
    "    join DL_GBL_TAS_BI.D_TIME_FY_V6 as a14\n",
    "        on (a11.ACCOUNTING_PERIOD_DATE = a14.DAY_CALENDAR_DATE)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_SALES_DATA as a15\n",
    "        on (a11.DISTRIBUTION_CHANNEL_ID = a15.DISTRIBUTION_CHANNEL_ID and \n",
    "        a11.MATERIAL_ID = a15.MATERIAL_ID and \n",
    "        a11.SALES_ORGANISATION_ID = a15.SALES_ORGANISATION_ID)\n",
    "    join DL_GBL_TAS_BI.VW_H_CUSTOMER_ALL_DIVISION00 as a16\n",
    "        on (a11.CUSTOMER_ID = a16.CUSTOMER and \n",
    "        a11.DISTRIBUTION_CHANNEL_ID = a16.DISTRIBUTION_CHANNEL and \n",
    "        a11.SALES_ORGANISATION_ID = a16.SALES_ORGANISATION)\n",
    "    join DL_GBL_TAS_BI.D_DIVISION as a17\n",
    "        on (a13.DIVISION_ID = a17.DIVISION_ID)\n",
    "    join DL_GBL_TAS_BI.D_MATERIAL_PRICING_GROUP as a18\n",
    "        on (a15.MATERIAL_PRICING_GROUP_ID = a18.MATERIAL_PRICING_GROUP_ID)\n",
    "    where (a14.FISCAL_YEAR_CODE in ('FY2019', 'FY2020', 'FY2021','FY2022')\n",
    "        and a11.SALES_ORGANISATION_ID in ('US01')\n",
    "        and a11.DISTRIBUTION_CHANNEL_ID in ('10')\n",
    "        and RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)) in ('6500002801','6500002807'))\n",
    "        and a14.CALENDAR_WEEK_NAME < ''' + str(to_week) + ''' \n",
    "    group by a14.FISCAL_WEEK_NUMBER,\n",
    "        (a14.FISCAL_WEEK_NUMBER_DESCR || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        a14.CALENDAR_WEEK_NAME,\n",
    "        (a14.CALENDAR_WEEK_LONG_DESCRIPTION || ' ' || a14.START_DATE_OF_SAPYW),\n",
    "        RIGHT(a16.CUSTOMER_HIER_LVL_1,CAST(10 AS INTEGER)),\n",
    "        a16.CUSTOMER_HIER_LVL_1_NAME,\n",
    "        a13.DIVISION_ID,\n",
    "        a17.DIVISION_NAME,\n",
    "        a12.CATEGORY_SHORT_CODE,\n",
    "        a12.CATEGORY_DESC,\n",
    "        a12.SUB_CATEGORY_SHORT_CODE,\n",
    "        a12.SUB_CATEGORY_DESC,\n",
    "        a15.MATERIAL_PRICING_GROUP_ID,\n",
    "        a18.MATERIAL_PRICING_GROUP_DESCRIPTION,\n",
    "        TRIM (LEADING '0' FROM a13.MATERIAL_ID),\n",
    "        a13.MATERIAL_DESCRIPTION\n",
    ";'''\n",
    "\n",
    "    #create dataframe using both functions td_to_pandas and td_dataframe\n",
    "    df = td_dataframe(select_db, query)\n",
    "    \n",
    "    return teradata_transform(df, sellout)\n",
    "\n",
    "\n",
    "def teradata_transform(sellin, sellout):\n",
    "    #consolidates teradata sales with sellout data\n",
    "    \n",
    "    #convert from object datatype to float (exports as a number instead of string)\n",
    "    sellin['actual_volume_lbs'] = sellin['actual_volume_lbs'].astype('float64')\n",
    "    \n",
    "    #rename columns for consistancy\n",
    "    sellin = sellin.rename(columns = {'actual_volume_lbs':'LBS', 'calendar_week_number':'Calendar Week Year'})\n",
    "    \n",
    "    #transform calendar week year from teradata\n",
    "    sellin['Calendar Week Year'] = pd.to_numeric(sellin['Calendar Week Year'], errors = 'coerce')\n",
    "\n",
    "    #transform category so its consolidated\n",
    "    sellin['Consolidated Category'] = sellin['category_desc']\n",
    "    sellin.loc[sellin['Consolidated Category'] == 'Sweet Potato' , 'Consolidated Category'] = 'Potato'\n",
    "    sellin.loc[sellin['Consolidated Category'] != 'Potato' , 'Consolidated Category'] = 'Prepared Foods'\n",
    "    \n",
    "    #analyze sellin data\n",
    "    sellin = analyze_1(sellin, ['Consolidated Category'], 201910, 202009)\n",
    "    \n",
    "    #rename columns accordingly\n",
    "    sellin = sellin.rename(columns = {'LBS':'MCCAIN LBS',\n",
    "                                      'SMA_4':'MCCAIN SMA_4',\n",
    "                                      'SMA_8':'MCCAIN SMA_8',\n",
    "                                      'SMA_12':'MCCAIN SMA_12',\n",
    "                                      'LBS_PRECOVID':'MCCAIN PRECOVID',\n",
    "                                      'LBS_Lag_1':'MCCAIN Lag_1',\n",
    "                                      'LBS_Lag_2':'MCCAIN Lag_2',\n",
    "                                      'LBS_Lag_3':'MCCAIN Lag_3',\n",
    "                                      'LBS_Lag_4':'MCCAIN Lag_4',\n",
    "                                      'LBS_Baseline' : 'MCCAIN LBS_Baseline',\n",
    "                                      'SMA_4_Baseline' : 'MCCAIN SMA_4_Baseline',\n",
    "                                      'SMA_8_Baseline' : 'MCCAIN SMA_8_Baseline',\n",
    "                                      'SMA_12_Baseline' : 'MCCAIN SMA_12_Baseline',\n",
    "                                      'SMA_4_Lag_1':'MCCAIN SMA_4_Lag_1',\n",
    "                                      'SMA_4_Baseline_Lag_1' : 'MCCAIN SMA_4_Baseline_Lag_1',\n",
    "                                      'LBS_Baseline_Lag_1': 'MCCAIN LBS_Baseline_Lag_1'\n",
    "                                     })\n",
    "    \n",
    "    #analyze sellout data\n",
    "    df = analyze_1(sellout, ['Consolidated Category'], 201910, 202009)\n",
    "                                    \n",
    "    df = df.merge(sellin[['Calendar Week Year','Consolidated Category','MCCAIN LBS','MCCAIN SMA_4','MCCAIN SMA_8','MCCAIN SMA_12','MCCAIN PRECOVID',\n",
    "                          'MCCAIN LBS_Baseline','MCCAIN SMA_4_Baseline','MCCAIN SMA_8_Baseline','MCCAIN SMA_12_Baseline',\n",
    "                          'MCCAIN Lag_1', 'MCCAIN Lag_2', 'MCCAIN Lag_3', 'MCCAIN Lag_4','MCCAIN LBS_Baseline_Lag_1',\n",
    "                          'MCCAIN SMA_4_Lag_1', 'MCCAIN SMA_4_Baseline_Lag_1']], how = 'left', \n",
    "                  left_on = ['Calendar Week Year','Consolidated Category'], right_on = ['Calendar Week Year','Consolidated Category'])\n",
    "    \n",
    "    df = df.fillna({'MCCAIN LBS': 0,\n",
    "                    'MCCAIN SMA_4': 0,\n",
    "                    'MCCAIN SMA_8': 0,\n",
    "                    'MCCAIN SMA_12': 0,\n",
    "                    'MCCAIN PRECOVID': 0,\n",
    "                    'MCCAIN Lag_1': 0,\n",
    "                    'MCCAIN Lag_2': 0,\n",
    "                    'MCCAIN Lag_3': 0,\n",
    "                    'MCCAIN Lag_4': 0,\n",
    "                    'MCCAIN LBS_Baseline': 0,\n",
    "                    'MCCAIN SMA_4_Baseline': 0,\n",
    "                    'MCCAIN SMA_8_Baseline': 0,\n",
    "                    'MCCAIN SMA_12_Baseline': 0,\n",
    "                    'MCCAIN LBS_Baseline_Lag_1':0,\n",
    "                    'MCCAIN SMA_4_Lag_1' : 0,\n",
    "                    'MCCAIN SMA_4_Baseline_Lag_1' : 0\n",
    "                   })\n",
    "    \n",
    "    df['Distributor'] = 'PFG'\n",
    "\n",
    "    df = add_time(df)\n",
    "    \n",
    "    df = df[['Consolidated Category','Distributor','Calendar Week Year',\n",
    "             'LBS','SMA_4','SMA_8','SMA_12','LBS_PRECOVID',\n",
    "             'LBS_Baseline','SMA_4_Baseline','SMA_8_Baseline','SMA_12_Baseline',\n",
    "             'LBS_Lag_1', 'LBS_Lag_2', 'LBS_Lag_3', 'LBS_Lag_4', 'LBS_Baseline_Lag_1', 'SMA_4_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
    "             'MCCAIN LBS','MCCAIN SMA_4','MCCAIN SMA_8','MCCAIN SMA_12','MCCAIN PRECOVID',\n",
    "             'MCCAIN LBS_Baseline','MCCAIN SMA_4_Baseline','MCCAIN SMA_8_Baseline','MCCAIN SMA_12_Baseline',\n",
    "             'MCCAIN Lag_1', 'MCCAIN Lag_2', 'MCCAIN Lag_3', 'MCCAIN Lag_4','MCCAIN LBS_Baseline_Lag_1','MCCAIN SMA_4_Lag_1','MCCAIN SMA_4_Baseline_Lag_1',\n",
    "             'Week Starting (Sun)','Week Ending (Sat)','COVID Week']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Import Raw Data\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\NEWATTER\\\\OneDrive - McCain Foods Limited\\\\Historical Sell-Out Sales\\\\\\\\Meal Ticket - PFG Sales Report v1.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_13492/1520135054.py\u001b[0m in \u001b[0;36mimport_file\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0m_import\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0m_import\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'openpyxl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1417\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1419\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m    524\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"openpyxl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m    506\u001b[0m         )\n\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workbook_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m    509\u001b[0m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m             )\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\NEWATTER\\\\OneDrive - McCain Foods Limited\\\\Historical Sell-Out Sales\\\\\\\\Meal Ticket - PFG Sales Report v1.xlsx'"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_13492/2535229457.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_new = import_file(PATH + 'Meal Ticket - PFG Sales Report v1.xlsx')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Imported {_new.shape[0]} records'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '_new' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 425666 entries, 0 to 425665\n",
    "Data columns (total 29 columns):\n",
    " #   Column            Non-Null Count   Dtype         \n",
    "---  ------            --------------   -----         \n",
    " 0   Division          425666 non-null  object        \n",
    " 1   Sales Manager     322447 non-null  object        \n",
    " 2   Sales Rep Number  293051 non-null  object        \n",
    " 3   Sales Rep         381138 non-null  object        \n",
    " 4   Account Number    425666 non-null  int64         \n",
    " 5   Account Name      425666 non-null  object        \n",
    " 6   Address           406082 non-null  object        \n",
    " 7   City              425666 non-null  object        \n",
    " 8   State             425666 non-null  object        \n",
    " 9   Postal Code       425666 non-null  object        \n",
    " 10  Customer Class    425666 non-null  object        \n",
    " 11  Segment           425655 non-null  object        \n",
    " 12  Account Type      425666 non-null  object        \n",
    " 13  Vendor            425666 non-null  object        \n",
    " 14  Manufacturer      425666 non-null  object        \n",
    " 15  Brand             425666 non-null  object        \n",
    " 16  Brand Type        424398 non-null  object        \n",
    " 17  Product Category  380120 non-null  object        \n",
    " 18  Sub-Category      380120 non-null  object        \n",
    " 19  GTIN              425666 non-null  int64         \n",
    " 20  MFR SKU           425666 non-null  object        \n",
    " 21  Dist SKU          425666 non-null  object        \n",
    " 22  Item Name         425666 non-null  object        \n",
    " 23  Pack              425666 non-null  object        \n",
    " 24  Size              412230 non-null  object        \n",
    " 25  Unit Type         425666 non-null  object        \n",
    " 26  Qty               425666 non-null  float64       \n",
    " 27  Weight            425666 non-null  float64       \n",
    " 28  Invoice Week      425666 non-null  datetime64[ns]\n",
    "dtypes: datetime64[ns](1), float64(2), int64(2), object(24)\n",
    "memory usage: 94.2+ MB\n",
    "'''\n",
    "\n",
    "#new file name in historical file directory\n",
    "%time _new = import_file(PATH + 'Meal Ticket - PFG Sales Report v1.xlsx')\n",
    "\n",
    "print(f'Imported {_new.shape[0]} records', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Apply Dictionary to Raw Data / Check For Missing Segmentation\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataframe shape: (516436, 29)\n",
      "These manufacturers were included: ['ANCHOR FROZEN FOODS INC', 'DOT FOODS (FROZEN)', 'ENDICO POTATOES INC.', 'MC CAIN FOODS INC', 'MCCAIN FOODS AFD', 'MCCAIN FOODS USA INC', 'MCCAIN FOODS USA INC-ACH', 'MCCAIN FOODS, INC. ', 'MCCAIN FOODSERVICE INC', 'MCCAIN PRODUCE INC', 'WH MOSELEY CO', 'MCCAIN FOODS', 'McCain Foods USA']\n",
      "These manufacturers were not included:                  Manufacturer\n",
      "2             SUPPLIER VARIES\n",
      "4   JR SIMPLOT COMPANY FROZEN\n",
      "6         CAVENDISH FARMS INC\n",
      "8       LAMB WESTON SALES INC\n",
      "9                 LAMB WESTON\n",
      "10   ROSINA FOOD PRODUCTS INC\n",
      "16       ROSINA FOOD PRODUCTS\n",
      "18        BIG DADDY FOODS INC\n",
      "19   J. R. SIMPLOT (ROCHELLE)\n",
      "Shape before adding dictionary: (501993, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\Users\\newatter\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\Users\\newatter\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after adding dictionary: (501993, 41)\n",
      "Nothing missing for COVID Segmentation - L1\n",
      "Nothing missing for Product\n"
     ]
    }
   ],
   "source": [
    "#add dictionary to new data\n",
    "_new_df = apply_dictionary(_new, 'PFG - US.xlsx')\n",
    "\n",
    "is_missing(_new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Combine New and Base Datasets\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported shape...(7151430, 17)\n",
      "Final shape...(7620063, 16)\n"
     ]
    }
   ],
   "source": [
    "_import_df = pd.read_csv(BACKUP + 'PFG.csv', low_memory = False, thousands = ',', decimal = '.', dtype = {\n",
    "            'Calendar Week Year':np.int64,\n",
    "            'LBS':np.float64})\n",
    "\n",
    "print(f'Imported shape...{_import_df.shape}', flush = True)\n",
    "\n",
    "#create list to exclude from base data\n",
    "exclude_list = _new_df['Calendar Week Year'].values.squeeze().tolist()\n",
    "#exclude_list = _new_df['Calendar Week Year'].to_list()\n",
    "\n",
    "#turn list to string\n",
    "\n",
    "#only keep last 12 weeks\n",
    "exclude_list = exclude_list[-12:]\n",
    "\n",
    "include = str(exclude_list)[1:-1]\n",
    "\n",
    "#import all records from base data minus the new data\n",
    "_base = _import_df[~_import_df['Calendar Week Year'].isin(exclude_list)][['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                 'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type',\n",
    "                 'Consolidated Category','SKU ID','Brand','Cuisine Type','L1 Product Hierarchy','L2 Product Hierarchy','LBS','Calendar Week Year']].append(\n",
    "        _new_df[['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                 'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type',\n",
    "                 'Consolidated Category','SKU ID','Brand','Cuisine Type','L1 Product Hierarchy','L2 Product Hierarchy','LBS','Calendar Week Year']])\n",
    "\n",
    "_base = clean_city(_base)\n",
    "\n",
    "#create a copy but just for restaurants#\n",
    "#_restaurants = restaurants(_base)\n",
    "\n",
    "print(f'Final shape...{_base.shape}', flush = True)\n",
    "#print(f'Restaurants shape...{_restaurants.shape}', flush = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202205\n",
      "[202146, 202147, 202148, 202149, 202150, 202151, 202152, 202201, 202202, 202203, 202204, 202205]\n",
      "202146, 202147, 202148, 202149, 202150, 202151, 202152, 202201, 202202, 202203, 202204, 202205\n"
     ]
    }
   ],
   "source": [
    "exclude_list = _new_df.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})['Calendar Week Year'].sort_values().to_list()[-12:]\n",
    "#exclude_list = _new_df['Calendar Week Year'].to_list()\n",
    "\n",
    "#turn list to string\n",
    "\n",
    "print(_new_df['Calendar Week Year'].max())\n",
    "\n",
    "#only keep last 12 weeks\n",
    "exclude_list = exclude_list[-12:]\n",
    "\n",
    "include = str(exclude_list)[1:-1]\n",
    "\n",
    "print(exclude_list)\n",
    "print(include)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Execute Analysis\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Region\n",
      "Processing Sell in vs Sell out\n",
      "Starting Teradata connect...\n",
      "Database selected!\n",
      "07/07/2022 04:11:34 PM\n",
      "Query: Execution started...finished. 0:01:09.049508\n",
      "Query: Fetching data started...finished. 0:00:48.116217\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.171968\n",
      "Dim: (87008, 17)\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_list = []\n",
    "\n",
    "#Output 1: COVID L1 - List 0\n",
    "_list.append(['City', 'State Name','COVID Segmentation - L1','COVID Segmentation - L2','Restaurant Service Type','Consolidated Category'])\n",
    "\n",
    "#Output 2: COVID L1 - List 1\n",
    "_list.append(['State Name','COVID Segmentation - L1','COVID Segmentation - L2','Restaurant Service Type','SKU ID','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy'])\n",
    "\n",
    "print(f'Processing Region', flush = True)\n",
    "output1 = process_list(_base, _list[0])\n",
    "\n",
    "print(f'Processing Sell in vs Sell out', flush = True)\n",
    "output2 = teradata_sales(_base)\n",
    "\n",
    "#print(f'Processing SKU', flush = True)\n",
    "#output3 = process_list(_base, _list[1])\n",
    "\n",
    "#Output 2: COVID L1 - List 1\n",
    "#_list.append(['COVID Segmentation - L1','COVID Segmentation - L2','Restaurant Service Type','Consolidated Category','L1 Product Hierarchy','L2 Product Hierarchy'])\n",
    "#output4 = process_list(_base, _list[2])\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Upload Analysis to Teradata\n",
    "Run cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teradata Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database selected! 07/07/2022 04:13:43 PM\n",
      "Deleting records for: PFG in table: SELLOUT_REGION\n",
      "Inserting records into SELLOUT_REGION\n",
      "Inserted 22548 records\n",
      "Database selected! 07/07/2022 04:14:44 PM\n",
      "Deleting records for: PFG in table: SELLOUT_AND_SELLIN\n",
      "Inserting records into SELLOUT_AND_SELLIN\n",
      "Inserted 24 records\n"
     ]
    }
   ],
   "source": [
    "def td_upload(select_db, df, table_name):\n",
    "    with teradatasql.connect(None, \n",
    "                         host='172.29.3.43',\n",
    "                         user='PNWATTERS',\n",
    "                         password='teradata123') as con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute (select_db)\n",
    "            d = dt.now().strftime('%m/%d/%Y %r')\n",
    "            print(f'Database selected! {d}', flush=True)           \n",
    "\n",
    "            delete_from_td(df, table_name, cur)\n",
    "            insert_into_td(df, table_name, cur)\n",
    "\n",
    "def delete_from_td(df, table_name, cur):\n",
    "    distributor = df.groupby('Distributor').size().reset_index().drop(columns=0).to_numpy()[0][0]\n",
    "    \n",
    "    print(f'Deleting records for: {distributor} in table: {table_name}', flush = True)          \n",
    "        \n",
    "    query = '''\n",
    "    DELETE FROM ''' + table_name  + ''' \n",
    "    WHERE \"Distributor\" = ''' + \"'\" + distributor + \"'\" + ''' AND \"Calendar Week Year\" IN (''' + include + \")\"\n",
    "    \n",
    "    #query = '''\n",
    "    #DELETE FROM ''' + table_name  + ''' \n",
    "    #WHERE \"Distributor\" = ''' + \"'\" + distributor + \"'\"\n",
    "    \n",
    "    cur.execute (query)\n",
    "    \n",
    "def insert_into_td(df, table_name, cur):\n",
    "    insert_list = df.values.tolist()\n",
    "    \n",
    "    #creates ?, ?,.... string used in query for teradata fastload\n",
    "    insert_columns = ('?, ' * len(df.columns)).rstrip(', ')\n",
    "    \n",
    "    print(f'Inserting records into {table_name}', flush = True)\n",
    "    \n",
    "    query = \"INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    #query = \"{fn teradata_try_fastload}INSERT INTO \" + table_name  + \" (\" + insert_columns + \")\"\n",
    "    \n",
    "    cur.execute (query, insert_list)\n",
    "    \n",
    "    print(f'Inserted {df.shape[0]} records', flush = True)\n",
    "    \n",
    "\n",
    "select_db = 'DATABASE DL_NA_PROTOTYPING'\n",
    "\n",
    "exclude_list = _base.groupby(['Calendar Week Year']).size().reset_index().drop(columns={0})['Calendar Week Year'].sort_values(ascending=True).to_list()[-12:]\n",
    "include = str(exclude_list)[1:-1]\n",
    "\n",
    "td_upload(select_db, output1[output1['Calendar Week Year'].isin(exclude_list)], 'SELLOUT_REGION')\n",
    "td_upload(select_db, output2[output2['Calendar Week Year'].isin(exclude_list)], 'SELLOUT_AND_SELLIN')\n",
    "#td_upload(select_db, output3[output3['Calendar Week Year'].isin(exclude_list)].astype({'SKU ID':'str'}), 'SELLOUT_REGION_SKU')\n",
    "\n",
    "#For testing and cleanup\n",
    "#td_upload(select_db, output1, 'SELLOUT_REGION')\n",
    "#td_upload(select_db, output2, 'SELLOUT_AND_SELLIN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Backup When Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_backup(_base, 'PFG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Teradata connect...\n",
      "Database selected!\n",
      "07/07/2022 04:25:23 PM\n",
      "Query: Execution started...finished. 0:00:18.348386\n",
      "Query: Fetching data started...finished. 0:30:22.081655\n",
      "Query: Creating DataFrame for started...finished. 0:00:06.457176\n",
      "Dim: (825468, 35)\n"
     ]
    }
   ],
   "source": [
    "def teradata_sellout():\n",
    "    \n",
    "    print(f'Starting Teradata connect...', flush = True)\n",
    "    \n",
    "    select_db = \"DATABASE DL_NA_PROTOTYPING\"\n",
    "\n",
    "    query = '''\n",
    "    select * from SELLOUT_REGION where DISTRIBUTOR not in ('SYSCO US');\n",
    "    '''\n",
    "   \n",
    "    return td_dataframe(select_db, query)\n",
    "\n",
    "teradata_sellout().to_csv('SELLOUT_REGION.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Teradata connect...\n",
      "Database selected!\n",
      "07/07/2022 04:56:33 PM\n",
      "Query: Execution started...finished. 0:00:06.303671\n",
      "Query: Fetching data started...finished. 0:00:00.241042\n",
      "Query: Creating DataFrame for started...finished. 0:00:00.013714\n",
      "Dim: (1973, 38)\n"
     ]
    }
   ],
   "source": [
    "def teradata_sellout_and_sellin():\n",
    "    \n",
    "    print(f'Starting Teradata connect...', flush = True)\n",
    "    \n",
    "    select_db = \"DATABASE DL_NA_PROTOTYPING\"\n",
    "\n",
    "    query = '''\n",
    "    select * from SELLOUT_AND_SELLIN where DISTRIBUTOR not in ('SYSCO US');\n",
    "    '''\n",
    "   \n",
    "    return td_dataframe(select_db, query)\n",
    "\n",
    "teradata_sellout_and_sellin().to_csv('SELLOUT_AND_SELLIN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State Name', 'COVID Segmentation - L1',\n",
       "       'COVID Segmentation - L2', 'Restaurant Service Type',\n",
       "       'Consolidated Category', 'Calendar Week Year', 'Distributor', 'LBS',\n",
       "       'SMA_4', 'SMA_8', 'SMA_12', 'YOY Week', 'LBS_LY', 'SMA_4_LY',\n",
       "       'SMA_8_LY', 'SMA_12_LY', 'Baseline Week', 'LBS_Baseline',\n",
       "       'SMA_4_Baseline', 'SMA_8_Baseline', 'SMA_12_Baseline', 'LBS_Lag_1',\n",
       "       'LBS_Lag_2', 'LBS_Lag_3', 'LBS_Lag_4', 'LBS_Baseline_Lag_1',\n",
       "       'LBS_LY_Lag_1', 'SMA_4_Lag_1', 'SMA_4_LY_Lag_1', 'SMA_4_Baseline_Lag_1',\n",
       "       'LBS_PRECOVID', 'Week Starting (Sun)', 'Week Ending (Sat)',\n",
       "       'COVID Week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PFG Refresh\n",
    "Refresh with raw data\n",
    "\n",
    "Noticed very high spikes 15-20m LBs. when regular volume is 2-4m LBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup file\n",
    "backup = pd.read_csv(r'C:\\Users\\newatter\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\Backups\\PFG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frame(file_name):\n",
    "    date = file_name[118:128]\n",
    "    \n",
    "    #print(len(file_name))\n",
    "    #print(file_name[118:128])\n",
    "    \n",
    "    df = pd.read_csv(file_name, low_memory=False, thousands=',', dtype={'Qty':'float64','Weight':'float64'})\n",
    "    \n",
    "    df['Invoice Week'] = pd.to_datetime(date, format='%Y-%m-%d')\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2446248\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# assign directory\n",
    "directory = r'C:\\Users\\newatter\\OneDrive - McCain Foods Limited\\Historical Sell-Out Sales\\PFG Refresh'\n",
    "\n",
    "#Loop through all files in directory and create a dataframe\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        if len(f) > 120:\n",
    "            df = pd.concat([df, build_frame(f)])\n",
    "        \n",
    "print(df.shape[0])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-06-27 00:00:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Invoice Week'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run new data through dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataframe shape: (2446248, 29)\n",
      "These manufacturers were included: ['ANCHOR FROZEN FOODS INC', 'DOT FOODS (FROZEN)', 'ENDICO POTATOES INC.', 'MC CAIN FOODS INC', 'MCCAIN FOODS AFD', 'MCCAIN FOODS USA INC', 'MCCAIN FOODS USA INC-ACH', 'MCCAIN FOODS, INC. ', 'MCCAIN FOODSERVICE INC', 'MCCAIN PRODUCE INC', 'WH MOSELEY CO', 'MCCAIN FOODS', 'McCain Foods USA']\n",
      "These manufacturers were not included:                  Manufacturer\n",
      "2             SUPPLIER VARIES\n",
      "4   JR SIMPLOT COMPANY FROZEN\n",
      "5         CAVENDISH FARMS INC\n",
      "8       LAMB WESTON SALES INC\n",
      "11                LAMB WESTON\n",
      "15   ROSINA FOOD PRODUCTS INC\n",
      "16        BIG DADDY FOODS INC\n",
      "17       ROSINA FOOD PRODUCTS\n",
      "19          PFS IFH (HICKORY)\n",
      "20   J. R. SIMPLOT (ROCHELLE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_23496\\2172914771.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'Segment'] = df['Segment'].str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before adding dictionary: (2377491, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_23496\\2172914771.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'Invoice Week'] = pd.to_datetime(df['Invoice Week'])\n",
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_23496\\2172914771.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'customer_class_lower'] = df['Customer Class'].str.lower()\n",
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_23496\\2172914771.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'segment_lower'] = df['Segment'].str.strip().str.lower()\n",
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_23496\\2172914771.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'account_type_lower'] = df['Account Type'].str.strip().str.lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after adding dictionary: (2377491, 41)\n",
      "Nothing missing for COVID Segmentation - L1\n",
      "Nothing missing for Product\n"
     ]
    }
   ],
   "source": [
    "#add dictionary to new data\n",
    "_new_df = apply_dictionary(df, 'PFG - US.xlsx')\n",
    "\n",
    "is_missing(_new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "back up rows: 6043544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newatter\\AppData\\Local\\Temp\\1\\ipykernel_23496\\3725550509.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  _base = backup[~backup['Calendar Week Year'].isin(exclude)][['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base rows: 6080988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6080988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude = _new_df['Calendar Week Year'].to_list()\n",
    "\n",
    "print(f'back up rows: {backup.shape[0]}')\n",
    "\n",
    "#import all records from base data minus the new data\n",
    "_base = backup[~backup['Calendar Week Year'].isin(exclude)][['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                 'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type',\n",
    "                 'Consolidated Category','SKU ID','Brand','Cuisine Type','L1 Product Hierarchy','L2 Product Hierarchy','LBS','Calendar Week Year']].append(\n",
    "        _new_df[['City','State','State Name','COVID Segmentation - L1','COVID Segmentation - L2',\n",
    "                 'COVID Segmentation - (Restaurants)','COVID Segmentation - (Restaurants: Sub-Segment)','Restaurant Service Type',\n",
    "                 'Consolidated Category','SKU ID','Brand','Cuisine Type','L1 Product Hierarchy','L2 Product Hierarchy','LBS','Calendar Week Year']])\n",
    "\n",
    "print(f'base rows: {_base.shape[0]}')\n",
    "\n",
    "_base = clean_city(_base)\n",
    "\n",
    "_base.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910607.42\n",
      "2910607.42\n"
     ]
    }
   ],
   "source": [
    "print(_new_df[_new_df['Calendar Week Year']==202205]['LBS'].sum())\n",
    "\n",
    "print(_base[_base['Calendar Week Year']==202205]['LBS'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_backup(_base, 'PFG.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
